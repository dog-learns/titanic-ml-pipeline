{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNhjfvf9ryWT"
      },
      "source": [
        "# Titanic ML Pipeline: From Preprocessing to Model Evaluation  \n",
        "## ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼šå‰å‡¦ç†ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¾ã§  \n",
        "\n",
        "### ğŸ§­ Introduction / ã¯ã˜ã‚ã«  \n",
        "The Titanic dataset is a classic introductory dataset for binary classification tasks in machine learning.  \n",
        "In this notebook, I build a complete machine learning pipeline using multiple models (e.g., RandomForest, XGBoost, SVM), perform hyperparameter tuning, and analyze model interpretability with SHAP.\n",
        "\n",
        "The final stacking model (with XGBoost as meta learner) achieved 83.6% accuracy and 87.4% ROC AUC in cross-validation.  \n",
        "All steps are explained in both English and Japanese to make the content more accessible and educational.\n",
        "\n",
        "---\n",
        "\n",
        "ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®ãƒã‚¤ãƒŠãƒªåˆ†é¡å•é¡Œã¨ã—ã¦æœ‰åãªå…¥é–€ç”¨èª²é¡Œã§ã™ã€‚  \n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ»XGBoostãƒ»SVM ãªã©è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨SHAPã‚’ç”¨ã„ãŸè§£é‡ˆã‚‚è¡Œã„ã¾ã™ã€‚\n",
        "\n",
        "æœ€çµ‚çš„ãªã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoostã‚’ãƒ¡ã‚¿å­¦ç¿’å™¨ã¨ã—ã¦ä½¿ç”¨ï¼‰ã¯ã€äº¤å·®æ¤œè¨¼ã«ã¦æ­£è§£ç‡83.6%ã€ROC AUC 87.4%ã‚’é”æˆã—ã¾ã—ãŸã€‚  \n",
        "ã™ã¹ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯è‹±èªã¨æ—¥æœ¬èªã®èª¬æ˜ã‚’åŠ ãˆã¦ãŠã‚Šã€å­¦ç¿’ç”¨ãƒ»ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªç”¨ã®ä¸¡æ–¹ã«å½¹ç«‹ã¤å†…å®¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0O4IP3Zczdc"
      },
      "source": [
        "## ğŸ¯ Objective  \n",
        "- Predict passenger survival on the Titanic dataset  \n",
        "ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã®ä¹—å®¢ã®ç”Ÿå­˜äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨\n",
        "\n",
        "- Build a reproducible and interpretable ML pipeline  \n",
        "å†ç¾æ€§ãƒ»èª¬æ˜å¯èƒ½æ€§ã®é«˜ã„æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨\n",
        "---\n",
        "\n",
        "## ğŸ“ Contents  \n",
        "1. ğŸ“¥ Data Loading & Initial Exploration / ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸç¢ºèª\n",
        "2. ğŸ”€ Data Preprocessing / ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†  \n",
        "3. â™’ Baseline Modeling / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n",
        "4. ğŸ›  Feature Engineering / ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
        "5. ğŸ¤– Model Training & Tuning / ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "6. ğŸ“Š Evaluation & Comparison / ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨æ¯”è¼ƒ\n",
        "7. ğŸ“¤ Final Submission / æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ   \n",
        "8. ğŸ¯ SHAP-Based Model Contribution (Meta Model: XGBoost) / SHAPã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è²¢çŒ®åº¦åˆ†æ  \n",
        "9. ğŸ” Final Model Analysis / æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨è€ƒå¯Ÿ  \n",
        "10. ğŸ“Œ Conclusion / çµè«–\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”§ Tools & Libraries\n",
        "\n",
        "| Library             | Usage                                   |\n",
        "| ------------------- | --------------------------------------- |\n",
        "| pandas, numpy       | Data preprocessing / ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†             |\n",
        "| matplotlib, seaborn | Visualization / å¯è¦–åŒ–                     |\n",
        "| scikit-learn        | Modeling & Evaluation / ãƒ¢ãƒ‡ãƒ«ã¨è©•ä¾¡          |\n",
        "| xgboost, lightgbm   | High-performance ML algorithms / é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ« |\n",
        "| SHAP                | Model interpretability / ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§        |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Œ Notes  \n",
        "- Feature selection based on top 80% cumulative importance  \n",
        "é‡è¦åº¦ã®ç´¯ç©80%ã‚’åŸºæº–ã«ç‰¹å¾´é‡é¸å®šã‚’è¡Œã„ã¾ã—ãŸ\n",
        "\n",
        "- GridSearchCV & RandomizedSearchCV used for hyperparameter tuning  \n",
        "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã«ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã‚’ä½¿ç”¨\n",
        "\n",
        "- Developed in Google Colab, dataset loaded from Google Drive  \n",
        "Colabä¸Šã§ä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã¯Google Driveã‹ã‚‰èª­ã¿è¾¼ã¿\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¬ Feedback Welcome! / ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ­“è¿\n",
        "This notebook is a work in progress as I continue improving my skills.\n",
        "ã”æ„è¦‹ãƒ»ã”ææ¡ˆãŒã‚ã‚Œã°ã€ãœã²ãŠå¯„ã›ãã ã•ã„ï¼\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ZWgAsqL99T"
      },
      "source": [
        "## ğŸ“ Data Access (Google Colab or Kaggle) / ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "\n",
        "ğŸ” This notebook automatically detects the environment (Google Colab or Kaggle)  \n",
        "and sets the file path accordingly.  \n",
        "Google Colab users should save the dataset in Google Drive.  \n",
        "Kaggle users should upload it using the \"Add Data\" button.\n",
        "\n",
        "ğŸ” ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ Google Colab ã¾ãŸã¯ Kaggle ã®å®Ÿè¡Œç’°å¢ƒã‚’è‡ªå‹•åˆ¤å®šã—ã€ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚  \n",
        "Colab ã‚’ä½¿ã†å ´åˆã¯ Google Drive ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ãã ã•ã„ã€‚  \n",
        "Kaggle ã®å ´åˆã¯ã€ŒAdd Dataã€ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEOkuVTLLWi9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# âœ… Detect environment / å®Ÿè¡Œç’°å¢ƒã®åˆ¤å®š\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ğŸ“ Set file path depending on environment / å®Ÿè¡Œç’°å¢ƒã«å¿œã˜ã¦ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/Kaggle/Titanic/'\n",
        "else:\n",
        "\n",
        "    path = '/kaggle/input/titanic-data/'\n",
        "\n",
        "# ğŸ“„ Load datasets / ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
        "df_train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "\n",
        "# ğŸ” Preview\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf3VVXpk8kmK"
      },
      "outputs": [],
      "source": [
        "# ğŸ”§ Basic Libraries / åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "# ğŸ“Š Visualization / å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# âš™ï¸ Preprocessing & Model Selection / å‰å‡¦ç†ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«é¸å®šé–¢é€£ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, StratifiedKFold, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.base import clone\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# ğŸ¤– Classification Algorithms / åˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb  # for Booster API if needed\n",
        "\n",
        "# ğŸ“ˆ Evaluation Metrics & Tools / è©•ä¾¡æŒ‡æ¨™ãŠã‚ˆã³ãƒ„ãƒ¼ãƒ«\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.tree import plot_tree\n",
        "import time\n",
        "\n",
        "# ğŸš« Suppress Warnings / è­¦å‘Šã®éè¡¨ç¤ºè¨­å®š\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… check data / ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "j5xk4QLa1mZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9dq35Oq-Ukw"
      },
      "outputs": [],
      "source": [
        "# Check data types and missing values / ãƒ‡ãƒ¼ã‚¿ã¨æ¬ æå€¤ã®ç¢ºèª\n",
        "df.info()\n",
        "\n",
        "# View summary statistics for numerical features / æ•°å€¤ç‰¹å¾´é‡ã®è¦ç´„çµ±è¨ˆé‡ã‚’è¡¨ç¤ºã™ã‚‹\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15tFE13VRLQb"
      },
      "source": [
        "# 1. Data Exploration / ãƒ‡ãƒ¼ã‚¿ç†è§£ãƒ»æ¢ç´¢\n",
        "### 1.1 Check Missing Values / æ¬ æå€¤ã®ç¢ºèª\n",
        "\n",
        "We check the number of missing values for each feature in both training and test datasets to identify columns that need imputation or special handling.\n",
        "\n",
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å„ç‰¹å¾´é‡ã«ã¤ã„ã¦ã€æ¬ æå€¤ã®æ•°ã‚’ç¢ºèªã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è£œå®Œã‚„ç‰¹åˆ¥ãªå‡¦ç†ãŒå¿…è¦ãªåˆ—ã‚’ç‰¹å®šã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hyTqjDN_m7k"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UVeugipCS4H"
      },
      "outputs": [],
      "source": [
        "df_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFOKNeEyN4gU"
      },
      "source": [
        "### ğŸ” Missing Values / æ¬ æå€¤ã®ç¢ºèª\n",
        "\n",
        "We found missing values in the following columns:  \n",
        "ä»¥ä¸‹ã®ã‚«ãƒ©ãƒ ã«æ¬ æå€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š\n",
        "\n",
        "- **Age**: Needs to be imputed. â†’ Will fill with the median value.  \n",
        "  **å¹´é½¢**ï¼šè£œå®ŒãŒå¿…è¦ â†’ ä¸­å¤®å€¤ã§è£œå®Œã—ã¾ã™ã€‚\n",
        "\n",
        "- **Cabin**: Many missing values. â†’ Will drop or ignore this feature. Extract Deck (first letter) â†’ Replace missing with 'U' (Unknown) â†’ One-hot encode Deck â†’ Create binary flag for Cabin presence â†’ Drop original Cabin and Deck columns    \n",
        "  **å®¢å®¤ç•ªå·**ï¼šæ¬ æãŒéå¸¸ã«å¤šã„ â†’ å…ˆé ­æ–‡å­—ã‚’æŠ½å‡ºã—ã¦Deckã¨ã—ã¦æ‰±ã† â†’ æ¬ æã¯'U'ï¼ˆä¸æ˜ï¼‰ã«ç½®æ› â†’ Deckã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ– â†’ Cabinã®æœ‰ç„¡ã‚’ç¤ºã™2å€¤ç‰¹å¾´é‡ã‚’ä½œæˆ â†’ å…ƒã®Cabinã¨Deckåˆ—ã‚’å‰Šé™¤\n",
        "\n",
        "- **Embarked**: 2 missing values. â†’ Will fill with the most frequent value (mode).  \n",
        "  **ä¹—èˆ¹åœ°**ï¼š2ä»¶ã®æ¬ æ â†’ æœ€é »å€¤ã§è£œå®Œã—ã¾ã™ã€‚\n",
        "\n",
        "- **Fare (in test set)**: 1 missing value. â†’ Will fill with the median value.  \n",
        "  **é‹è³ƒï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰**ï¼š1ä»¶ã®æ¬ æ â†’ ä¸­å¤®å€¤ã§è£œå®Œã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHw-T_rpNNE_"
      },
      "source": [
        "### 1.2 Visualizing Survival Rate / ç”Ÿå­˜ç‡ã®å¯è¦–åŒ–\n",
        "Let's visualize the distribution of survivors and non-survivors in the dataset using both a pie chart and a count plot.\n",
        "\n",
        "å††ã‚°ãƒ©ãƒ•ã¨æ£’ã‚°ãƒ©ãƒ•ï¼ˆã‚«ã‚¦ãƒ³ãƒˆãƒ—ãƒ­ãƒƒãƒˆï¼‰ã‚’ä½¿ã£ã¦ã€ç”Ÿå­˜è€…ã¨éç”Ÿå­˜è€…ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚\n",
        "\n",
        "This helps us understand the class balance in the target variable.\n",
        "\n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€ç›®çš„å¤‰æ•°ï¼ˆSurvivedï¼‰ã®ã‚¯ãƒ©ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ40HPQxDwjw"
      },
      "outputs": [],
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
        "df['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
        "ax[0].set_title('Survived')\n",
        "ax[0].set_ylabel('')\n",
        "sns.countplot(x = 'Survived',data=df,ax=ax[1])\n",
        "ax[1].set_title('Survived')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymvUT8aDprDo"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨\n",
        "\n",
        "- About **62%** of passengers did not survive, while **38%** survived.  \n",
        "  ç´„ **62%** ã®ä¹—å®¢ãŒç”Ÿå­˜ã§ããšã€**38%** ãŒç”Ÿå­˜ã—ã¦ã„ã¾ã—ãŸã€‚\n",
        "- The dataset is somewhat imbalanced, so using accuracy alone might be misleading.  \n",
        "  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚„ã‚„ä¸å‡è¡¡ã§ã‚ã‚Šã€å˜ç´”ãªç²¾åº¦ã ã‘ã§ã¯è©•ä¾¡ã«åã‚ŠãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- Therefore, other metrics like ROC AUC and F1 score should also be considered.  \n",
        "  ã“ã®ãŸã‚ã€ROC AUC ã‚„ F1ã‚¹ã‚³ã‚¢ãªã©ã®æŒ‡æ¨™ã®ä½µç”¨ãŒé‡è¦ã«ãªã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_LQpTDR7Bk"
      },
      "source": [
        "## 1.3 Passenger Class Distribution and Survival by Class / ä¹—å®¢ã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒã¨ã‚¯ãƒ©ã‚¹ã”ã¨ã®ç”Ÿå­˜ç‡\n",
        "\n",
        "In this section, we explore how passengers are distributed among the three classes (`Pclass`).  \n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ä¹—å®¢ãŒ3ã¤ã®ã‚¯ãƒ©ã‚¹ï¼ˆPclassï¼‰ã«ã©ã®ã‚ˆã†ã«åˆ†å¸ƒã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
        "\n",
        "The bar chart on the left shows the number of passengers in each class.  \n",
        "å·¦ã®æ£’ã‚°ãƒ©ãƒ•ã¯ã€å„ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹ä¹—å®¢ã®äººæ•°ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "The count plot on the right breaks down survival status by class, giving us insight into survival trends per passenger class.  \n",
        "å³ã®æ£’è”µã‚°ãƒ©ãƒ•ã§ã¯ã€ç”Ÿå­˜çŠ¶æ³ã‚’ã‚¯ãƒ©ã‚¹ã”ã¨ã«åˆ†è§£ã—ã¦è¡¨ç¤ºã—ã€ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹ç”Ÿå­˜å‚¾å‘ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "As we can see, passengers in 1st class had a higher chance of survival, while those in 3rd class faced lower survival rates.  \n",
        "ã”è¦§ã®ã¨ãŠã‚Šã€1ç­‰èˆ¹å®¤ã®ä¹—å®¢ã¯ç”Ÿå­˜ç‡ãŒé«˜ãã€3ç­‰èˆ¹å®¤ã®ä¹—å®¢ã¯ç”Ÿå­˜ç‡ãŒä½ã„å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhbzVuMpAcDW"
      },
      "outputs": [],
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8), facecolor='gray')\n",
        "df['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\n",
        "ax[0].set_title('Number of Passengers By Pclass')\n",
        "ax[0].set_ylabel('Count')\n",
        "sns.countplot(x='Pclass',hue='Survived',data=df,ax=ax[1])\n",
        "ax[1].set_title('Pclass:Perished vs Survived')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX-HtIG3zHdJ"
      },
      "source": [
        "## 1.4 Survival Rate by Fare Group within Pclass 3 / 3ç­‰èˆ¹å®¤ã«ãŠã‘ã‚‹é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ç”Ÿå­˜ç‡\n",
        "\n",
        "We select passengers in Pclass 3 and group them by fare quartiles to analyze survival rates.  \n",
        "3ç­‰èˆ¹å®¤ã®ä¹—å®¢ã‚’æŠ½å‡ºã—ã€é‹è³ƒã®å››åˆ†ä½æ•°ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ã—ã¦ç”Ÿå­˜ç‡ã‚’åˆ†æã—ã¾ã™ã€‚\n",
        "\n",
        "The survival rates are calculated for each fare group to see if fare impacts survival within Pclass 3.  \n",
        "å„é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç”Ÿå­˜ç‡ã‚’è¨ˆç®—ã—ã€3ç­‰èˆ¹å®¤å†…ã§é‹è³ƒãŒç”Ÿå­˜ã«å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlMY4rgzImZh"
      },
      "outputs": [],
      "source": [
        "# Select Pclass 3 passengers\n",
        "pclass3 = df[df['Pclass'] == 3].copy()\n",
        "\n",
        "# Create fare quartile groups\n",
        "pclass3['FareGroup'] = pd.qcut(pclass3['Fare'], q=4)\n",
        "\n",
        "# Calculate survival rate by FareGroup\n",
        "survival_by_fare = pclass3.groupby('FareGroup')['Survived'].mean()\n",
        "\n",
        "print(survival_by_fare)\n",
        "\n",
        "# Optional: visualize survival rates by fare group\n",
        "survival_by_fare.plot(kind='bar')\n",
        "plt.title('Survival Rate by Fare Group (Pclass 3)')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5wcEZCKv8Q"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "As shown in the plot, survival rates are similar across fare quartiles,  \n",
        "indicating that fare does not significantly affect survival within Pclass 3.  \n",
        "ã‚°ãƒ©ãƒ•ã®é€šã‚Šã€é‹è³ƒã®å››åˆ†ä½æ•°ã”ã¨ã«ç”Ÿå­˜ç‡ã¯ã»ã¼åŒã˜ã§ã‚ã‚Šã€  \n",
        "3ç­‰èˆ¹å®¤å†…ã§ã¯é‹è³ƒãŒç”Ÿå­˜ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPpCl-IPRzuF"
      },
      "source": [
        "## 1.5 ğŸš» Survival Rate by Sex within Pclass 3 / 3ç­‰èˆ¹å®¤ã«ãŠã‘ã‚‹æ€§åˆ¥ã”ã¨ã®ç”Ÿå­˜ç‡  \n",
        "Next, we calculate survival rates by sex among passengers in Pclass 3.  \n",
        "æ¬¡ã«ã€3ç­‰èˆ¹å®¤ã®ä¹—å®¢ã«ã¤ã„ã¦æ€§åˆ¥ã”ã¨ã®ç”Ÿå­˜ç‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
        "\n",
        "This helps us understand how gender influenced survival chances within the lowest passenger class.  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€æœ€ã‚‚ä¸‹ä½ã®ä¹—å®¢ã‚¯ãƒ©ã‚¹å†…ã§æ€§åˆ¥ãŒç”Ÿå­˜ç‡ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã—ãŸã‹ãŒã‚ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKhbW36hSU71"
      },
      "outputs": [],
      "source": [
        "# Calculate survival rate by sex within Pclass 3\n",
        "survival_by_sex_pclass3 = pclass3.groupby('Sex')['Survived'].mean()\n",
        "print(survival_by_sex_pclass3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTV4X-2mSOGr"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "The results show that females in Pclass 3 had a significantly higher survival rate compared to males.  \n",
        "çµæœã‚’è¦‹ã‚‹ã¨ã€3ç­‰èˆ¹å®¤ã®å¥³æ€§ã¯ç”·æ€§ã«æ¯”ã¹ã¦ã‹ãªã‚Šé«˜ã„ç”Ÿå­˜ç‡ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "This reflects the \"women and children first\" evacuation policy practiced on the Titanic.  \n",
        "ã“ã‚Œã¯ã€ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã§å®Ÿæ–½ã•ã‚ŒãŸã€Œå¥³æ€§ã¨å­ä¾›ã‚’å…ˆã«æ•‘åŠ©ã™ã‚‹ã€æ–¹é‡ã‚’åæ˜ ã—ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hJxDDrXJjEO"
      },
      "source": [
        "## 1.6 ğŸ» Visualizing Age Distribution by Pclass and Sex (Violin Plot) /        Pclassãƒ»æ€§åˆ¥ã”ã¨ã®å¹´é½¢åˆ†å¸ƒã¨ç”Ÿå­˜çŠ¶æ³ã®å¯è¦–åŒ–  \n",
        "We use violin plots to visualize the age distribution of passengers,  \n",
        "split by survival status across different `Pclass` and `Sex` categories.  \n",
        "ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ä¹—å®¢ã®å¹´é½¢åˆ†å¸ƒã‚’ç”Ÿå­˜çŠ¶æ³ã”ã¨ã«è¡¨ç¤ºã—ã¾ã™ã€‚  \n",
        "ã“ã“ã§ã¯ `Pclass`ï¼ˆä¹—å®¢ã‚¯ãƒ©ã‚¹ï¼‰ã¨ `Sex`ï¼ˆæ€§åˆ¥ï¼‰ã”ã¨ã«åˆ†ã‘ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "These plots help identify patterns in how age and other factors affect survival.  \n",
        "ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã£ã¦ã€å¹´é½¢ã‚„ä»–ã®è¦ç´ ãŒç”Ÿå­˜ç‡ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã—ãŸã‹ã®å‚¾å‘ã‚’è¦–è¦šçš„ã«æŠŠæ¡ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBsPtplpISHh"
      },
      "outputs": [],
      "source": [
        "# Create side-by-side violin plots\n",
        "f, ax = plt.subplots(1, 2, figsize=(18, 8), facecolor='gray')\n",
        "\n",
        "# Pclass Ã— Age Ã— Survived\n",
        "sns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=df, split=True, ax=ax[0])\n",
        "ax[0].set_title('Pclass and Age vs Survived')\n",
        "ax[0].set_yticks(range(0, 110, 10))\n",
        "\n",
        "# Sex Ã— Age Ã— Survived\n",
        "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=df, split=True, ax=ax[1])\n",
        "ax[1].set_title('Sex and Age vs Survived')\n",
        "ax[1].set_yticks(range(0, 110, 10))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne90luXsUVsJ"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "In both plots, we observe that younger passengers (especially children) had higher survival rates,  \n",
        "particularly in 1st and 2nd class, and among females.  \n",
        "ã©ã¡ã‚‰ã®ãƒ—ãƒ­ãƒƒãƒˆã§ã‚‚ã€ç‰¹ã«1ç­‰ãƒ»2ç­‰èˆ¹å®¤ã‚„å¥³æ€§ã®ä¸­ã§ã€å­ã©ã‚‚ï¼ˆä½å¹´é½¢å±¤ï¼‰ã®ç”Ÿå­˜ç‡ãŒé«˜ã„å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "On the other hand, older male passengers in 3rd class had lower survival rates.  \n",
        "ä¸€æ–¹ã§ã€3ç­‰èˆ¹å®¤ã®å¹´é…ç”·æ€§ã®ç”Ÿå­˜ç‡ã¯ä½ã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "This visualization supports previous findings and reinforces the influence of class, gender, and age.  \n",
        "ã“ã®å¯è¦–åŒ–ã¯ã€ã“ã‚Œã¾ã§ã®åˆ†æçµæœã‚’è£ä»˜ã‘ã€ã‚¯ãƒ©ã‚¹ãƒ»æ€§åˆ¥ãƒ»å¹´é½¢ã®å½±éŸ¿ã‚’å†ç¢ºèªã™ã‚‹ã‚‚ã®ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Lz5zBMVAYJ"
      },
      "source": [
        "We plot the average survival rate for each gender using a bar chart.  \n",
        "æ€§åˆ¥ã”ã¨ã®å¹³å‡ç”Ÿå­˜ç‡ã‚’æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚\n",
        "\n",
        "This clearly shows that female passengers had a much higher survival rate than males.  \n",
        "ã“ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€å¥³æ€§ã®æ–¹ãŒç”Ÿå­˜ç‡ãŒã¯ã‚‹ã‹ã«é«˜ã„ã“ã¨ãŒæ˜ç¢ºã«ã‚ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7FWqLbEN-B_"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='Sex', y='Survived', data=df)\n",
        "plt.title(\"Survival Rate by Sex\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2t7uHqaUZs"
      },
      "source": [
        "## 1.7 ğŸ‘¶ Survival Rate of Children by Sex /                                   å­ã©ã‚‚ï¼ˆ10æ­³æœªæº€ï¼‰ã®ç”Ÿå­˜ç‡ã‚’æ€§åˆ¥ã”ã¨ã«ç¢ºèª  \n",
        "We define a new column `Child` that marks passengers under 10 years old as 1, and others as 0.  \n",
        "10æ­³æœªæº€ã®ä¹—å®¢ã‚’ã€Œå­ã©ã‚‚ï¼ˆChild = 1ï¼‰ã€ã¨ã—ã€ãã‚Œä»¥å¤–ã‚’0ã¨ã—ã¦æ–°ã—ã„åˆ—ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
        "\n",
        "Then, we create a normalized crosstab to compare survival rates by `Child` and `Sex`.  \n",
        "æ¬¡ã«ã€`Child`ã¨`Sex`ã®çµ„ã¿åˆã‚ã›ã§ã€ç”Ÿå­˜ç‡ã‚’ã‚¯ãƒ­ã‚¹é›†è¨ˆï¼ˆæ­£è¦åŒ–ï¼‰ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MKfsy_uQCAs"
      },
      "outputs": [],
      "source": [
        "# å­ã©ã‚‚ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ ã—ã€Child Ã— Sex ã”ã¨ã®ç”Ÿå­˜ç‡ã‚’ã‚¯ãƒ­ã‚¹é›†è¨ˆ\n",
        "df.assign(Child = df['Age'].apply(lambda x: 1 if x < 10 else 0)) \\\n",
        "  .pipe(lambda d: pd.crosstab([d['Child'], d['Sex']], d['Survived'], normalize='index'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7FM9s82WUbm"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "- **Adult women (Child = 0, female)** had the **highest survival rate** (75.4%), consistent with the \"ladies first\" policy.  \n",
        "  ï¼ˆå¤§äººã®å¥³æ€§ã¯æœ€ã‚‚é«˜ã„ç”Ÿå­˜ç‡ã‚’æŒã£ã¦ãŠã‚Šã€ã“ã‚Œã¯ã€Œå¥³æ€§å„ªå…ˆã€ã®åŸå‰‡ã¨ä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚ï¼‰\n",
        "\n",
        "- **Adult men (Child = 0, male)** had the **lowest survival rate** (16.5%), as expected.  \n",
        "  ï¼ˆå¤§äººã®ç”·æ€§ã®ç”Ÿå­˜ç‡ã¯æœ€ã‚‚ä½ãã€äºˆæƒ³é€šã‚Šã®çµæœã§ã™ã€‚ï¼‰\n",
        "\n",
        "- **Child males** had a significantly **higher survival rate (59.4%)** than adult males.  \n",
        "  ï¼ˆå­ã©ã‚‚ã®ç”·æ€§ã¯å¤§äººã®ç”·æ€§ã‚ˆã‚Šã‚‚å¤§å¹…ã«é«˜ã„ç”Ÿå­˜ç‡ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ï¼‰\n",
        "\n",
        "- Interestingly, **child females** had a lower survival rate (63.3%) than **adult females** (75.4%).  \n",
        "  ï¼ˆèˆˆå‘³æ·±ã„ã“ã¨ã«ã€å¥³ã®å­ã®ç”Ÿå­˜ç‡ã¯å¤§äººã®å¥³æ€§ã‚ˆã‚Šã‚‚ã‚„ã‚„ä½ããªã£ã¦ã„ã¾ã™ã€‚ï¼‰\n",
        "\n",
        "This may be due to factors such as the location of cabins, family groups, or evacuation priority.  \n",
        "ï¼ˆã“ã‚Œã¯ã€ã‚­ãƒ£ãƒ“ãƒ³ã®å ´æ‰€ã€å®¶æ—å˜ä½ã®ç§»å‹•ã€é¿é›£ã®å„ªå…ˆé †ä½ãªã©ãŒå½±éŸ¿ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ï¼‰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My2wYFm8ZH0V"
      },
      "source": [
        "## 1.8 ğŸ”¥ Correlation Heatmap of Numerical Features / æ•°å€¤ç‰¹å¾´é‡ã®ç›¸é–¢é–¢ä¿‚ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—  \n",
        "We plot a correlation heatmap of all numerical features in the dataset.  \n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®æ•°å€¤å‹ç‰¹å¾´é‡ã«ã¤ã„ã¦ã€ç›¸é–¢é–¢ä¿‚ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»ã—ã¾ã™ã€‚\n",
        "\n",
        "This helps identify which features are strongly related to each other or to the target variable (`Survived`).  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€ç‰¹å¾´é‡é–“ã€ã¾ãŸã¯ç›®çš„å¤‰æ•°ï¼ˆ`Survived`ï¼‰ã¨ã®å¼·ã„é–¢ä¿‚ãŒã‚ã‚‹ã‚‚ã®ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPw58i9QSBWg"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns only\n",
        "df_numeric = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Plot heatmap of correlations\n",
        "sns.heatmap(df_numeric.corr(), annot=True, cmap='bwr', linewidths=0.2)\n",
        "\n",
        "# Set figure size\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dFKrWAdaNzf"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "- `Fare` shows a moderate positive correlation with `Survived`.  \n",
        "  `Fare`ï¼ˆé‹è³ƒï¼‰ã¯ `Survived` ã¨ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢ã‚’æŒã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- `Pclass` has a negative correlation with `Survived`, indicating that higher classes had better survival chances.  \n",
        "  `Pclass` ã¯ `Survived` ã¨è² ã®ç›¸é–¢ãŒã‚ã‚Šã€é«˜ã„ã‚¯ãƒ©ã‚¹ï¼ˆ1ç­‰ï¼‰ãŒç”Ÿå­˜ã—ã‚„ã™ã‹ã£ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- Some features like `SibSp` and `Parch` also have weak correlations, which might still be useful in combination.  \n",
        "  `SibSp` ã‚„ `Parch` ã®ã‚ˆã†ãªç‰¹å¾´é‡ã‚‚å¼±ã„ç›¸é–¢ã‚’ç¤ºã—ã¾ã™ãŒã€ä»–ã®ç‰¹å¾´ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨æœ‰åŠ¹ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsLwvPplgn4C"
      },
      "source": [
        "# 2 ğŸ§¼ Data Preprocessing / ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
        "\n",
        "Before training models, we need to clean and preprocess the dataset.  \n",
        "This includes handling missing values, encoding categorical variables, and standardizing formats.  \n",
        "ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹å‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¬ æå€¤å‡¦ç†ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®çµ±ä¸€ãªã©ã‚’è¡Œã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_m1oSXHaqvJ"
      },
      "source": [
        "ğŸ“¦ Backup the Preprocessed Data / å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—  \n",
        "We create backup copies of the preprocessed train and test datasets.  \n",
        "å‰å‡¦ç†æ¸ˆã¿ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä¿å­˜ã—ã¦ãŠãã¾ã™ã€‚\n",
        "\n",
        "This is useful when experimenting with feature engineering or trying different preprocessing strategies,  \n",
        "as we can always revert to the original cleaned state.  \n",
        "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚„å‰å‡¦ç†æ–¹æ³•ã‚’è©¦ã™éš›ã«ã€å…ƒã®çŠ¶æ…‹ã«æˆ»ã›ã‚‹ã®ã§ä¾¿åˆ©ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPH8H2Ixaftg"
      },
      "outputs": [],
      "source": [
        "df_base = df.copy() # baseline\n",
        "df_base_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrPd67x-auUV"
      },
      "source": [
        "We create backup copies of the preprocessed train and test datasets.  \n",
        "å‰å‡¦ç†æ¸ˆã¿ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä¿å­˜ã—ã¦ãŠãã¾ã™ã€‚\n",
        "\n",
        "This is useful when experimenting with feature engineering or trying different preprocessing strategies,  \n",
        "as we can always revert to the original cleaned state.  \n",
        "ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚„å‰å‡¦ç†æ–¹æ³•ã‚’è©¦ã™éš›ã«ã€å…ƒã®çŠ¶æ…‹ã«æˆ»ã›ã‚‹ã®ã§ä¾¿åˆ©ã§ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5p0uC1-6RlE"
      },
      "source": [
        "## 2.1 ğŸ§“ Age Imputation / Ageã®è£œå®Œ\n",
        "\n",
        "`Age` has a relatively large number of missing values.  \n",
        "To ensure consistency between training and test data, we will fill the missing values using the **median age of the combined train and test datasets**.\n",
        "\n",
        "`Age` ã«ã¯æ¯”è¼ƒçš„å¤šãã®æ¬ æå€¤ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã«ã€**è¨“ç·´ï¼‹ãƒ†ã‚¹ãƒˆå…¨ä½“ã®å¹´é½¢ã®ä¸­å¤®å€¤**ã‚’ä½¿ã£ã¦è£œå®Œã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF3uDUhgTFij"
      },
      "outputs": [],
      "source": [
        "# Combine train and test Age columns to calculate a consistent median\n",
        "age = pd.concat([df_base['Age'],df_base_test['Age']])\n",
        "\n",
        "# Fill missing Age values in both datasets with the same median\n",
        "df_base['Age'] = df_base['Age'].fillna(age.median())\n",
        "df_base_test['Age'] = df_base_test['Age'].fillna(age.median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg_7zBe_6tVZ"
      },
      "source": [
        "## 2.2 ğŸ’° Fare Imputation (Test Data) / Fareã®è£œå®Œï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰\n",
        "\n",
        "The test dataset contains one missing value in the `Fare` column.  \n",
        "Instead of using the overall median, we calculate the median `Fare` by passenger class (`Pclass`) and fill the missing value with the median of the corresponding class.\n",
        "\n",
        "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ `Fare` ã®æ¬ æå€¤ãŒ1ä»¶å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚  \n",
        "å˜ç´”ã«å…¨ä½“ã®ä¸­å¤®å€¤ã§è£œå®Œã™ã‚‹ã®ã§ã¯ãªãã€ä¹—å®¢ã®ç­‰ç´šï¼ˆ`Pclass`ï¼‰ã”ã¨ã« `Fare` ã®ä¸­å¤®å€¤ã‚’è¨ˆç®—ã—ã€  \n",
        "è©²å½“ã™ã‚‹ `Pclass` ã®ä¸­å¤®å€¤ã§è£œå®Œã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå¦¥å½“ãªæ¨å®šã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFmt5VqjmAuP"
      },
      "outputs": [],
      "source": [
        "# Calculate the median Fare for each Pclass from the training data\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ Pclass ã”ã¨ã® Fare ã®ä¸­å¤®å€¤ã‚’è¨ˆç®—\n",
        "fare_per_class = df_base.groupby('Pclass')['Fare'].median()\n",
        "\n",
        "# Apply class-specific median to fill missing Fare in the test data\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã«å¯¾ã—ã¦ã€å¯¾å¿œã™ã‚‹ Pclass ã®ä¸­å¤®å€¤ã§è£œå®Œ\n",
        "df_base_test['Fare'] = df_base_test.apply(\n",
        "    lambda row: fare_per_class[row['Pclass']] if pd.isnull(row['Fare']) else row['Fare'],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-cB5Z0J7BTV"
      },
      "source": [
        "## 2.3 ğŸ›³ï¸ Cabin Feature Engineering / Cabinæƒ…å ±ã®ç‰¹å¾´é‡å¤‰æ›\n",
        "\n",
        "The `Cabin` column contains many missing values and detailed cabin numbers, which are difficult to use directly.  \n",
        "We extract the first letter of each cabin as a new feature called `Deck`, since it may relate to passenger class or location on the ship.  \n",
        "We also create a binary feature `Has_Cabin` that indicates whether cabin information was available.\n",
        "\n",
        "`Cabin` åˆ—ã«ã¯éå¸¸ã«å¤šãã®æ¬ æãŒã‚ã‚Šã€è©³ç´°ãªã‚­ãƒ£ãƒ“ãƒ³ç•ªå·ã®ã¾ã¾ã§ã¯ä½¿ç”¨ãŒé›£ã—ã„ã§ã™ã€‚  \n",
        "ãã®ãŸã‚ã€ã‚­ãƒ£ãƒ“ãƒ³ã®æœ€åˆã®æ–‡å­—ã‚’ `Deck` ã¨ã„ã†æ–°ã—ã„ç‰¹å¾´é‡ã¨ã—ã¦æŠ½å‡ºã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ä¹—å®¢ã®ç­‰ç´šã‚„èˆ¹å†…ã§ã®ä½ç½®ã«é–¢ä¿‚ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ã¾ãŸã€`Cabin` æƒ…å ±ã®æœ‰ç„¡ã‚’ç¤ºã™2å€¤ã®ç‰¹å¾´é‡ `Has_Cabin` ã‚‚ä½œæˆã—ã¾ã—ãŸï¼ˆ1: ã‚ã‚Šã€0: ãªã—ï¼‰ã€‚\n",
        "\n",
        "å…ƒã® `Cabin` åˆ—ã¯ä½¿ç”¨ã—ãªã„ãŸã‚å‰Šé™¤ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3eEIS_-YmFp"
      },
      "outputs": [],
      "source": [
        "# Extract the first letter from Cabin to represent Deck\n",
        "# ã‚­ãƒ£ãƒ“ãƒ³ã®å…ˆé ­æ–‡å­—ã‚’å–ã‚Šå‡ºã—ã¦Deckã¨ã—ã¦æ‰±ã†ã€‚æ¬ æã¯'U'ï¼ˆUnknownï¼‰ã«ã€‚\n",
        "df_base['Deck'] = df_base['Cabin'].str[0].fillna('U')\n",
        "df_base_test['Deck'] = df_base_test['Cabin'].str[0].fillna('U')\n",
        "\n",
        "# Replace rare 'T' deck with 'U' (Unknown)\n",
        "# ç¨€ã«å­˜åœ¨ã™ã‚‹ 'T' ã¯æƒ…å ±ãŒå°‘ãªã„ãŸã‚ 'U' ã«çµ±åˆ\n",
        "df_base['Deck'] = df_base['Deck'].replace('T', 'U')\n",
        "df_base_test['Deck'] = df_base_test['Deck'].replace('T', 'U')\n",
        "\n",
        "# Visualize survival rate by Deck\n",
        "# Deckã”ã¨ã®ç”Ÿå­˜ç‡ã‚’è¡¨ç¤ºã—ã¦å¯è¦–åŒ–\n",
        "deck_survival = df_base.groupby('Deck')['Survived'].mean().sort_values().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=deck_survival, x='Deck', y='Survived', palette='viridis')\n",
        "plt.title('Survival Rate by Deck')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.xlabel('Deck')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWy3hf6CtPq_"
      },
      "outputs": [],
      "source": [
        "# Combine train and test Deck columns to create dummy variables\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®Deckåˆ—ã‚’é€£çµã—ã€ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–\n",
        "all_decks = pd.concat([df_base['Deck'], df_base_test['Deck']], axis=0)\n",
        "decks_ohe = pd.get_dummies(all_decks, prefix='Deck').astype(int)\n",
        "\n",
        "# Split back to train and test datasets and concatenate\n",
        "# å…ƒã®è¡Œæ•°ã§åˆ†å‰²ã—ã¦ã€ãã‚Œãã‚Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµåˆ\n",
        "df_base = pd.concat([\n",
        "    df_base.reset_index(drop=True),\n",
        "    decks_ohe.iloc[:len(df_base)].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "df_base_test = pd.concat([\n",
        "    df_base_test.reset_index(drop=True),\n",
        "    decks_ohe.iloc[len(df_base):].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "# Create a binary feature indicating whether Cabin info is available\n",
        "# Cabinæƒ…å ±ãŒã‚ã£ãŸã‹ã©ã†ã‹ã®2å€¤ç‰¹å¾´é‡ï¼ˆ1: ã‚ã‚Š, 0: ãªã—ï¼‰\n",
        "df_base['Has_Cabin'] = df_base['Cabin'].notnull().astype(int)\n",
        "df_base_test['Has_Cabin'] = df_base_test['Cabin'].notnull().astype(int)\n",
        "\n",
        "# Drop the original Cabin and Deck columns as they are no longer needed\n",
        "# å…ƒã® Cabin ã¨ Deck ã®åˆ—ã¯ä¸è¦ãªã®ã§å‰Šé™¤\n",
        "df_base.drop(['Cabin', 'Deck'], axis=1, inplace=True)\n",
        "df_base_test.drop(['Cabin', 'Deck'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR7lxUv97PI7"
      },
      "source": [
        "## 2.5 âš“ Embarked Imputation / Embarkedã®æ¬ æè£œå®Œ\n",
        "\n",
        "The `Embarked` column has a few missing values.  \n",
        "Since most passengers embarked from 'S' (Southampton),  \n",
        "we fill missing values with 'S', the most frequent port.\n",
        "\n",
        "`Embarked` åˆ—ã«ã¯æ¬ æå€¤ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚  \n",
        "ä¹—å®¢ã®å¤§å¤šæ•°ãŒ 'S'ï¼ˆã‚µã‚¦ã‚µãƒ³ãƒ—ãƒˆãƒ³ï¼‰ã‹ã‚‰ä¹—èˆ¹ã—ã¦ã„ã‚‹ãŸã‚ã€  \n",
        "æ¬ æå€¤ã¯æœ€é »å€¤ã§ã‚ã‚‹ 'S' ã§è£œå®Œã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y31AymFQTeYT"
      },
      "source": [
        "### 2.5.1 âš“ Visualization of Embarked Port Counts / Embarkedã®ä¹—èˆ¹äººæ•°ã®å¯è¦–åŒ–\n",
        "\n",
        "This plot shows the distribution of passengers by their port of embarkation.  \n",
        "It helps us understand the class imbalance and supports the decision to impute missing values with the most frequent port.\n",
        "\n",
        "ä¹—èˆ¹åœ°ã”ã¨ã®ä¹—å®¢æ•°ã‚’ç¤ºã—ãŸã‚°ãƒ©ãƒ•ã§ã™ã€‚  \n",
        "ã‚¯ãƒ©ã‚¹ã®åã‚Šã‚’æŠŠæ¡ã§ãã€æ¬ æå€¤ã‚’æœ€é »å€¤ã§è£œå®Œã™ã‚‹æ ¹æ‹ ã®ä¸€ã¤ã«ãªã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6ZeW4qNrPo0"
      },
      "outputs": [],
      "source": [
        "# Visualize the count of passengers by Embarked port\n",
        "# Embarkedã”ã¨ã®ä¹—èˆ¹äººæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆãƒ—ãƒ­ãƒƒãƒˆã§è¡¨ç¤º\n",
        "sns.countplot(x='Embarked', data=df_base)\n",
        "plt.title('Number of Passengers by Embarked Port')\n",
        "plt.xlabel('Embarked Port')\n",
        "plt.ylabel('Number of Passengers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYj_tWYJV79y"
      },
      "source": [
        "### 2.5.2 Survival Rate by Embarked / Embarkedã”ã¨ã®ç”Ÿå­˜ç‡\n",
        "\n",
        "The following bar plot shows the survival rates grouped by the port of embarkation (`Embarked`).  \n",
        "Among the three main embarkation points â€” Southampton (S), Cherbourg (C), and Queenstown (Q) â€”  \n",
        "passengers who boarded at Cherbourg (C) had the highest survival rate.\n",
        "\n",
        "ä¸‹ã®æ£’ã‚°ãƒ©ãƒ•ã¯ã€ä¹—èˆ¹åœ° (`Embarked`) ã”ã¨ã®ç”Ÿå­˜ç‡ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚  \n",
        "3ã¤ã®ä¸»ãªä¹—èˆ¹åœ°ã€ã‚µã‚¦ã‚µãƒ³ãƒ—ãƒˆãƒ³ (S)ã€ã‚·ã‚§ãƒ«ãƒ–ãƒ¼ãƒ« (C)ã€ã‚¯ã‚¤ãƒ¼ãƒ³ã‚¹ã‚¿ã‚¦ãƒ³ (Q) ã®ä¸­ã§ã€  \n",
        "ã‚·ã‚§ãƒ«ãƒ–ãƒ¼ãƒ« (C) ã‹ã‚‰ä¹—èˆ¹ã—ãŸä¹—å®¢ãŒæœ€ã‚‚é«˜ã„ç”Ÿå­˜ç‡ã‚’ç¤ºã—ã¾ã—ãŸã€‚\n",
        "\n",
        "This indicates that embarkation point might be related to survival outcomes,  \n",
        "possibly reflecting differences in passenger demographics or ticket classes.\n",
        "\n",
        "ã“ã®ã“ã¨ã¯ã€ä¹—èˆ¹åœ°ãŒç”Ÿå­˜çµæœã«é–¢ä¿‚ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€  \n",
        "ä¹—å®¢ã®å±æ€§ã‚„ãƒã‚±ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®é•ã„ã‚’åæ˜ ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL5U1EOxruHL"
      },
      "outputs": [],
      "source": [
        "# Visualize survival rate by Embarked port\n",
        "# Embarkedã”ã¨ã®ç”Ÿå­˜ç‡ã‚’æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–\n",
        "sns.barplot(x='Embarked', y='Survived', data=df_base)\n",
        "plt.title('Survival Rate by Embarked')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.xlabel('Embarked Port')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic57czfmtnsl"
      },
      "outputs": [],
      "source": [
        "# Fill missing Embarked values with the most frequent value 'S'\n",
        "# Embarkedã®æ¬ æå€¤ã‚’æœ€é »å€¤'S'ã§è£œå®Œ\n",
        "df_base['Embarked'].fillna('S', inplace=True)\n",
        "df_base.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfil7Tgw1m4S"
      },
      "outputs": [],
      "source": [
        "df_base_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kadE02gT4ES3"
      },
      "source": [
        "## 2.6 Categorical Data Preparation / ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
        "\n",
        "We have completed handling all missing values.  \n",
        "Next, we will prepare categorical data so that models can handle them effectively.  \n",
        "\n",
        "ã“ã‚Œã§å…¨ã¦ã®æ¬ æå€¤å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚  \n",
        "æ¬¡ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ã„ã‚„ã™ã„ã‚ˆã†ã«ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjX4M-OMbJo2"
      },
      "source": [
        "### 2.6.1 Dropping Complex Text Features: Name and Ticket /  Name ã¨ Ticket ã®è¤‡é›‘ãªæ–‡å­—åˆ—ç‰¹å¾´é‡ã‚’å‰Šé™¤  \n",
        "The categorical columns include `Name`, `Sex`, `Ticket`, `Embarked`, and `Pclass`.  \n",
        "As a baseline, we will start by simply dropping the `Name` and `Ticket` columns since they are complex and not immediately useful.\n",
        "\n",
        "ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã¯ `Name`, `Sex`, `Ticket`, `Embarked`, `Pclass` ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ä»Šå›ã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã€è¤‡é›‘ãª `Name` ã¨ `Ticket` ã¯ç°¡å˜ã«å‰Šé™¤ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzdOizhN7z98"
      },
      "outputs": [],
      "source": [
        "# Drop 'Name' column (too detailed for baseline model)\n",
        "# 'Name'åˆ—ã¯è¤‡é›‘ãªã®ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ã¯å‰Šé™¤\n",
        "df_base.drop(['Name'], axis=1, inplace=True)\n",
        "df_base_test.drop(['Name'], axis=1, inplace=True)\n",
        "\n",
        "# Drop 'Ticket' column (too noisy for now)\n",
        "# 'Ticket'åˆ—ã‚‚ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ã¯æƒ…å ±ãŒä¸æ˜ç­ãªãŸã‚å‰Šé™¤\n",
        "df_base.drop(['Ticket'], axis=1, inplace=True)\n",
        "df_base_test.drop(['Ticket'], axis=1, inplace=True)\n",
        "\n",
        "# Check remaining columns after drop\n",
        "# å‰Šé™¤å¾Œã®ã‚«ãƒ©ãƒ ã‚’ç¢ºèª\n",
        "df_base.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMlTumtjwAey"
      },
      "source": [
        "### 2.6.2 Encoding Sex Column / Sexåˆ—ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
        "\n",
        "The `Sex` column is a binary categorical variable.  \n",
        "We convert it into numeric format by mapping `'male'` to `0` and `'female'` to `1`.  \n",
        "\n",
        "This allows models to interpret the gender information as numerical input.\n",
        "\n",
        "`Sex` åˆ—ã¯2å€¤ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã§ã™ã€‚  \n",
        "`'male'` ã‚’ `0`ã€`'female'` ã‚’ `1` ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒæ€§åˆ¥æƒ…å ±ã‚’æ•°å€¤ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGs2mAj-syBJ"
      },
      "outputs": [],
      "source": [
        "df_base['Sex'] = df_base['Sex'].map( {'male':0, 'female':1})\n",
        "df_base_test['Sex'] = df_base_test['Sex'].map( {'male':0, 'female':1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNUzP6buW375"
      },
      "source": [
        "### 2.6.3 One-Hot Encoding for Embarked / Embarkedåˆ—ã®One-Hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°  \n",
        "The `Embarked` column contains three categories: `'S'`, `'C'`, and `'Q'`.  \n",
        "To convert this categorical data into a format usable by machine learning models,  \n",
        "we apply One-Hot Encoding and create separate binary columns for each port.\n",
        "\n",
        "`Embarked` åˆ—ã«ã¯ `'S'`, `'C'`, `'Q'` ã®3ã¤ã®ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ã“ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§æ‰±ã„ã‚„ã™ãã™ã‚‹ãŸã‚ã€  \n",
        "One-Hot Encoding ã‚’é©ç”¨ã—ã€ãã‚Œãã‚Œã®æ¸¯ã«å¯¾å¿œã—ãŸãƒã‚¤ãƒŠãƒªåˆ—ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
        "\n",
        "We concatenate the training and test data before encoding to ensure consistency in the column order and structure.  \n",
        "Then we split them back and merge with the original datasets.  \n",
        "Finally, we drop the original `Embarked` column.\n",
        "\n",
        "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å‰ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é€£çµã™ã‚‹ã“ã¨ã§ã€åˆ—ã®é †åºã¨æ§‹é€ ã®ä¸€è²«æ€§ã‚’ä¿ã¡ã¾ã™ã€‚  \n",
        "ãã®å¾Œã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¦çµåˆã—ã€`Embarked` åˆ—ã¯å‰Šé™¤ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_k7PEkUX3Ri"
      },
      "outputs": [],
      "source": [
        "# Combine Embarked column from train and test to ensure consistent encoding\n",
        "# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®Embarkedåˆ—ã‚’çµåˆã—ã¦ã€One-Hotã®æ•´åˆæ€§ã‚’ä¿ã¤\n",
        "embarked = pd.concat([df_base['Embarked'], df_base_test['Embarked']])\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "# One-Hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨\n",
        "embarked_ohe = pd.get_dummies(embarked).astype(int)\n",
        "\n",
        "# Split back into train and test\n",
        "# è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²\n",
        "embarked_ohe_train = embarked_ohe[:len(df_base)]\n",
        "embarked_ohe_test = embarked_ohe[len(df_base):]\n",
        "\n",
        "# Add the encoded columns to the datasets\n",
        "# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµæœã‚’å…ƒãƒ‡ãƒ¼ã‚¿ã«çµåˆ\n",
        "df_base = pd.concat([df_base, embarked_ohe_train], axis=1)\n",
        "df_base_test = pd.concat([df_base_test, embarked_ohe_test], axis=1)\n",
        "\n",
        "# Drop the original Embarked column\n",
        "# å…ƒã®Embarkedåˆ—ã‚’å‰Šé™¤\n",
        "df_base.drop(columns=['Embarked'], inplace=True)\n",
        "df_base_test.drop(columns=['Embarked'], inplace=True)\n",
        "\n",
        "# Preview the result\n",
        "# å‡¦ç†å¾Œã®å…ˆé ­ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
        "df_base.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgZhK8kfESNk"
      },
      "source": [
        "### 2.6.4 One-Hot Encoding for Pclass / Pclassåˆ—ã®One-Hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°  \n",
        "Although `Pclass` is already numeric (1, 2, 3),  \n",
        "it is actually a categorical feature representing socio-economic class, not a continuous value.  \n",
        "\n",
        "Therefore, we apply One-Hot Encoding to prevent the model from interpreting it as ordinal.\n",
        "\n",
        "`Pclass` ã¯ã™ã§ã«æ•°å€¤ï¼ˆ1, 2, 3ï¼‰ã¨ã—ã¦è¡¨ç¾ã•ã‚Œã¦ã„ã¾ã™ãŒã€  \n",
        "ã“ã‚Œã¯é€£ç¶šå€¤ã§ã¯ãªãç¤¾ä¼šçš„éšç´šã‚’ç¤ºã™ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã§ã™ã€‚\n",
        "\n",
        "ãã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ãŒèª¤ã£ã¦é †åºçš„ãªæ„å‘³ã‚’æŒã¤ã¨åˆ¤æ–­ã—ãªã„ã‚ˆã†ã«ã€One-Hot Encoding ã‚’é©ç”¨ã—ã¾ã™ã€‚\n",
        "\n",
        "As before, we concatenate train and test data first, encode, and then split them back.  \n",
        "The original `Pclass` column is removed after encoding.\n",
        "\n",
        "ã“ã‚Œã¾ã§ã¨åŒæ§˜ã«ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é€£çµã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€  \n",
        "ãã®å¾Œåˆ†å‰²ã—ã¦å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«çµåˆã—ã¾ã™ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œã€`Pclass` åˆ—ã¯å‰Šé™¤ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V54VnlQQEU_-"
      },
      "outputs": [],
      "source": [
        "# Combine Pclass from train and test to ensure consistent encoding\n",
        "# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã®Pclassåˆ—ã‚’çµåˆã—ã¦ã€One-Hotã®æ•´åˆæ€§ã‚’ä¿ã¤\n",
        "pclass = pd.concat([df_base['Pclass'], df_base_test['Pclass']])\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "# One-Hotã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ï¼ˆprefixä»˜ãï¼‰\n",
        "pclass_ohe = pd.get_dummies(pclass, prefix='Pclass').astype(int)\n",
        "\n",
        "# Split back into train and test\n",
        "# è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²\n",
        "pclass_ohe_train = pclass_ohe[:len(df_base)]\n",
        "pclass_ohe_test = pclass_ohe[len(df_base):]\n",
        "\n",
        "# Add encoded columns to original datasets\n",
        "# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœã‚’å…ƒãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
        "df_base = pd.concat([df_base, pclass_ohe_train], axis=1)\n",
        "df_base_test = pd.concat([df_base_test, pclass_ohe_test], axis=1)\n",
        "\n",
        "# Drop original Pclass column\n",
        "# å…ƒã®Pclassåˆ—ã‚’å‰Šé™¤\n",
        "df_base.drop(columns=['Pclass'], inplace=True)\n",
        "df_base_test.drop(columns=['Pclass'], inplace=True)\n",
        "\n",
        "# Check resulting columns\n",
        "# çµæœã®ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’ç¢ºèª\n",
        "df_base.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDzQwFpZ8XhC"
      },
      "source": [
        "# 3. Baseline Modeling / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰\n",
        "\n",
        "## ğŸ¤– Preparing for Model Training / ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã«å‘ã‘ã¦\n",
        "\n",
        "Through the preprocessing steps so far, all features have been converted into numeric values,  \n",
        "and missing values have been appropriately handled.\n",
        "\n",
        "ã“ã“ã¾ã§ã®å‰å‡¦ç†ã«ã‚ˆã‚Šã€å…¨ã¦ã®ç‰¹å¾´é‡ã¯æ•°å€¤åŒ–ã•ã‚Œã€æ¬ æå€¤ã‚‚é©åˆ‡ã«è£œå®Œã•ã‚Œã¾ã—ãŸã€‚  \n",
        "ã“ã®çŠ¶æ…‹ã§ã‚ã‚Œã°ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ãã®ã¾ã¾å…¥åŠ›ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "In this section, we will start building a baseline model using Logistic Regression.  \n",
        "To do so, we will first split the training data into training and validation sets.\n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ä½¿ã£ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã„ãã¾ã™ã€‚  \n",
        "ã¾ãšã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cht2sh_5ECxw"
      },
      "source": [
        "## ğŸ“‚ 3.1 Preparing the Data / ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
        "\n",
        "We first separate the training data into features (`X`) and the target variable (`y`).  \n",
        "The target column is `Survived`, which we want to predict.  \n",
        "Therefore, we remove `Survived` from the feature set and assign it to `y`.\n",
        "\n",
        "ã¾ãšã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´é‡ (`X`) ã¨ç›®çš„å¤‰æ•° (`y`) ã«åˆ†ã‘ã¾ã™ã€‚  \n",
        "`Survived` ã¯äºˆæ¸¬å¯¾è±¡ã®å¤‰æ•°ã§ã‚ã‚‹ãŸã‚ã€`X` ã‹ã‚‰ã¯é™¤å¤–ã—ã€`y` ã«æ ¼ç´ã—ã¾ã™ã€‚\n",
        "\n",
        "## ğŸ§ª 3.2 Splitting the Data and Training a Baseline Model / è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
        "\n",
        "We remove unnecessary columns such as `PassengerId` from the feature set.  \n",
        "Then we split the data into training and validation sets (70% train / 30% validation)  \n",
        "to evaluate the model's performance.\n",
        "\n",
        "`PassengerId` ãªã©ä¸è¦ãªåˆ—ã‚’é™¤å¤–ã—ãŸå¾Œã€  \n",
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ï¼ˆ70%ï¼‰ã¨æ¤œè¨¼ç”¨ï¼ˆ30%ï¼‰ã«åˆ†å‰²ã—ã¦ã€  \n",
        "ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’é©åˆ‡ã«è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "\n",
        "We use Logistic Regression as our baseline model.  \n",
        "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFVvrdoV8aSC"
      },
      "outputs": [],
      "source": [
        "# Feature matrix: drop PassengerId and Survived\n",
        "# ç‰¹å¾´é‡è¡Œåˆ—ï¼ˆPassengerIdã¨Survivedã¯é™¤å¤–ï¼‰\n",
        "X = df_base.drop(columns=['PassengerId', 'Survived'])\n",
        "\n",
        "# Target variable\n",
        "# ç›®çš„å¤‰æ•°\n",
        "y = df_base['Survived']\n",
        "\n",
        "# Test data (for final prediction)\n",
        "# æå‡ºç”¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆäºˆæ¸¬å°‚ç”¨ï¼‰\n",
        "X_test = df_base_test.drop(columns=['PassengerId'])\n",
        "\n",
        "# Split training data: 70% train, 30% validation\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ7:3ï¼‰\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
        "lr = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn9XiJ3piFkV"
      },
      "source": [
        "## ğŸ“Š 3.3 Baseline Model Performance / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦è©•ä¾¡\n",
        "\n",
        "We evaluate the baseline Logistic Regression model using accuracy on both the training and validation sets.  \n",
        "This gives us a rough idea of how well the model is learning and whether it is overfitting.  \n",
        "Detailed metrics such as precision and recall will be used later when comparing multiple models.\n",
        "\n",
        "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã§ç²¾åº¦ï¼ˆAccuracyï¼‰ã‚’ä½¿ã£ã¦è©•ä¾¡ã—ã¾ã™ã€‚  \n",
        "ã“ã®ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒéå­¦ç¿’ã—ã¦ã„ãªã„ã‹ã€ãŠãŠã¾ã‹ãªæ€§èƒ½ã‚’ç¢ºèªã§ãã¾ã™ã€‚\n",
        "\n",
        "è©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ï¼ˆprecisionã‚„recallãªã©ï¼‰ã¯ã€å¾Œã®ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ•ã‚§ãƒ¼ã‚ºã§ä½¿ç”¨ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3MuW0F-hBrX"
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance on training and validation sets\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦ï¼ˆAccuracyï¼‰ã‚’è¡¨ç¤º\n",
        "print('Train Score: {}'.format(round(lr.score(X_train, y_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr.score(X_valid, y_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5zDbt3bQcFr"
      },
      "source": [
        "# 4 ğŸ§± ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° / Feature Engineering\n",
        "\n",
        "æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã®ãŸã‚ã«æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚  \n",
        "ç‰¹ã« Titanic ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€ã€Œåå‰ã€ã‚„ã€Œå®¶æ—æ§‹æˆã€ã‹ã‚‰æ„å‘³ã®ã‚ã‚‹ç‰¹å¾´ã‚’æŠ½å‡ºã§ãã¾ã™ã€‚\n",
        "\n",
        "In this section, we will create new features to improve the model's performance.  \n",
        "In the Titanic dataset, features like \"Name\" and \"Family composition\" often provide useful information.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ6staZ6EfNd"
      },
      "source": [
        "## 4.1 Family Size / å®¶æ—äººæ•°\n",
        "We create a new feature called Family by adding the values of SibSp (number of siblings/spouses aboard), Parch (number of parents/children aboard), and 1 (the passenger themselves).  \n",
        "SibSpï¼ˆå…„å¼Ÿãƒ»é…å¶è€…ã®åŒä¹—äººæ•°ï¼‰ã¨Parchï¼ˆè¦ªãƒ»å­ä¾›ã®åŒä¹—äººæ•°ï¼‰ã€ãã—ã¦æœ¬äººã‚’è¶³ã—ã¦ã€æ–°ã—ã„ç‰¹å¾´é‡Familyï¼ˆåŒä¹—å®¶æ—äººæ•°ï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G67uwRUNE2vE"
      },
      "outputs": [],
      "source": [
        "df_fe = df_base.copy()       # Create a copy for feature engineering (training data) / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹\n",
        "df_fe_test = df_base_test.copy()  # Create a copy for feature engineering (test data)\n",
        "\n",
        "# Add new feature 'Family' by summing SibSp, Parch, and 1 (the passenger themselves) / æ–°ã—ã„ç‰¹å¾´é‡ã€€Familyã‚’ä½œæˆ\n",
        "df_fe['Family'] = df_fe['SibSp'] + df_fe['Parch'] + 1\n",
        "df_fe_test['Family'] = df_fe_test['SibSp'] + df_fe_test['Parch'] + 1\n",
        "\n",
        "df_fe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fm9RLsRp32F"
      },
      "source": [
        "### 4.1.1 Baseline model with Family feature using Logistic Regression /  Familyç‰¹å¾´é‡ã‚’å«ã‚ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰\n",
        "We will now train a baseline model using Logistic Regression, including the newly added `Family` feature.  \n",
        "We split the dataset into training and validation sets (70:30), then evaluate accuracy.\n",
        "\n",
        "æ–°ãŸã«ä½œæˆã—ãŸ `Family` ç‰¹å¾´é‡ã‚’å«ã‚ã¦ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚  \n",
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã« 7:3 ã«åˆ†å‰²ã—ã€æ­£è§£ç‡ï¼ˆAccuracyï¼‰ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNwktqKvOsIF"
      },
      "outputs": [],
      "source": [
        "X_fe = df_fe.drop(columns=['PassengerId','Survived'])\n",
        "y_fe = df_fe['Survived']\n",
        "\n",
        "X_fe_test =  df_fe_test.drop(columns=['PassengerId'])\n",
        "\n",
        "X_fe_train, X_fe_valid, y_fe_train, y_fe_valid = train_test_split(X_fe, y_fe, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_fe = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe.fit(X_fe_train, y_fe_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(lr_fe.score(X_fe_train, y_fe_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe.score(X_fe_valid, y_fe_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnVN6OhKTY7A"
      },
      "source": [
        "### 4.1.2 Comparison of L1 and L2 Regularization / L1ãƒ»L2æ­£å‰‡åŒ–ã®æ¯”è¼ƒ  \n",
        "Next, we apply **L1 and L2 regularization** to logistic regression and compare their performance.  \n",
        "L1 regularization helps with feature selection by pushing some coefficients to zero.  \n",
        "L2 regularization helps prevent overfitting by shrinking all coefficients.\n",
        "\n",
        "æ¬¡ã«ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã« **L1ï¼ˆLassoï¼‰ã¨L2ï¼ˆRidgeï¼‰æ­£å‰‡åŒ–** ã‚’é©ç”¨ã—ã€ãã‚Œãã‚Œã®ç²¾åº¦ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚  \n",
        "L1æ­£å‰‡åŒ–ã¯ä¸€éƒ¨ã®ä¿‚æ•°ã‚’ã‚¼ãƒ­ã«ã™ã‚‹ã“ã¨ã§ã€ç‰¹å¾´é‡é¸æŠã«åŠ¹æœãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "L2æ­£å‰‡åŒ–ã¯å…¨ã¦ã®ä¿‚æ•°ã‚’å°ã•ãã—ã¦ã€éå­¦ç¿’ã‚’æŠ‘åˆ¶ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXfq0o9kHNRN"
      },
      "outputs": [],
      "source": [
        "# L1 æ­£å‰‡åŒ–ï¼špenalty='l1', solver='liblinear'\n",
        "score_l1 = cross_val_score(\n",
        "    LogisticRegression(penalty='l1', solver='liblinear', C=0.5, max_iter=200, random_state=42),\n",
        "    X_fe_train, y_fe_train, cv=5\n",
        ").mean()\n",
        "\n",
        "# L2 æ­£å‰‡åŒ–ï¼špenalty='l2', solver='liblinear'\n",
        "score_l2 = cross_val_score(\n",
        "    LogisticRegression(penalty='l2', solver='liblinear', C=0.5, max_iter=200, random_state=42),\n",
        "    X_fe_train, y_fe_train, cv=5\n",
        ").mean()\n",
        "\n",
        "print(f\"L1 score: {score_l1:.3f}\")\n",
        "print(f\"L2 score: {score_l2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5tCPioNT9gv"
      },
      "source": [
        "### 4.1.3 ğŸ§® Checking Feature Coefficients (Importance) in Logistic Regression / ç‰¹å¾´é‡ã®ä¿‚æ•°ï¼ˆé‡ã¿ï¼‰ç¢ºèª  \n",
        "Let's now examine the **coefficients of the logistic regression model** to understand which features contribute the most to survival prediction.\n",
        "\n",
        "ã“ã“ã§ã¯ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã® **ä¿‚æ•°ï¼ˆé‡ã¿ï¼‰** ã‚’ç¢ºèªã—ã€ã©ã®ç‰¹å¾´é‡ãŒç”Ÿå­˜äºˆæ¸¬ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbE7M2jDOifV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Save feature names / ç‰¹å¾´é‡åã‚’ä¿å­˜\n",
        "feature_names = df_fe.drop(columns=['PassengerId','Survived']).columns\n",
        "\n",
        "# Get model coefficients / ãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•°ã‚’å–å¾—\n",
        "coef = lr_fe.coef_[0]\n",
        "\n",
        "# Create DataFrame / ç‰¹å¾´é‡åã¨å¯¾å¿œä»˜ã‘ã¦DataFrameã«ã™ã‚‹\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coef\n",
        "})\n",
        "\n",
        "# Sort by absolute value / çµ¶å¯¾å€¤ã®å¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "print(coef_df.sort_values(by='Coefficient', key=abs, ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeW5h7TUUl3u"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨\n",
        "From the sorted coefficients, we can see the most influential features.  \n",
        "In particular:\n",
        "\n",
        "- `Sex` has the largest coefficient â†’ gender is the strongest predictor of survival.\n",
        "- Some deck-related features like `Deck_E`, `Deck_G`, and `Deck_C` also have large weights.\n",
        "- `Family`, the new feature we added, has a small coefficient (â‰ˆ -0.11), suggesting its impact is limited in the current model.\n",
        "\n",
        "ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸä¿‚æ•°ã‹ã‚‰ã€æœ€ã‚‚å½±éŸ¿ã®å¤§ãã„ç‰¹å¾´é‡ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- `Sex`ï¼ˆæ€§åˆ¥ï¼‰ãŒæœ€ã‚‚å¤§ããªä¿‚æ•° â†’ ç”Ÿå­˜ç‡äºˆæ¸¬ã«æœ€ã‚‚å¼·ãå¯„ä¸\n",
        "- `Deck_E` ã‚„ `Deck_G` ãªã©ã€å®¢å®¤éšå±¤ã®ç‰¹å¾´ã‚‚é«˜ã„å½±éŸ¿åº¦\n",
        "- æ–°ãŸã«è¿½åŠ ã—ãŸ `Family` ã¯ä¿‚æ•°ãŒå°ã•ãï¼ˆç´„ -0.11ï¼‰ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯å½±éŸ¿ãŒé™å®šçš„ã§ã‚ã‚‹å¯èƒ½æ€§\n",
        "---\n",
        "#### â“ Why is the impact of `Family` small?\n",
        "\n",
        "The `Family` feature may show low importance for several reasons:\n",
        "\n",
        "- The effect of `Family` might be **nonlinear**: both solo travelers and large families may have lower survival rates.\n",
        "- Logistic regression, being a linear model, may not capture such nonlinear effects well.\n",
        "\n",
        "You may consider the following approaches:\n",
        "\n",
        "- Using `FamilySize` bins (e.g., Small, Medium, Large)\n",
        "- Adding interaction terms\n",
        "- Switching to tree-based models (e.g., RandomForest)\n",
        "\n",
        "---\n",
        "\n",
        "#### â“ `Family` ã®å½±éŸ¿ãŒå°ã•ã„ç†ç”±ã¯ï¼Ÿ\n",
        "\n",
        "`Family` ã®å½±éŸ¿ãŒå°ã•ãè¦‹ãˆã‚‹ã®ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç†ç”±ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "- `Family` ã¨ç”Ÿå­˜ç‡ã®é–¢ä¿‚ãŒ **éç·šå½¢** ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€1äººæ—…ã‚„å¤§å®¶æ—ã§ã¯ç”Ÿå­˜ç‡ãŒä½ã„å‚¾å‘ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n",
        "- ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ã‚ˆã†ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ã“ã†ã—ãŸéç·šå½¢ã®å½±éŸ¿ã‚’ã†ã¾ãæ‰ãˆã‚‰ã‚Œãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "å¯¾å¿œç­–ã¨ã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå·¥å¤«ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "- `FamilySize` ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã™ã‚‹ï¼ˆä¾‹ï¼š1äººï¼Soloã€2â€“4äººï¼Mediumã€5äººä»¥ä¸Šï¼Largeï¼‰\n",
        "- ä»–ã®ç‰¹å¾´é‡ã¨ã®äº¤äº’ä½œç”¨é …ã‚’è¿½åŠ ã™ã‚‹\n",
        "- éç·šå½¢ã®é–¢ä¿‚ã‚’æ‰ãˆã‚„ã™ã„æœ¨ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼šRandomForestï¼‰ã«åˆ‡ã‚Šæ›¿ãˆã‚‹\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulbty8rXY2LS"
      },
      "source": [
        "### 4.1.4  Model with Random Forest Classifier  \n",
        "We now train a **Random Forest classifier** using the same features including `Family`, and compare its performance with logistic regression.\n",
        "Random Forest is a tree-based ensemble model, which can capture **nonlinear relationships and feature interactions** better than logistic regression.\n",
        "This allows features like `Family`, which might have a nonlinear effect, to contribute more effectively.  \n",
        "\n",
        "---\n",
        "\n",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡å™¨ã‚’ä½¿ç”¨ã—ã¦ã€`Family` ã‚’å«ã‚€ç‰¹å¾´é‡ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨ã®æ€§èƒ½æ¯”è¼ƒã‚’è¡Œã„ã¾ã™ã€‚    \n",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¯æœ¨æ§‹é€ ã«åŸºã¥ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€\n",
        "éç·šå½¢ãªé–¢ä¿‚æ€§ã‚„ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚ˆã‚Šã‚‚é©åˆ‡ã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚    \n",
        "ãã®ãŸã‚ã€`Family` ã®ã‚ˆã†ãªéç·šå½¢ãªå½±éŸ¿ã‚’æŒã¤ç‰¹å¾´é‡ãŒåŠ¹æœçš„ã«åƒãå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oErV8hIJEzNV"
      },
      "outputs": [],
      "source": [
        "# Train Random Forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å­¦ç¿’\n",
        "rfc_fe = RandomForestClassifier(\n",
        "    max_depth=6, min_samples_leaf=5, n_estimators=100,\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "rfc_fe.fit(X_fe_train, y_fe_train)\n",
        "\n",
        "# Print accuracy / ç²¾åº¦è¡¨ç¤º\n",
        "print('Train Score: {}'.format(round(rfc_fe.score(X_fe_train, y_fe_train), 3)))\n",
        "print(' Test Score: {}'.format(round(rfc_fe.score(X_fe_valid, y_fe_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xseyvKt9bnt0"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨\n",
        "From the above comparison, we can observe the following:\n",
        "\n",
        "- **L2-regularized logistic regression** showed the best validation performance (CV â‰ˆ 0.812).\n",
        "- **Random Forest** slightly overfits: higher train score (0.859) but lower validation score (0.799).\n",
        "- **L1-regularized logistic regression** performed slightly worse, suggesting that some weak features might be useful.\n",
        "\n",
        "ã“ã‚Œã‚‰ã®æ¯”è¼ƒã‹ã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç¤ºå”†ãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "- **L2æ­£å‰‡åŒ–ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°** ãŒæœ€ã‚‚é«˜ã„æ¤œè¨¼ã‚¹ã‚³ã‚¢ï¼ˆCV â‰ˆ 0.812ï¼‰ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚\n",
        "- **ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ** ã¯å­¦ç¿’ã‚¹ã‚³ã‚¢ãŒé«˜ã„ä¸€æ–¹ã§ã€æ¤œè¨¼ã‚¹ã‚³ã‚¢ãŒã‚„ã‚„ä½ä¸‹ã—ã¦ãŠã‚Šã€ã‚„ã‚„éå­¦ç¿’æ°—å‘³ã§ã™ã€‚\n",
        "- **L1æ­£å‰‡åŒ–** ã®ç²¾åº¦ã¯ã‚ãšã‹ã«åŠ£ã‚Šã€ä¸€éƒ¨ã®å¼±ã„ç‰¹å¾´ã‚‚å®Ÿã¯æœ‰åŠ¹ã ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVk81SsBwLsW"
      },
      "source": [
        "## 4.2 Extracting Titles from Names / æ•¬ç§°ï¼ˆTitleï¼‰ã®æŠ½å‡º\n",
        "We extract **titles (e.g., Mr, Mrs, Miss)** from the `Name` column using regular expressions.  \n",
        "These titles can represent social status, gender, or age group, and may be useful for prediction.\n",
        "\n",
        "`Name` åˆ—ã‹ã‚‰æ•¬ç§°ï¼ˆä¾‹ï¼šMr, Mrs, Missãªã©ï¼‰ã‚’æ­£è¦è¡¨ç¾ã‚’ç”¨ã„ã¦æŠ½å‡ºã—ã¾ã™ã€‚  \n",
        "æ•¬ç§°ã¯ç¤¾ä¼šçš„åœ°ä½ãƒ»æ€§åˆ¥ãƒ»å¹´é½¢å±¤ãªã©ã‚’è¡¨ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€äºˆæ¸¬ã«å½¹ç«‹ã¤ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATnbyUUr7mpE"
      },
      "outputs": [],
      "source": [
        "df_fe1 = df_fe.copy() ## + Title features\n",
        "df_fe1_test = df_fe_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgv1vedgVi8_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to extract title / æ•¬ç§°ï¼ˆTitleï¼‰ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°\n",
        "def extract_title(name):\n",
        "    title_search = re.search(r' ([A-Za-z]+)\\.', name)  # Match pattern like \" Mr.\" / ã€Œç©ºç™½ï¼‹è‹±å­—ï¼‹ãƒ‰ãƒƒãƒˆã€ã«ãƒãƒƒãƒã•ã›ã‚‹\n",
        "    if title_search:\n",
        "        return title_search.group(1)  # Return only the title part / æ•¬ç§°ã®éƒ¨åˆ†ã®ã¿è¿”ã™\n",
        "    return \"\"  # Return empty string if no match / æ•¬ç§°ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™\n",
        "\n",
        "# Combine train and test data (need Name column) / è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆNameåˆ—ãŒå¿…è¦ï¼‰\n",
        "all_name = pd.concat([df, df_test], axis=0)\n",
        "\n",
        "# Apply the function to extract Title from Name / Nameã‹ã‚‰Titleã‚’æŠ½å‡ºã—ã¦æ–°ã—ã„åˆ—ã‚’ä½œæˆ\n",
        "all_name['Title'] = all_name['Name'].apply(extract_title)\n",
        "\n",
        "# Check unique titles extracted / æŠ½å‡ºã•ã‚ŒãŸæ•¬ç§°ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã‚’ç¢ºèª\n",
        "print(all_name['Title'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9BZiq5hdd5n"
      },
      "source": [
        "### 4.2.1 ğŸ” Count Title Frequency / æ•¬ç§°ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\n",
        "We count how frequently each title appears.  \n",
        "This helps us identify **rare titles** that should be grouped into a single category (e.g., 'Rare').\n",
        "\n",
        "å„æ•¬ç§°ã®å‡ºç¾å›æ•°ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã€  \n",
        "**å‡ºç¾æ•°ãŒå°‘ãªã„ãƒ¬ã‚¢ãªæ•¬ç§°** ã‚’è¦‹ã¤ã‘ã€1ã¤ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆ'Rare'ï¼‰ã«ã¾ã¨ã‚ã‚‹ãŸã‚ã®æº–å‚™ãŒã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NziVJS3KBHsn"
      },
      "outputs": [],
      "source": [
        "# Count how many times each title appears / å„æ•¬ç§°ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "title_counts = all_name['Title'].value_counts()\n",
        "print(title_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUEHVkrld3GK"
      },
      "source": [
        "### 4.2.2 ğŸ© Title Feature Cleaning & One-Hot Encoding / æ•¬ç§°ï¼ˆTitleï¼‰ã®æ•´å½¢ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°  \n",
        "We clean and group the extracted `Title` values from the Name column.  \n",
        "Some titles are equivalent (e.g., 'Mlle' â‰ˆ 'Miss'), and rare titles are grouped as 'Rare' to reduce dimensionality.  \n",
        "Afterward, we perform one-hot encoding and merge them back into the training and test datasets.\n",
        "\n",
        "`Name`åˆ—ã‹ã‚‰æŠ½å‡ºã—ãŸ`Title`ï¼ˆæ•¬ç§°ï¼‰ã‚’æ•´å½¢ã—ã€  \n",
        "åŒç¾©ã®ã‚‚ã®ã‚’çµ±åˆï¼ˆä¾‹ï¼š'Mlle' â‰ˆ 'Miss'ï¼‰ã—ã€å‡ºç¾é »åº¦ã®å°‘ãªã„æ•¬ç§°ã¯ 'Rare' ã«ã¾ã¨ã‚ã¦æ¬¡å…ƒæ•°ã‚’æ¸›ã‚‰ã—ã¾ã™ã€‚  \n",
        "ãã®å¾Œã€ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€è¨“ç·´ï¼ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«çµåˆã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0EAiVa42pki"
      },
      "outputs": [],
      "source": [
        "# ä¸€éƒ¨ã®æ•¬ç§°ã‚’çµ±åˆ\n",
        "all_name['Title'] = all_name['Title'].replace({\n",
        "    'Mlle': 'Miss',\n",
        "    'Ms': 'Miss',\n",
        "    'Mme': 'Mrs'\n",
        "})\n",
        "\n",
        "# é »åº¦ã‚’å†è¨ˆç®—\n",
        "title_counts = all_name['Title'].value_counts()\n",
        "\n",
        "# å‡ºç¾é »åº¦ãŒå°‘ãªã„æ•¬ç§°ã‚’ã€ŒRareã€ã¨ã—ã¦çµ±åˆ\n",
        "rare_titles = title_counts[title_counts < 10].index       #10æœªæº€ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å€¤ï¼ˆæ•¬ç§°ã®åå‰ï¼‰ã ã‘ã‚’å–ã‚Šå‡ºã™\n",
        "\n",
        "all_name['Title'] = all_name['Title'].replace(rare_titles, 'Rare')\n",
        "\n",
        "# æ•¬ç§°ã®ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–ã‚’ã—ã€ãƒ€ãƒŸãƒ¼å¤‰æ•°ã‚’True/Falseã‹ã‚‰0/1ã«å¤‰æ›\n",
        "title_ohe = pd.get_dummies(all_name['Title'], prefix='Title').astype(int)\n",
        "\n",
        "# df_fe1/df_fe1_test ã«è¡Œæ•°ã‚’åˆã‚ã›ã¦çµåˆ\n",
        "df_fe1 = pd.concat([\n",
        "    df_fe1.reset_index(drop=True),\n",
        "    title_ohe.iloc[:len(df)].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "df_fe1_test = pd.concat([\n",
        "    df_fe1_test.reset_index(drop=True),\n",
        "    title_ohe.iloc[len(df):].reset_index(drop=True)\n",
        "], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT4BCu4Dfa45"
      },
      "source": [
        "### 4.2.3 ğŸ¤– Logistic Regression with Title Feature / æ•¬ç§°ã‚’åŠ ãˆãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«  \n",
        "We now retrain the logistic regression model using the dataset that includes the newly added `Title` features.  \n",
        "This allows us to check whether including titles improves model performance.\n",
        "\n",
        "æ–°ãŸã«è¿½åŠ ã—ãŸ `Title` ç‰¹å¾´é‡ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¾ã™ã€‚  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€æ•¬ç§°ã®è¿½åŠ ãŒãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eyDu0GPgd29"
      },
      "outputs": [],
      "source": [
        "# Separate features and target / èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã‚’åˆ†å‰²\n",
        "X_fe1 = df_fe1.drop(columns=['PassengerId', 'Survived'])\n",
        "y_fe1 = df_fe1['Survived']\n",
        "\n",
        "X_fe1_test = df_fe1_test.drop(columns=['PassengerId'])\n",
        "\n",
        "# Split into training and validation sets / è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
        "X_fe1_train, X_fe1_valid, y_fe1_train, y_fe1_valid = train_test_split(\n",
        "    X_fe1, y_fe1, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression / ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
        "lr_fe1 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe1.fit(X_fe1_train, y_fe1_train)\n",
        "\n",
        "# Accuracy on train and validation / è¨“ç·´ãƒ»æ¤œè¨¼ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º\n",
        "print('Train Score: {}'.format(round(lr_fe1.score(X_fe1_train, y_fe1_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe1.score(X_fe1_valid, y_fe1_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_TlIe-_fxkA"
      },
      "source": [
        "Adding the `Title` feature improved the model's validation score from 0.812 to 0.821.  \n",
        "This suggests that **titles carry meaningful information** related to passenger survival â€” such as gender, age group, and social status.\n",
        "\n",
        "`Title` ç‰¹å¾´é‡ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€æ¤œè¨¼ã‚¹ã‚³ã‚¢ãŒ 0.812 â†’ 0.821 ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸã€‚  \n",
        "ã“ã‚Œã¯ã€æ•¬ç§°ãŒæ€§åˆ¥ãƒ»å¹´é½¢å±¤ãƒ»ç¤¾ä¼šçš„åœ°ä½ãªã©ã€**ç”Ÿå­˜ã«é–¢ä¿‚ã™ã‚‹æœ‰ç›Šãªæƒ…å ±ã‚’å«ã‚“ã§ã„ã‚‹**ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPhQkgVBLl_g"
      },
      "source": [
        "### 4.2.4ğŸŒ² Random Forest with Title Feature / æ•¬ç§°ã‚’åŠ ãˆãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ  \n",
        "We retrain the Random Forest model using the updated dataset that includes the `Title` features.  \n",
        "This allows us to compare its performance with logistic regression and see whether nonlinear models benefit more from the new features.\n",
        "\n",
        "`Title` ã‚’å«ã‚ãŸæ›´æ–°æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¾ã™ã€‚  \n",
        "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨ã®æ¯”è¼ƒã‚„ã€éç·šå½¢ãƒ¢ãƒ‡ãƒ«ãŒã“ã®ç‰¹å¾´é‡ã®æ©æµã‚’ã‚ˆã‚Šå—ã‘ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MMkuCVBCAEW"
      },
      "outputs": [],
      "source": [
        "# Train Random Forest with Title features / Titleã‚’åŠ ãˆãŸãƒ‡ãƒ¼ã‚¿ã§RFå­¦ç¿’\n",
        "rfc_fe1 = RandomForestClassifier(\n",
        "    max_depth=6, min_samples_leaf=5,\n",
        "    n_estimators=100, n_jobs=-1, random_state=42\n",
        ")\n",
        "rfc_fe1.fit(X_fe1_train, y_fe1_train)\n",
        "\n",
        "# Accuracy / ç²¾åº¦\n",
        "print('Train Score: {}'.format(round(rfc_fe1.score(X_fe1_train, y_fe1_train), 3)))\n",
        "print(' Test Score: {}'.format(round(rfc_fe1.score(X_fe1_valid, y_fe1_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHc4VvuogNt3"
      },
      "source": [
        "### ğŸ” What We found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "Both models improved with the addition of the `Title` feature.  \n",
        "Logistic Regression slightly outperforms Random Forest on the validation score (0.821 vs 0.817),  \n",
        "but Random Forest has a higher training score, which may suggest mild overfitting.\n",
        "\n",
        "ä¸¡ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ã€`Title` ç‰¹å¾´é‡ã®è¿½åŠ ã«ã‚ˆã£ã¦ã‚¹ã‚³ã‚¢ãŒæ”¹å–„ã—ã¾ã—ãŸã€‚  \n",
        "æ¤œè¨¼ã‚¹ã‚³ã‚¢ã§ã¯ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãŒã‚ãšã‹ã«å„ªã‚Œã¦ã„ã¾ã™ï¼ˆ0.821 vs 0.817ï¼‰ãŒã€  \n",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¯è¨“ç·´ã‚¹ã‚³ã‚¢ãŒé«˜ãã€ã‚„ã‚„éå­¦ç¿’ã®å…†å€™ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibqvzdqXXzOY"
      },
      "source": [
        "## 4.3 Ticket Feature / ãƒã‚±ãƒƒãƒˆã®å‡¦ç†\n",
        "We examine the `Ticket` feature to extract useful information.  \n",
        "Ticket numbers often contain prefixes that indicate ticket type or cabin location, which might affect survival rates.\n",
        "\n",
        "`Ticket`ï¼ˆãƒã‚±ãƒƒãƒˆç•ªå·ï¼‰ã‚’èª¿æŸ»ã—ã€ãã“ã‹ã‚‰æœ‰ç›Šãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã™ã€‚  \n",
        "ãƒã‚±ãƒƒãƒˆç•ªå·ã«ã¯ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆæ¥é ­è¾ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šãã€ã“ã‚ŒãŒãƒã‚±ãƒƒãƒˆã®ç¨®é¡ã‚„ã‚­ãƒ£ãƒ“ãƒ³ã®ä½ç½®ã‚’ç¤ºã—ã€ç”Ÿå­˜ç‡ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqxTpsvfSJ1a"
      },
      "outputs": [],
      "source": [
        "df_fe2 = df_fe1.copy() ## + Ticket features\n",
        "df_fe2_test = df_fe1_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-j57jLFkciv"
      },
      "source": [
        "### 4.3.1 Explore Frequent Ticket Values / ãƒã‚±ãƒƒãƒˆç•ªå·ã®å‡ºç¾é »åº¦ã‚’èª¿æŸ»ã™ã‚‹\n",
        "\n",
        "Some passengers appear to have shared the same ticket number, possibly indicating group bookings (e.g., families or friends).  \n",
        "By checking the most common ticket numbers, we can get a sense of shared bookings or duplicates in the data.\n",
        "\n",
        "è¤‡æ•°ã®ä¹—å®¢ãŒåŒã˜ãƒã‚±ãƒƒãƒˆç•ªå·ã‚’æŒã£ã¦ã„ã‚‹å ´åˆãŒã‚ã‚Šã€å®¶æ—ã‚„å‹äººåŒå£«ã®ã‚°ãƒ«ãƒ¼ãƒ—ä¹—èˆ¹ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "é »å‡ºã™ã‚‹ãƒã‚±ãƒƒãƒˆç•ªå·ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã€ã“ã†ã—ãŸã‚°ãƒ«ãƒ¼ãƒ—ã®å­˜åœ¨ã‚„ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ã®å¯èƒ½æ€§ã‚’æŠŠæ¡ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i6nExiWeUF-"
      },
      "outputs": [],
      "source": [
        "# Show the most common Ticket values / ãƒã‚±ãƒƒãƒˆç•ªå·ã®é »å‡ºå€¤ã‚’è¡¨ç¤º\n",
        "print(df['Ticket'].value_counts().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAXPZN00mLJY"
      },
      "source": [
        "### 4.3.2 ğŸŸï¸ Create Group Size Feature Based on Ticket / ãƒã‚±ãƒƒãƒˆç•ªå·ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹\n",
        "\n",
        "Passengers who share the same ticket number may have traveled together as a group (e.g., family or friends).  \n",
        "We compute the number of passengers per ticket and use it to create two new features:\n",
        "- `TicketGroupSize`: the size of the group (capped at 8 to reduce the effect of outliers)\n",
        "- `IsGroup`: binary flag indicating whether the passenger was in a group (size > 1)\n",
        "\n",
        "åŒã˜ãƒã‚±ãƒƒãƒˆç•ªå·ã‚’æŒã¤ä¹—å®¢ã¯ã€å®¶æ—ã‚„å‹äººãªã©ã®ã‚°ãƒ«ãƒ¼ãƒ—ã§ä¹—èˆ¹ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ãã“ã§ã€ãƒã‚±ãƒƒãƒˆã”ã¨ã®äººæ•°ã‚’é›†è¨ˆã—ã€ä»¥ä¸‹ã®2ã¤ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ï¼š\n",
        "- `TicketGroupSize`: ãƒã‚±ãƒƒãƒˆã”ã¨ã®äººæ•°ï¼ˆå¤–ã‚Œå€¤å¯¾ç­–ã¨ã—ã¦æœ€å¤§8ã«åˆ¶é™ï¼‰\n",
        "- `IsGroup`: ä¹—å®¢ãŒã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆ2äººä»¥ä¸Šï¼‰ã«å±ã—ã¦ã„ã‚‹ã‹ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vz0MkNnMvAl"
      },
      "outputs": [],
      "source": [
        "# Combine training and test data to compute ticket frequencies / ãƒã‚±ãƒƒãƒˆã®å‡ºç¾å›æ•°ã‚’å…¨ä½“ã§è¨ˆç®—\n",
        "all_data = pd.concat([df, df_test], axis=0)\n",
        "ticket_counts = all_data['Ticket'].value_counts()\n",
        "\n",
        "# Map the ticket count to each passenger / å„ä¹—å®¢ã«ãƒã‚±ãƒƒãƒˆã®äººæ•°ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
        "df_fe2['TicketGroupSize'] = df['Ticket'].map(ticket_counts).fillna(1)\n",
        "df_fe2_test['TicketGroupSize'] = df_test['Ticket'].map(ticket_counts).fillna(1)\n",
        "\n",
        "# Limit maximum group size to 8 to avoid outliers / å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’é¿ã‘ã‚‹ãŸã‚ã€æœ€å¤§ã‚’8ã«åˆ¶é™\n",
        "df_fe2['TicketGroupSize'] = df_fe2['TicketGroupSize'].clip(upper=8)\n",
        "df_fe2_test['TicketGroupSize'] = df_fe2_test['TicketGroupSize'].clip(upper=8)\n",
        "\n",
        "# Create binary feature: Is the passenger in a group? / ã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã—ã¦ã„ã‚‹ã‹ï¼ˆ2äººä»¥ä¸Šï¼‰\n",
        "df_fe2['IsGroup'] = (df_fe2['TicketGroupSize'] > 1).astype(int)\n",
        "df_fe2_test['IsGroup'] = (df_fe2_test['TicketGroupSize'] > 1).astype(int)\n",
        "\n",
        "# Check survival rate by ticket group size / TicketGroupSizeã”ã¨ã®ç”Ÿå­˜ç‡ã‚’ç¢ºèª\n",
        "print(df_fe2[['TicketGroupSize', 'Survived']].groupby('TicketGroupSize').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dou7GkJ2qkyE"
      },
      "source": [
        " ### ğŸ“Š 4.3.3 Ticket Group Size Distribution / ãƒã‚±ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã®åˆ†å¸ƒ  \n",
        " We plot the distribution of `TicketGroupSize` to understand how common different group sizes are among passengers.  \n",
        "This helps us verify whether most people traveled alone or with others.\n",
        "\n",
        "`TicketGroupSize`ï¼ˆãƒã‚±ãƒƒãƒˆã‚’å…±æœ‰ã™ã‚‹äººæ•°ï¼‰ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§ã€  \n",
        "ä¹—å®¢ãŒä¸€äººæ—…ãªã®ã‹è¤‡æ•°äººã§è¡Œå‹•ã—ã¦ã„ãŸã®ã‹ã®å‚¾å‘ã‚’æŠŠæ¡ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPGAdl4cstS5"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of ticket group sizes / ãƒã‚±ãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—äººæ•°ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–\n",
        "df_fe2['TicketGroupSize'].hist(bins=20)\n",
        "plt.title('Ticket Group Size Distribution')  # ã‚¿ã‚¤ãƒˆãƒ«\n",
        "plt.xlabel('Group Size')                    # xè»¸ï¼šäººæ•°\n",
        "plt.ylabel('Frequency')                     # yè»¸ï¼šé »åº¦\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9wQ2NXcq6R3"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨  \n",
        "The histogram shows that most passengers had a group size of 1 or 2,  \n",
        "suggesting many were traveling alone or with just one companion.  \n",
        "This supports the idea that group-related features might reveal social structures affecting survival.\n",
        "\n",
        "ã“ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‹ã‚‰ã€å¤šãã®ä¹—å®¢ãŒã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º1ã€œ2ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚  \n",
        "ã“ã‚Œã¯ä¸€äººæ—…ã¾ãŸã¯å°‘äººæ•°ã§ã®è¡Œå‹•ãŒå¤šã‹ã£ãŸã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€  \n",
        "ã‚°ãƒ«ãƒ¼ãƒ—ã«é–¢ã™ã‚‹ç‰¹å¾´é‡ãŒç”Ÿå­˜ã«é–¢ä¿‚ã™ã‚‹ç¤¾ä¼šçš„æ§‹é€ ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGiKJM9dtLmU"
      },
      "source": [
        "### 4.3.4 Logistic Regression with TicketGroupSize and IsGroup /\n",
        "### TicketGroupSizeãƒ»IsGroup ã‚’åŠ ãˆãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yuLJUQPSZuH"
      },
      "outputs": [],
      "source": [
        "X_fe2 = df_fe2.drop(columns=['PassengerId','Survived'])\n",
        "y_fe2 = df_fe2['Survived']\n",
        "\n",
        "X_fe2_test =  df_fe2_test.drop(columns=['PassengerId'])\n",
        "\n",
        "X_fe2_train, X_fe2_valid, y_fe2_train, y_fe2_valid = train_test_split(X_fe2, y_fe2, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_fe2 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe2.fit(X_fe2_train, y_fe2_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(lr_fe2.score(X_fe2_train, y_fe2_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe2.score(X_fe2_valid, y_fe2_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdY5UL1ssjIJ"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨\n",
        "Adding `TicketGroupSize` and `IsGroup` did not significantly change the validation score,  \n",
        "but slightly improved the training accuracy.  \n",
        "This suggests these features may provide helpful context, especially when used with other variables like `Title`.\n",
        "\n",
        "`TicketGroupSize` ã¨ `IsGroup` ã‚’è¿½åŠ ã—ã¦ã‚‚æ¤œè¨¼ã‚¹ã‚³ã‚¢ã«ã¯å¤§ããªå¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€  \n",
        "è¨“ç·´ã‚¹ã‚³ã‚¢ãŒã‚ãšã‹ã«å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚  \n",
        "ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯å˜ç‹¬ã§ã¯å½±éŸ¿ãŒå°ã•ãã¦ã‚‚ã€`Title` ãªã©ä»–ã®ç‰¹å¾´é‡ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€èƒŒæ™¯æƒ…å ±ã¨ã—ã¦å½¹ç«‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpluHVcRE1BS"
      },
      "source": [
        "## ğŸ§© 4.4 Create Age Categories / å¹´é½¢ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã™ã‚‹  \n",
        "ğŸ¯ Why Categorize Age? / ãªãœå¹´é½¢ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã™ã‚‹ã®ã‹ï¼Ÿ\n",
        "\n",
        "- In models like **logistic regression**, numerical features such as `Age` are interpreted linearly.  \n",
        "  This means the model assumes that survival probability changes steadily with age.\n",
        "\n",
        "- However, in reality, survival rates often vary **non-linearly across age groups** â€”  \n",
        "  for example, children, young adults, and seniors may have very different outcomes.\n",
        "\n",
        "- By converting age into **categories**, we allow the model to explicitly learn patterns specific to each age group.  \n",
        "  This can improve interpretability and performance, especially for tree-based and linear models.\n",
        "\n",
        "---\n",
        "\n",
        "- ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€`Age` ã®ã‚ˆã†ãªæ•°å€¤ç‰¹å¾´é‡ã¯ç·šå½¢çš„ã«è§£é‡ˆã•ã‚Œã¾ã™ã€‚  \n",
        "  ã¤ã¾ã‚Šã€å¹´é½¢ãŒä¸ŠãŒã‚‹ã»ã©ç”Ÿå­˜ç‡ãŒä¸€æ§˜ã«å¤‰åŒ–ã™ã‚‹ã€ã¨ã„ã†å‰æã«ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "- ã—ã‹ã—å®Ÿéš›ã«ã¯ã€ç”Ÿå­˜ç‡ã¯ã€Œå­ã©ã‚‚ã€ã€Œè‹¥è€…ã€ã€Œé«˜é½¢è€…ã€ãªã©ã® **å¹´é½¢å±¤ã”ã¨ã«éç·šå½¢ã«å¤‰åŒ–** ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- å¹´é½¢ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯ **å„å¹´é½¢å±¤ã«ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³** ã‚’æ˜ç¤ºçš„ã«å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚  \n",
        "  ç‰¹ã«ã€æ±ºå®šæœ¨ç³»ã‚„ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjKIZYjswMnP"
      },
      "source": [
        "### 4.4.1ğŸ‘¶ğŸ‘¨â€ğŸ¦°ğŸ§“ Categorizing Age into Groups / `Age` ã‚’å¹´é½¢å±¤ã§ã‚«ãƒ†ã‚´ãƒªåŒ–\n",
        "\n",
        "We convert the continuous `Age` feature into 3 categories:\n",
        "- `Child` for ages 0â€“10\n",
        "- `Adult` for ages 10â€“50\n",
        "- `Senior` for ages 50â€“100\n",
        "\n",
        "This helps the model understand non-linear survival patterns across different age groups.  \n",
        "We then apply one-hot encoding to make these categories usable in machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "`Age`ï¼ˆå¹´é½¢ï¼‰ã‚’é€£ç¶šå€¤ã®ã¾ã¾ä½¿ã†ã®ã§ã¯ãªãã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚«ãƒ†ã‚´ãƒªã«åˆ†ã‘ã¦æ‰±ã„ã¾ã™ï¼š\n",
        "\n",
        "- `Child`ï¼š0ã€œ10æ­³  \n",
        "- `Adult`ï¼š10ã€œ50æ­³  \n",
        "- `Senior`ï¼š50ã€œ100æ­³  \n",
        "\n",
        "ã“ã‚Œã¯ã€å¹´é½¢å±¤ã”ã¨ã«ç•°ãªã‚‹ç”Ÿå­˜ç‡å‚¾å‘ï¼ˆéç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã§ã™ã€‚  \n",
        "ãã®å¾Œã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã€ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§æ•°å€¤åŒ–ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McbWj9zBoB15"
      },
      "outputs": [],
      "source": [
        "# Copy the latest dataframe / æœ€æ–°ã®ç‰¹å¾´é‡ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
        "df_fe3 = df_fe2.copy()  # + AgeGroup features\n",
        "df_fe3_test = df_fe2_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t48SO8DxoiyQ"
      },
      "outputs": [],
      "source": [
        "# Combine Age from train and test to ensure consistent binning\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®Ageåˆ—ã‚’çµåˆã—ã€åŒã˜åŸºæº–ã§ãƒ“ãƒ³åˆ†ã‘ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "all_age = pd.concat([df_fe3['Age'], df_fe3_test['Age']], axis=0)\n",
        "\n",
        "# Create age group categories: Child (0â€“10), Adult (10â€“50), Senior (50â€“100)\n",
        "# å¹´é½¢ã‚’ã‚«ãƒ†ã‚´ãƒªã«å¤‰æ›ï¼šChildï¼ˆ0â€“10ï¼‰ã€Adultï¼ˆ10â€“50ï¼‰ã€Seniorï¼ˆ50â€“100ï¼‰\n",
        "age_group = pd.cut(all_age, bins=[0, 10, 50, 100], labels=['Child','Adult', 'Senior'])\n",
        "\n",
        "# Assign AgeGroup back to train and test data\n",
        "# AgeGroup ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æˆ»ã™\n",
        "df_fe3['AgeGroup'] = age_group[:len(df_fe3)].reset_index(drop=True)\n",
        "df_fe3_test['AgeGroup'] = age_group[len(df_fe3):].reset_index(drop=True)\n",
        "\n",
        "# Combine AgeGroup from both sets and one-hot encode\n",
        "# ä¸¡æ–¹ã® AgeGroup ã‚’çµåˆã—ã¦ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
        "all_group = pd.concat([df_fe3['AgeGroup'], df_fe3_test['AgeGroup']], axis=0)\n",
        "age_dummies = pd.get_dummies(all_group, columns=['AgeGroup'], prefix='AgeGroup').astype(int)\n",
        "\n",
        "# Add one-hot encoded columns back to original train and test sets\n",
        "# ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆåŒ–ã—ãŸ AgeGroup ã‚’å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ \n",
        "df_fe3 = pd.concat([df_fe3.reset_index(drop=True), age_dummies.iloc[:len(df_fe3)].reset_index(drop=True)], axis=1)\n",
        "df_fe3_test = pd.concat([df_fe3_test.reset_index(drop=True), age_dummies.iloc[len(df_fe3):].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Drop original AgeGroup categorical column\n",
        "# å…ƒã®ã‚«ãƒ†ã‚´ãƒªåˆ—ï¼ˆAgeGroupï¼‰ã¯å‰Šé™¤\n",
        "df_fe3.drop(columns=['AgeGroup'], inplace=True)\n",
        "df_fe3_test.drop(columns=['AgeGroup'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIC9Mmgz1U9-"
      },
      "source": [
        "### 4.4.2 Logistic Regression with AgeGroup / Age ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã—ã¦è¿½åŠ ã—ãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL5hXUHxZf0w"
      },
      "outputs": [],
      "source": [
        "X_fe3 = df_fe3.drop(columns=['PassengerId','Survived']).values\n",
        "y_fe3 = df_fe3['Survived'].values\n",
        "\n",
        "X_fe3_test =  df_fe3_test.drop(columns=['PassengerId']).values\n",
        "\n",
        "X_fe3_train, X_fe3_valid, y_fe3_train, y_fe3_valid = train_test_split(X_fe3, y_fe3, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_fe3 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe3.fit(X_fe3_train, y_fe3_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(lr_fe3.score(X_fe3_train, y_fe3_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe3.score(X_fe3_valid, y_fe3_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA4Wma3Z1ufi"
      },
      "source": [
        "### ğŸ” What We Found / è¦‹ãˆã¦ããŸã“ã¨\n",
        "We added age group features (Child, Adult, Senior) to capture potential non-linear effects of age on survival.  \n",
        "However, the validation score slightly decreased from 0.821 to 0.817, suggesting the new features did not significantly improve model performance.\n",
        "\n",
        "This may indicate that the model was already capturing the impact of age through other correlated features (e.g., Title or Family size),  \n",
        "or that the age groups chosen were not optimal.\n",
        "\n",
        "---\n",
        "\n",
        "ç”Ÿå­˜ç‡ã«å¯¾ã™ã‚‹å¹´é½¢ã®éç·šå½¢ãªå½±éŸ¿ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€Age ã‚’ã‚«ãƒ†ã‚´ãƒªåŒ–ã—ã¦è¿½åŠ ã—ã¾ã—ãŸï¼ˆChildã€Adultã€Seniorï¼‰ã€‚  \n",
        "ã—ã‹ã—ã€æ¤œè¨¼ã‚¹ã‚³ã‚¢ã¯ 0.821 ã‹ã‚‰ 0.817 ã«ã‚ãšã‹ã«ä½ä¸‹ã—ã€æ–°ã—ã„ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’æ˜ç¢ºã«æ”¹å–„ã—ãŸã¨ã¯è¨€ãˆã¾ã›ã‚“ã€‚\n",
        "\n",
        "ã“ã‚Œã¯ã€å¹´é½¢ã®å½±éŸ¿ãŒã™ã§ã«ä»–ã®é–¢é€£ç‰¹å¾´ï¼ˆä¾‹ï¼šTitleã‚„Family sizeï¼‰ã§æ‰ãˆã‚‰ã‚Œã¦ã„ãŸå¯èƒ½æ€§ã‚„ã€  \n",
        "å¹´é½¢å±¤ã®åŒºåˆ†ã‘ãŒæœ€é©ã§ãªã‹ã£ãŸå¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmVMprgwuKKq"
      },
      "source": [
        "## 4.5 ğŸ’° Transforming Fare for Stability and Discretization / Fareã®å®‰å®šåŒ–ã¨åŒºé–“åŒ–å‡¦ç†\n",
        "\n",
        "The Fare feature has two main challenges: zero values and a heavily skewed distribution.  \n",
        "These issues can negatively affect model training and prediction accuracy.  \n",
        "To solve these, we apply several data transformations.\n",
        "\n",
        "---\n",
        "\n",
        "Fareï¼ˆé‹è³ƒï¼‰ã«ã¯ã€Œ0ã®å€¤ã€ã¨ã€Œå³ã«å¤§ããæ­ªã‚“ã åˆ†å¸ƒã€ã¨ã„ã†2ã¤ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ã“ã‚Œã‚‰ã¯ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„äºˆæ¸¬ç²¾åº¦ã«æ‚ªå½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ãã“ã§ã€ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’è¡Œã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2gsEhbS6rOz"
      },
      "source": [
        "### 4.5.1 ğŸ“Š Visualizing Fare Distribution / Fareã®åˆ†å¸ƒã‚’å¯è¦–åŒ–\n",
        "\n",
        "We plot the distribution of the original `Fare` values using a histogram with KDE (kernel density estimate).  \n",
        "This helps us understand how skewed the feature is and why a transformation is needed.\n",
        "\n",
        "---\n",
        "\n",
        "å…ƒã® `Fare` ã®åˆ†å¸ƒã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‹ã‚«ãƒ¼ãƒãƒ«å¯†åº¦æ¨å®šï¼ˆKDEï¼‰ä»˜ãã§å¯è¦–åŒ–ã—ã¾ã™ã€‚  \n",
        "ã“ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€`Fare` ãŒã„ã‹ã« **å³ã«å¤§ããåã£ã¦ã„ã‚‹ã‹ï¼ˆå³è£¾ãŒé•·ã„ï¼‰** ã‚’ç¢ºèªã§ãã¾ã™ã€‚  \n",
        "ã“ã†ã—ãŸæ­ªã‚“ã åˆ†å¸ƒã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã„ã¦ç‰¹å®šã®å€¤ï¼ˆé«˜é¡é‹è³ƒï¼‰ã‚’éå‰°ã«é‡è¦–ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å¾Œã»ã©å¯¾æ•°å¤‰æ›ãªã©ã®å‡¦ç†ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hWG_Fdi3Y7L"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['Fare'], bins=50, kde=True)\n",
        "plt.title('Distribution of Fare')\n",
        "plt.xlabel('Fare')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRVVBNmJ66MK"
      },
      "source": [
        "### 4.5.2ğŸ’° Fare Preprocessing: Handling Skewed and Zero Values / Fareã®å‰å‡¦ç†ï¼šæ­ªã¿ã¨0å€¤ã®å¯¾å‡¦\n",
        "\n",
        "To improve model performance, we apply the following preprocessing steps to the `Fare` feature:\n",
        "\n",
        "- **Replace zero fares** with the smallest positive fare in the training data.  \n",
        "  This avoids issues during logarithmic transformation.\n",
        "- **Apply log transformation** using `log1p`, which computes `log(Fare + 1)`.  \n",
        "  This compresses large values and reduces skewness in the distribution.\n",
        "- **Discretize Fare into 4 quantile-based bins** using `pd.qcut`.  \n",
        "  This creates a new feature called `FareBand`, which categorizes passengers based on relative fare levels.\n",
        "\n",
        "---\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®å‘ä¸Šã®ãŸã‚ã€`Fare` ã«ä»¥ä¸‹ã®å‰å‡¦ç†ã‚’è¡Œã„ã¾ã—ãŸï¼š\n",
        "\n",
        "- **0å††ã®Fare** ã‚’ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸­ã® **æœ€å°ã®æ­£ã®Fareå€¤** ã«ç½®ãæ›ãˆã¾ã—ãŸã€‚  \n",
        "  ã“ã‚Œã«ã‚ˆã‚Šã€å¯¾æ•°å¤‰æ›æ™‚ã®ã‚¨ãƒ©ãƒ¼ï¼ˆlog(0)ï¼‰ã‚’é˜²ãã¾ã™ã€‚\n",
        "- **å¯¾æ•°å¤‰æ›**ï¼ˆ`np.log1p`ï¼‰ã‚’è¡Œã„ã€`Fare` ã®å¤§ããªå€¤ã‚’åœ§ç¸®ã—ã¦åˆ†å¸ƒã®æ­ªã¿ã‚’è»½æ¸›ã—ã¾ã—ãŸã€‚  \n",
        "  `np.log1p(x)` ã¯ `log(x + 1)` ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€0ã«ã‚‚å¯¾å¿œå¯èƒ½ã§ã™ã€‚\n",
        "- **ç­‰é »åº¦ã®4ã¤ã®åŒºé–“ï¼ˆå››åˆ†ä½ï¼‰** ã« `Fare` ã‚’åˆ†å‰²ã—ã€æ–°ãŸã« `FareBand` ã¨ã„ã†ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã—ãŸã€‚  \n",
        "  ã“ã‚Œã«ã‚ˆã‚Šã€ä¹—å®¢ã‚’ç›¸å¯¾çš„ãªé‹è³ƒãƒ¬ãƒ™ãƒ«ã§åˆ†é¡ã§ãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3F4yvvtuJxh"
      },
      "outputs": [],
      "source": [
        "df_fe4 = df_fe3.copy()  # + Fare features\n",
        "df_fe4_test = df_fe3_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDKrRSCbvjTU"
      },
      "outputs": [],
      "source": [
        "# Replace 0 fare values with the minimum positive fare in the dataset\n",
        "# Fare ãŒ 0 ã®ä¹—å®¢ã¯ã€ãƒ‡ãƒ¼ã‚¿ä¸­ã®æœ€å°ã®æ­£ã®é‹è³ƒã§ç½®ãæ›ãˆã‚‹\n",
        "min_fare = df_fe4[df_fe4['Fare'] > 0]['Fare'].min()\n",
        "\n",
        "df_fe4['Fare'] = df_fe4['Fare'].replace(0, min_fare)\n",
        "df_fe4_test['Fare'] = df_fe4_test['Fare'].replace(0, min_fare)\n",
        "\n",
        "# Apply log transformation (log1p) to reduce skewness and handle outliers\n",
        "# å¯¾æ•°å¤‰æ›ï¼ˆlog1pï¼‰ã‚’é©ç”¨ã—ã¦ã€åˆ†å¸ƒã®æ­ªã¿ã‚’è»½æ¸›ã—ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠ‘ãˆã‚‹\n",
        "df_fe4['Fare_log'] = np.log1p(df_fe4['Fare'])  # log(x + 1) is safe even for x = 0\n",
        "df_fe4_test['Fare_log'] = np.log1p(df_fe4_test['Fare'])\n",
        "\n",
        "# Discretize fare into 4 equal-sized quantile bins (FareBand)\n",
        "# é‹è³ƒã‚’4ã¤ã®ç­‰é »åº¦ãƒ“ãƒ³ï¼ˆå››åˆ†ä½ï¼‰ã«åˆ†å‰²ã—ã€FareBand ã¨ã„ã†ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’ä½œæˆ\n",
        "df_fe4['FareBand'] = pd.qcut(df_fe4['Fare'], q=4, labels=False)\n",
        "df_fe4_test['FareBand'] = pd.qcut(df_fe4_test['Fare'], q=4, labels=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzCG0wM7a9b"
      },
      "source": [
        "### 4.5.3 ğŸ” Visualizing Log-Transformed Fare / å¯¾æ•°å¤‰æ›å¾Œã®Fareåˆ†å¸ƒã‚’å¯è¦–åŒ–\n",
        "\n",
        "To check the effect of the log transformation, we plot the histogram of `log1p(Fare)`.  \n",
        "The transformation compresses the extreme values and helps the feature become more normally distributed, which is better for many models such as logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "`log1p(Fare)` ã«ã‚ˆã‚‹ **å¯¾æ•°å¤‰æ›ã®åŠ¹æœã‚’ç¢ºèª**ã™ã‚‹ãŸã‚ã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’æç”»ã—ã¾ã™ã€‚  \n",
        "ã“ã®å¤‰æ›ã«ã‚ˆã‚Šã€é‹è³ƒã®æ¥µç«¯ã«é«˜ã„å€¤ãŒåœ§ç¸®ã•ã‚Œã€åˆ†å¸ƒãŒã‚ˆã‚Šæ­£è¦åˆ†å¸ƒã«è¿‘ã¥ãã¾ã™ã€‚  \n",
        "ã“ã‚Œã¯ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦æœ‰åˆ©ã«åƒãã“ã¨ãŒå¤šã„ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELTVDz6z3n52"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(np.log1p(df_fe4['Fare']), bins=50, kde=True)\n",
        "plt.title('Log-Transformed Fare Distribution')\n",
        "plt.xlabel('log1p(Fare)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeJIWole7wVj"
      },
      "source": [
        "### 4.5.4 âš–ï¸ Standardizing Fare / Fare ã®æ¨™æº–åŒ–\n",
        "\n",
        "To prepare for models that are sensitive to feature scales (such as SVM or linear regression),  \n",
        "we apply **Z-score normalization** to the `Fare` feature using `StandardScaler`.\n",
        "\n",
        "---\n",
        "\n",
        "SVM ã‚„ç·šå½¢å›å¸°ãªã©ã€ã‚¹ã‚±ãƒ¼ãƒ«ã«æ•æ„Ÿãªãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€`Fare` ã‚’ **Zã‚¹ã‚³ã‚¢æ­£è¦åŒ–ï¼ˆæ¨™æº–åŒ–ï¼‰** ã—ã¾ã—ãŸã€‚  \n",
        "`StandardScaler` ã«ã‚ˆã‚Šã€Fare ã‚’ã€Œå¹³å‡0ãƒ»æ¨™æº–åå·®1ã€ã«å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "> â€» ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚‚æ¨™æº–åŒ–ã®åŠ¹æœãŒã‚ã‚‹ç¨‹åº¦æœŸå¾…ã§ãã¾ã™ãŒã€æ±ºå®šæœ¨ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãªã©ï¼‰ã«ã¯ä¸è¦ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsO5HZdJ4mVF"
      },
      "outputs": [],
      "source": [
        "# Standardization (Z-score normalization) - especially useful for numerical models like SVM or linear models\n",
        "# æ¨™æº–åŒ–ï¼ˆZã‚¹ã‚³ã‚¢æ­£è¦åŒ–ï¼‰ - SVMã‚„ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãªã©ã€æ•°å€¤ã«æ•æ„Ÿãªãƒ¢ãƒ‡ãƒ«ã«ç‰¹ã«æœ‰åŠ¹\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ãƒ»æ¨™æº–åå·®ã‚’å­¦ç¿’ã—ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ã™ã‚‹\n",
        "df['Fare_scaled'] = scaler.fit_transform(df[['Fare']])\n",
        "\n",
        "# Apply the same transformation to the test data\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã‚‚åŒã˜ãƒ«ãƒ¼ãƒ«ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜å¹³å‡ãƒ»æ¨™æº–åå·®ï¼‰ã§æ¨™æº–åŒ–ã‚’é©ç”¨\n",
        "df_test['Fare_scaled'] = scaler.transform(df_test[['Fare']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZwHHvlm8BZf"
      },
      "source": [
        "### 4.5.5 ğŸ“Š Logistic Regression with Fare Features / Fareç‰¹å¾´é‡ã‚’å«ã‚ãŸãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°\n",
        "\n",
        "We train a logistic regression model using all features including:\n",
        "- `Fare_log`: log-transformed fare (to reduce skewness)\n",
        "- `FareBand`: binned fare values\n",
        "- Other engineered features like `Family`, `Title`, and `AgeGroup`\n",
        "\n",
        "The dataset is split into training and validation sets (70/30) to evaluate generalization performance.\n",
        "\n",
        "---\n",
        "\n",
        "`Fare_log`ï¼ˆå¯¾æ•°å¤‰æ›å¾Œã®é‹è³ƒï¼‰ã€`FareBand`ï¼ˆãƒ“ãƒ³åˆ†å‰²ã•ã‚ŒãŸé‹è³ƒï¼‰ãªã©ã®ç‰¹å¾´é‡ã‚’å«ã‚ã¦ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚  \n",
        "ãƒ‡ãƒ¼ã‚¿ã¯ 70% è¨“ç·´ç”¨ã€30% æ¤œè¨¼ç”¨ ã«åˆ†å‰²ã—ã€æ±åŒ–æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blhPHwRtue1h"
      },
      "outputs": [],
      "source": [
        "# Prepare training and test features / è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
        "X_fe4 = df_fe4.drop(columns=['PassengerId', 'Survived'])  # Features for training\n",
        "y_fe4 = df_fe4['Survived']                                # Target variable\n",
        "\n",
        "X_fe4_test = df_fe4_test.drop(columns=['PassengerId'])    # Test features (no target)\n",
        "\n",
        "# Split training data into train and validation sets / è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²\n",
        "X_fe4_train, X_fe4_valid, y_fe4_train, y_fe4_valid = train_test_split(\n",
        "    X_fe4, y_fe4, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression model / ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
        "lr_fe4 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe4.fit(X_fe4_train, y_fe4_train)\n",
        "\n",
        "# Evaluate model performance / ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡\n",
        "print('Train Score: {}'.format(round(lr_fe4.score(X_fe4_train, y_fe4_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe4.score(X_fe4_valid, y_fe4_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLTHFq2A80P-"
      },
      "source": [
        "### Insights / è€ƒå¯Ÿ\n",
        "\n",
        "- Applying `log1p(Fare)` helped reduce the skewness and stabilize the model.\n",
        "- `FareBand` adds interpretability by grouping passengers based on fare range.\n",
        "- Logistic Regression benefits from normalized or transformed continuous features.\n",
        "\n",
        "---\n",
        "\n",
        "- `Fare` ã®å¯¾æ•°å¤‰æ›ã«ã‚ˆã£ã¦åˆ†å¸ƒã®åã‚ŠãŒæ”¹å–„ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚\n",
        "- `FareBand` ã«ã‚ˆã‚Šé‹è³ƒã«åŸºã¥ãã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚Šã€**è§£é‡ˆæ€§ãŒé«˜ã¾ã‚Šã¾ã—ãŸ**ã€‚\n",
        "- ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¯é€£ç¶šå€¤ã«æ•æ„ŸãªãŸã‚ã€ä»Šå›ã®ã‚ˆã†ãªå¤‰æ›ã¯æœ‰åŠ¹ã§ã™ã€‚\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8YB-pMg73O6"
      },
      "source": [
        "### 4.5.6 ğŸ“Š Random Forest with Fare Features / Fareç‰¹å¾´é‡ã‚’å«ã‚ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOUapJwg9LwG"
      },
      "outputs": [],
      "source": [
        "# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ\n",
        "rfc_fe4 = RandomForestClassifier(max_depth=5, min_samples_leaf=10, n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rfc_fe4.fit(X_fe4_train, y_fe4_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(rfc_fe4.score(X_fe4_train, y_fe4_train), 3)))\n",
        "print(' Test Score: {}'.format(round(rfc_fe4.score(X_fe4_valid, y_fe4_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAVC8Ftz9_Ri"
      },
      "source": [
        "### Insights / è€ƒå¯Ÿ  \n",
        "- The Random Forest model, enhanced with engineered fare features, shows balanced performance with similar train and test accuracies. This suggests that the model effectively captures the relationship between fare and survival without overfitting. Limiting the tree depth and minimum samples per leaf helps control model complexity and improves generalization. Including fare bins and log-transformed fare helps the model handle the skewed fare distribution better.  \n",
        "---\n",
        "- Fareç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã»ã¼åŒã˜ç²¾åº¦ã‚’ç¤ºã—ã€éå­¦ç¿’ãŒæŠ‘ãˆã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚æœ¨ã®æ·±ã•ã‚„è‘‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ãŒèª¿æ•´ã•ã‚Œã€æ±åŒ–æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚å¯¾æ•°å¤‰æ›ã‚„ç­‰é »åº¦ãƒ“ãƒ³åŒ–ã•ã‚ŒãŸFareã¯ã€åã£ãŸé‹è³ƒåˆ†å¸ƒã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ãŒã†ã¾ãå¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIckPDXsm2oO"
      },
      "source": [
        "### 4.5.6.1 Default Random Forest Model: Cross-Validation Performance  \n",
        "### / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡  \n",
        "\n",
        "We evaluate the default Random Forest model using 10-fold Stratified Cross-Validation on the entire training dataset.  \n",
        "This provides baseline metrics (accuracy and ROC AUC) to compare against tuned models later.  \n",
        "\n",
        "å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã„ã€10åˆ†å‰²ã®å±¤åŒ–äº¤å·®æ¤œè¨¼ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€å¾Œã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ãŸã‚ã®åŸºæº–ã¨ãªã‚‹ç²¾åº¦ã¨ROC AUCã®æŒ‡æ¨™ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNDZKMkwjtS_"
      },
      "outputs": [],
      "source": [
        "# Features and target / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°\n",
        "X = df_fe4.drop(columns=['PassengerId', 'Survived'])  # Features / ç‰¹å¾´é‡\n",
        "y = df_fe4['Survived']                                # Target / ç›®çš„å¤‰æ•°\n",
        "\n",
        "# Model / ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰\n",
        "rfc_default = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Cross-validation strategy / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ï¼ˆå±¤åŒ– KFoldï¼‰\n",
        "rfc_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "cv_acc_default = cross_val_score(rfc_default, X, y, cv=rfc_cv, scoring='accuracy')\n",
        "cv_auc_default = cross_val_score(rfc_default, X, y, cv=rfc_cv, scoring='roc_auc')\n",
        "\n",
        "print(\"Default Model - Mean CV Accuracy:\", np.mean(cv_acc_default))\n",
        "print(\"Default Model - Mean CV ROC AUC:\", np.mean(cv_auc_default))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYQomFlP_osd"
      },
      "source": [
        "### 4.5.7 Random Forest Hyperparameter Tuning with Randomized Search and Cross-Validation  \n",
        "#### / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã¨äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "\n",
        "- Define a parameter distribution with `max_depth`, `min_samples_leaf`, and other key options.  \n",
        "- Use `RandomizedSearchCV` with 10-fold **Stratified** cross-validation to efficiently search for the best hyperparameters.  \n",
        "- Evaluate models using **accuracy** as the primary metric.  \n",
        "- Fit `RandomizedSearchCV` on the full training data (no `train_test_split`) to maximize data usage.  \n",
        "- Retrieve and display the best hyperparameters and the best cross-validation accuracy.  \n",
        "- Using the best estimator, calculate **mean ROC AUC** and **mean accuracy** via 10-fold CV to assess model stability and generalization.  \n",
        "- This process balances performance and efficiency, making it suitable when feature engineering is still evolving.\n",
        "\n",
        "---\n",
        "\n",
        "- `max_depth` ã‚„ `min_samples_leaf` ãªã©ã®ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ã«å€™è£œã‚’è¨­å®šã€‚  \n",
        "- `RandomizedSearchCV` ã‚’ 10åˆ†å‰²ã® **StratifiedKFoldï¼ˆå±¤åŒ–äº¤å·®æ¤œè¨¼ï¼‰** ã¨ã¨ã‚‚ã«ä½¿ã„ã€åŠ¹ç‡çš„ã«æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã€‚  \n",
        "- è©•ä¾¡æŒ‡æ¨™ã«ã¯ `accuracy` ã‚’ä½¿ç”¨ã€‚  \n",
        "- ãƒ‡ãƒ¼ã‚¿ã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹ãŸã‚ã€`train_test_split` ã¯ä½¿ã‚ãšå…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã€‚  \n",
        "- æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã€äº¤å·®æ¤œè¨¼ã§ã®æœ€é«˜ç²¾åº¦ï¼ˆaccuracyï¼‰ã‚’è¡¨ç¤ºã€‚  \n",
        "- ãƒ™ã‚¹ãƒˆæ¨å®šå™¨ã‚’ä½¿ã„ã€10åˆ†å‰²äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹ **å¹³å‡ROC AUC** ã¨ **å¹³å‡Accuracy** ã‚’ç®—å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã‚’è©•ä¾¡ã€‚  \n",
        "- ç‰¹å¾´é‡ãŒã¾ã å¤‰åŒ–ã—ã¦ã„ã‚‹æ®µéšã§ã‚‚ã€åŠ¹ç‡ã‚ˆãæ±åŒ–æ€§èƒ½ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’å¾—ã‚‹ãŸã‚ã®é©åˆ‡ãªæ–¹æ³•ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksPIAhGA5svU"
      },
      "outputs": [],
      "source": [
        "# Test data / ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆç›®çš„å¤‰æ•°ã¯ãªã—ï¼‰\n",
        "X_test = df_fe4_test.drop(columns=['PassengerId'])\n",
        "\n",
        "# Define parameter distribution for RandomizedSearch / ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ã‚’å®šç¾©\n",
        "param_dist = {\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'min_samples_leaf': [1, 2, 4, 8]\n",
        "}\n",
        "\n",
        "# Cross-validation strategy / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ï¼ˆå±¤åŒ– KFoldï¼‰\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Set up RandomizedSearchCV / ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "rfc_rs = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,                 # Number of parameter settings sampled / è©¦ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\n",
        "    scoring='accuracy',       # Evaluation metric / è©•ä¾¡æŒ‡æ¨™\n",
        "    cv=cv,                    # Cross-validation strategy / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV / ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã®å®Ÿè¡Œ\n",
        "rfc_rs.fit(X, y)\n",
        "\n",
        "# Best parameters and CV score / æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨CVã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º\n",
        "print(\"Best Params:\", rfc_rs.best_params_)\n",
        "print(\"Best CV Accuracy:\", rfc_rs.best_score_)\n",
        "\n",
        "# Get best estimator / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
        "rf_rs_fe4 = rfc_rs.best_estimator_\n",
        "\n",
        "# Cross-validated performance / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ€§èƒ½è©•ä¾¡\n",
        "cv_auc = cross_val_score(rf_rs_fe4, X, y, cv=cv, scoring='roc_auc')\n",
        "cv_acc = cross_val_score(rf_rs_fe4, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"Mean CV ROC AUC:\", np.mean(cv_auc))\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_acc))\n",
        "\n",
        "# # Create a DataFrame to compare the default and tuned model performances\n",
        "# æ¯”è¼ƒç”¨ã®DataFrameä½œæˆ\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Default Random Forest', 'Tuned Random Forest'],\n",
        "    'Mean CV Accuracy': [cv_acc_default.mean(), cv_acc.mean()],\n",
        "    'Mean CV ROC AUC': [cv_auc_default.mean(), cv_auc.mean()]\n",
        "})\n",
        "\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Ejp3jrqHd7"
      },
      "source": [
        "### ğŸ“Š Model Comparison Summary / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã®ã¾ã¨ã‚\n",
        "| Model                 | Mean CV Accuracy | Mean CV ROC AUC |\n",
        "| --------------------- | ---------------- | --------------- |\n",
        "| Default Random Forest | 0.8125           | 0.8653          |\n",
        "| Tuned Random Forest   | 0.8383           | 0.8749          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMxPxr0_qWsd"
      },
      "source": [
        "### Insights / è€ƒå¯Ÿ\n",
        "\n",
        "The tuned Random Forest model outperforms the default model in both accuracy and ROC AUC. This indicates that hyperparameter tuning via RandomizedSearchCV helped improve both classification performance and the model's ability to distinguish between classes. The increase in ROC AUC suggests better ranking of predictions and improved generalization.\n",
        "\n",
        "ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¯ã€ç²¾åº¦ï¼ˆAccuracyï¼‰ã¨ROC AUCã®ä¸¡æ–¹ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€RandomizedSearchCVã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒã€åˆ†é¡æ€§èƒ½ãŠã‚ˆã³ã‚¯ãƒ©ã‚¹è­˜åˆ¥èƒ½åŠ›ã®å‘ä¸Šã«å¯„ä¸ã—ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ç‰¹ã«ROC AUCã®æ”¹å–„ã¯ã€äºˆæ¸¬ã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ã‚„æ±åŒ–æ€§èƒ½ã®å‘ä¸Šã‚’åæ˜ ã—ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHn3dHIYCUQ7"
      },
      "source": [
        "## 4.6 Feature Importance Analysis Using Random Forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ\n",
        "\n",
        "Feature importance helps us understand which features have the most influence on the model's predictions. By identifying the most important variables, we can:\n",
        "\n",
        "- Gain insights into the underlying data and factors affecting the target variable.\n",
        "- Improve model interpretability and explainability.\n",
        "- Select or engineer better features to enhance model performance.\n",
        "- Potentially reduce the dimensionality of the dataset by removing less important features, leading to simpler and faster models.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ç‰¹å¾´é‡é‡è¦åº¦ã¯ã€ã©ã®ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã«ã©ã‚Œã ã‘å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®æŒ‡æ¨™ã§ã™ã€‚é‡è¦ãªç‰¹å¾´é‡ã‚’çŸ¥ã‚‹ã“ã¨ã§ã€\n",
        "\n",
        "- ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªã‚„ç›®çš„å¤‰æ•°ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã®ç†è§£ãŒæ·±ã¾ã‚Šã¾ã™ã€‚\n",
        "- ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ã‚„èª¬æ˜åŠ›ãŒå‘ä¸Šã—ã¾ã™ã€‚\n",
        "- ã‚ˆã‚Šè‰¯ã„ç‰¹å¾´é‡ã®é¸æŠã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«å½¹ç«‹ã¡ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’é«˜ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
        "- é‡è¦åº¦ã®ä½ã„ç‰¹å¾´é‡ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§ã€æ¬¡å…ƒå‰Šæ¸›ã‚„ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdx1ot84NZGg"
      },
      "outputs": [],
      "source": [
        "# Get feature names excluding 'PassengerId' and 'Survived'\n",
        "# 'PassengerId'ã¨'ç”Ÿå­˜'åˆ—ã¯é™¤ãç‰¹å¾´é‡åã‚’å–å¾—\n",
        "feature_names = df_fe4.drop(columns=['PassengerId', 'Survived']).columns\n",
        "\n",
        "# Retrieve feature importances from the best fitted Random Forest model\n",
        "# æœ€é©ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—\n",
        "importances = rf_rs_fe4.feature_importances_\n",
        "\n",
        "# Create a DataFrame for features and their importance scores, then sort descending\n",
        "# ç‰¹å¾´é‡ã¨é‡è¦åº¦ã‚’ã¾ã¨ã‚ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã—ã€é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance as a horizontal bar chart\n",
        "# æ°´å¹³æ£’ã‚°ãƒ©ãƒ•ã§ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Feature Importance')  # Xè»¸ãƒ©ãƒ™ãƒ«ï¼šç‰¹å¾´é‡ã®é‡è¦åº¦\n",
        "plt.title('Feature Importance (Random Forest)')  # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«ï¼šãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show most important feature at topï¼ˆé‡è¦ãªç‰¹å¾´ã‚’ä¸Šã«è¡¨ç¤ºï¼‰\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEr1VNMsDyH"
      },
      "source": [
        "###ğŸ’¡ Insights from Feature Importances /  ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ãè€ƒå¯Ÿ  \n",
        "\n",
        "According to the Random Forest model, the top contributing features were:  \n",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦ã§ã¯ã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒç‰¹ã«å¤§ããªå½±éŸ¿ã‚’æŒã¤ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸï¼š  \n",
        "\n",
        "1.  Sex  ï¼ˆæ€§åˆ¥ï¼‰\n",
        "2.  Title_Mr  ï¼ˆç”·æ€§ã®æ•¬ç§°ï¼‰\n",
        "3.  Fare  ï¼ˆé‹è³ƒï¼‰\n",
        "\n",
        "\n",
        "These results align with known survival patterns on the Titanic:\n",
        "\n",
        "Sex is the most influential feature, with females having significantly higher survival rates.\n",
        "\n",
        "Title_Mr helps identify adult males, who historically had lower survival due to â€œwomen and children firstâ€ policies.\n",
        "\n",
        "Fare (log-transformed or binned) reflects passenger class and socio-economic status, both of which are closely tied to survival odds.\n",
        "\n",
        "\n",
        "ã“ã‚Œã¯ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ã®ç”Ÿå­˜å‚¾å‘ã¨æ•´åˆçš„ã§ã™ï¼š\n",
        "\n",
        "æ€§åˆ¥ã¯æœ€ã‚‚é‡è¦ã§ã€å¥³æ€§ã®ç”Ÿå­˜ç‡ãŒæ˜ã‚‰ã‹ã«é«˜ã„ã“ã¨ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚‚ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "Title_Mrã¯æˆäººç”·æ€§ã‚’ç‰¹å®šã§ãã€\"å¥³æ€§ã¨å­ä¾›ãŒå„ªå…ˆ\"ã¨ã„ã†å½“æ™‚ã®æ•‘åŠ©æ–¹é‡ã‚’åæ˜ ã—ãŸçµæœã§ã™ã€‚\n",
        "\n",
        "Fareï¼ˆå¯¾æ•°å¤‰æ›ã¾ãŸã¯ãƒ“ãƒ³åŒ–æ¸ˆã¿ï¼‰ã¯ã€ä¹—å®¢ã®éšç´šã‚„ç¤¾ä¼šçš„åœ°ä½ã‚’è¡¨ã—ã¦ãŠã‚Šã€ç”Ÿå­˜å¯èƒ½æ€§ã¨å¼·ãç›¸é–¢ã—ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWjNj4Jm_jJ0"
      },
      "source": [
        "## 4.7 Decision Tree Visualization (First Tree of Random Forest) /         æ±ºå®šæœ¨ã®å¯è¦–åŒ–ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ1æœ¬ç›®ï¼‰\n",
        "\n",
        "Below is the structure of the first decision tree within the Random Forest model.  \n",
        "In this tree, **sex** is used as the first splitting condition, indicating its high importance in the model.\n",
        "\n",
        "ä»¥ä¸‹ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå†…ã®1æœ¬ç›®ã®æ±ºå®šæœ¨ã®æ§‹é€ ã§ã™ã€‚  \n",
        "ã“ã®æœ¨ã§ã¯ **sex** ãŒæœ€åˆã®åˆ†å²æ¡ä»¶ã¨ãªã£ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦é‡è¦ãªç‰¹å¾´é‡ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrQf1ev_9Pq4"
      },
      "outputs": [],
      "source": [
        "# Extract one tree from the random forest and visualize it\n",
        "# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‹ã‚‰1æœ¬ã®æ±ºå®šæœ¨ã‚’æŠ½å‡ºã—ã¦å¯è¦–åŒ–ã™ã‚‹\n",
        "\n",
        "estimator = rf_rs_fe4.estimators_[0]  # The first tree in the random forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå†…ã®æœ€åˆã®æ±ºå®šæœ¨ã‚’å–å¾—\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(estimator,\n",
        "          feature_names=df_fe4.drop(columns=['PassengerId', 'Survived']).columns,  # Feature names for display\n",
        "          class_names=[\"Died\", \"Survived\"],  # Class labels\n",
        "          filled=True,  # Color nodes by class purity\n",
        "          max_depth=3,  # Limit depth of visualization for readability\n",
        "          fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoLcFsyavDWw"
      },
      "source": [
        "Note: The features used in the early splits of a single decision tree may not perfectly match the global feature importance rankings. Feature importance is computed across all trees, while the decision tree visualization only represents one treeâ€™s structure.  \n",
        "ï¼ˆæ³¨ï¼‰1æœ¬ã®æ±ºå®šæœ¨ã§æœ€åˆã«ä½¿ã‚ã‚Œã‚‹ç‰¹å¾´é‡ã¨ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå…¨ä½“ã®é‡è¦åº¦ä¸Šä½ã®ç‰¹å¾´é‡ã¯ã€å¿…ãšã—ã‚‚ä¸€è‡´ã—ã¾ã›ã‚“ã€‚é‡è¦åº¦ã¯ã™ã¹ã¦ã®æœ¨ã‚’é€šã˜ã¦ç®—å‡ºã•ã‚Œã‚‹çµ±è¨ˆçš„ãªæŒ‡æ¨™ã§ã‚ã‚Šã€å¯è¦–åŒ–ã•ã‚ŒãŸæœ¨ã¯ãã®ä¸­ã®ä¸€ä¾‹ã«ã™ãã¾ã›ã‚“ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsvSRWBHJ89C"
      },
      "source": [
        "## 4.8 Top 10 Feature Importance Analysis in Random Forest Model / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ä¸Šä½10ç‰¹å¾´é‡ã®é‡è¦åº¦åˆ†æ  \n",
        "\n",
        "Understanding which features contribute most to model performance helps in interpretation, feature selection, and further improvements.\n",
        "Here, we extract the top 10 most important features based on the tuned Random Forest model.\n",
        "\n",
        "ã©ã®ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã«æœ€ã‚‚è²¢çŒ®ã—ã¦ã„ã‚‹ã‹ã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ãŒé«˜ã¾ã‚Šã€ç‰¹å¾´é‡é¸æŠã‚„æ”¹å–„ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
        "ã“ã“ã§ã¯ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ã€é‡è¦åº¦ã®é«˜ã„ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10ã‚’æŠ½å‡ºã—ã¦å¯è¦–åŒ–ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktF2J2cF1ktz"
      },
      "outputs": [],
      "source": [
        "top_n = 10  # Extract top 10 features\n",
        "top_features = feature_importance_df.head(top_n)\n",
        "\n",
        "# Plot the top 10 feature importances\n",
        "# ä¸Šä½10ä»¶ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=top_features, x='Importance', y='Feature', palette='viridis')\n",
        "plt.title(f'Top {top_n} Feature Importances (Random Forest)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdqOkS3It7LU"
      },
      "source": [
        "###ğŸ’¡ Insights / è€ƒå¯Ÿ\n",
        "Sex, Title_Mr, and Fare are among the top influential predictors.\n",
        "\n",
        "Visualizing feature importance helps us focus on key variables and potentially eliminate or transform low-importance ones.\n",
        "\n",
        "These results reinforce earlier interpretations and guide future feature refinement.\n",
        "\n",
        "Sex, Title_Mr, Fare ãªã©ãŒä¸Šä½ã«ä½ç½®ã—ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦éå¸¸ã«é‡è¦ãªç‰¹å¾´é‡ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "ã“ã®å¯è¦–åŒ–ã¯ã€é‡è¦ãªå¤‰æ•°ã«æ³¨ç›®ã—ã€ä¸è¦ãªç‰¹å¾´ã®å‰Šé™¤ã‚„æ”¹å–„ã«å‘ã‘ãŸæ–¹å‘æ€§ã‚’ç¤ºã—ã¦ãã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiS_ImE06-QR"
      },
      "source": [
        "## 4.9 Cumulative Feature Importance for Feature Selection / ç‰¹å¾´é‡é¸æŠã®ãŸã‚ã®ç´¯ç©é‡è¦åº¦ã®å¯è¦–åŒ–\n",
        "To perform feature selection, we visualized the cumulative feature importance.  \n",
        "This helps us identify how many top features are needed to cover over 80% of the total importance.  \n",
        "The cumulative importance plot serves as a guide to balance model complexity and performance.\n",
        "\n",
        "---\n",
        "ç‰¹å¾´é‡é¸æŠã‚’è¡Œã†ãŸã‚ã«ã€ç‰¹å¾´é‡ã®ç´¯ç©é‡è¦åº¦ã‚’å¯è¦–åŒ–ã—ã¾ã—ãŸã€‚  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€å…¨ä½“ã®80%ä»¥ä¸Šã®é‡è¦åº¦ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãŸã‚ã«å¿…è¦ãªä¸Šä½ç‰¹å¾´é‡ã®æ•°ãŒã‚ã‹ã‚Šã¾ã™ã€‚  \n",
        "ç´¯ç©é‡è¦åº¦ã®ã‚°ãƒ©ãƒ•ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹éš›ã®æŒ‡æ¨™ã¨ã—ã¦å½¹ç«‹ã¡ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v82-V_Ma7At6"
      },
      "outputs": [],
      "source": [
        "# Calculate cumulative importance\n",
        "# ç´¯ç©é‡è¦åº¦ã‚’è¨ˆç®—ã™ã‚‹\n",
        "feature_importance_df['Cumulative'] = feature_importance_df['Importance'].cumsum()\n",
        "\n",
        "# Limit the number of displayed features (Top 20)\n",
        "# è¡¨ç¤ºã™ã‚‹ç‰¹å¾´é‡æ•°ã‚’åˆ¶é™ï¼ˆä¸Šä½20ä»¶ã®ã¿è¡¨ç¤ºï¼‰\n",
        "top_n_display = 20\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Line plot with feature names on the x-axis\n",
        "# ç‰¹å¾´é‡åã‚’Xè»¸ã«ç”¨ã„ãŸæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•\n",
        "sns.lineplot(\n",
        "    data=feature_importance_df.head(top_n_display),\n",
        "    x='Feature',\n",
        "    y='Cumulative',\n",
        "    marker='o'\n",
        ")\n",
        "\n",
        "# Rotate x-axis labels for readability\n",
        "# Xè»¸ãƒ©ãƒ™ãƒ«ã‚’å›è»¢ã—ã¦èª­ã¿ã‚„ã™ãã™ã‚‹\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Draw a horizontal line at 80% cumulative importance\n",
        "# ç´¯ç©é‡è¦åº¦80%ã®ã—ãã„å€¤ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ \n",
        "plt.axhline(0.8, color='red', linestyle='--', label='80% threshold')\n",
        "\n",
        "# Titles and labels / ã‚¿ã‚¤ãƒˆãƒ«ã¨è»¸ãƒ©ãƒ™ãƒ«\n",
        "plt.title('Cumulative Feature Importance (Top 20 Features)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Cumulative Importance')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust layout / ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErWshnjGEZl"
      },
      "source": [
        "## 4.10 Selected Features Covering Top 80% of Cumulative Importance / ç´¯ç©é‡è¦åº¦ä¸Šä½80%ã‚’å ã‚ã‚‹ç‰¹å¾´é‡ã®é¸æŠ\n",
        "\n",
        "This code selects and lists the features whose cumulative importance is less than 80%.  \n",
        "These features represent the most important ones that contribute up to 80% of the total importance in the model.  \n",
        "Using these selected features can help reduce dimensionality while retaining most of the predictive power.  \n",
        "\n",
        "---\n",
        "ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ç´¯ç©é‡è¦åº¦ãŒ80%æœªæº€ã®ç‰¹å¾´é‡ã‚’é¸æŠã—ã€ãƒªã‚¹ãƒˆåŒ–ã—ã¦ã„ã¾ã™ã€‚  \n",
        "ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯ãƒ¢ãƒ‡ãƒ«ã®å…¨ä½“é‡è¦åº¦ã®80%ã¾ã§ã‚’å ã‚ã‚‹æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ç¾¤ã§ã™ã€‚  \n",
        "ã“ã®ç‰¹å¾´é‡ç¾¤ã‚’ä½¿ã†ã“ã¨ã§ã€æ¬¡å…ƒå‰Šæ¸›ã—ã¤ã¤äºˆæ¸¬æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒæœŸå¾…ã§ãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MldIUfgJ9baE"
      },
      "outputs": [],
      "source": [
        "# Select features that contribute to cumulative importance less than 80%\n",
        "# ç´¯ç©é‡è¦åº¦ãŒ80%æœªæº€ã®ç‰¹å¾´é‡ã‚’é¸æŠ\n",
        "selected_features = feature_importance_df[feature_importance_df['Cumulative'] < 0.8]['Feature'].tolist()\n",
        "\n",
        "# Display the selected features\n",
        "# é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’è¡¨ç¤º\n",
        "print(\"Top 80% features:\", selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jT2EQ1fTWe"
      },
      "source": [
        "## 4.11 Grid Search Using Top 80% Features / ä¸Šä½80%ã®ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®å®Ÿæ–½\n",
        "\n",
        "We select only the features that cumulatively account for 80% of the importance and perform hyperparameter tuning (grid search) on the Random Forest model using these selected features.\n",
        "\n",
        "---\n",
        "ç´¯ç©é‡è¦åº¦ã§ä¸Šä½80%ã‚’å ã‚ã‚‹ç‰¹å¾´é‡ã ã‘ã‚’é¸æŠã—ã€ãã®ç‰¹å¾´é‡ã«çµã£ã¦ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼‰ã‚’è¡Œã„ã¾ã™ã€‚  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TgsMwG_AobB"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ä¸Šä½80%ã®ç‰¹å¾´é‡ã ã‘ã‚’ç‰¹å¾´é‡ã«å«ã‚ã‚‹\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldã®å®šç¾©ï¼ˆshuffleã‚ã‚Šï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Hyperparameter grid / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œè¨­å®š\n",
        "# =====================================================\n",
        "param_grid_rf_sel = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Grid Search with cross-validation / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ\n",
        "# =====================================================\n",
        "grid_search_rf_sel = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid_rf_sel,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Fit on all data / å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ï¼‰\n",
        "# =====================================================\n",
        "grid_search_rf_sel.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Get best estimator and parameters / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—\n",
        "# =====================================================\n",
        "best_model_rf_sel = grid_search_rf_sel.best_estimator_\n",
        "best_params_rf_sel = grid_search_rf_sel.best_params_\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluation function definition / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡é–¢æ•°å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluate model with cross-validation / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_rf_sel = evaluate_model_cv(best_model_rf_sel, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Classification report with cross-validated predictions / CVäºˆæ¸¬ã‚’ä½¿ã£ãŸåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
        "# =====================================================\n",
        "y_pred_cv_rf_sel = cross_val_predict(best_model_rf_sel, X_selected, y_selected, cv=cv)\n",
        "class_report_rf_sel = classification_report(y_selected, y_pred_cv_rf_sel)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Best Parameters (RF selected features):\", best_params_rf_sel)\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", scores_rf_sel['accuracy'])\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", scores_rf_sel['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", class_report_rf_sel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWIYttpmHc9P"
      },
      "source": [
        "### Discussion / è€ƒå¯Ÿ\n",
        "\n",
        "- **Effectiveness of Feature Selection**  \n",
        "  Selecting only the top 80% of features based on cumulative importance retained strong predictive performance, demonstrating that dimensionality reduction by removing less important features is effective.\n",
        "\n",
        "- **Model Performance**  \n",
        "  The mean cross-validated accuracy (~0.85) and ROC AUC (~0.88) show that the model generalizes well and maintains stable performance.\n",
        "\n",
        "- **Class-wise Performance Differences**  \n",
        "  The recall for the deceased class (class 0) is high, whereas the recall for the survived class (class 1) is somewhat lower, indicating the model tends to miss some survivors.\n",
        "\n",
        "- **Improved Computational Efficiency**  \n",
        "  Reducing the number of features lowers training and inference computational costs, making the model more practical for deployment.\n",
        "\n",
        "Future work could focus on improving recall for the survived class, exploring further feature engineering, and considering ensemble methods to boost accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "- **ç‰¹å¾´é‡é¸æŠã®æœ‰åŠ¹æ€§**  \n",
        "  ç´¯ç©é‡è¦åº¦ã®ä¸Šä½80%ã®ç‰¹å¾´é‡ã ã‘ã‚’é¸æŠã—ã¦ã‚‚é«˜ã„äºˆæ¸¬æ€§èƒ½ã‚’ç¶­æŒã—ã¦ãŠã‚Šã€é‡è¦åº¦ã®ä½ã„ç‰¹å¾´é‡ã‚’é™¤ãã“ã¨ã§æ¬¡å…ƒå‰Šæ¸›ãŒåŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "- **ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½**  \n",
        "  ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡Accuracyï¼ˆç´„0.85ï¼‰ãŠã‚ˆã³ROC AUCï¼ˆç´„0.88ï¼‰ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ãŒé«˜ãå®‰å®šã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚\n",
        "\n",
        "- **ã‚¯ãƒ©ã‚¹åˆ¥ã®æ€§èƒ½å·®**  \n",
        "  æ­»äº¡è€…ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹0ï¼‰ã®ãƒªã‚³ãƒ¼ãƒ«ã¯é«˜ã„ä¸€æ–¹ã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã®ãƒªã‚³ãƒ¼ãƒ«ãŒã‚„ã‚„ä½ãã€ç”Ÿå­˜è€…ã‚’è¦‹é€ƒã—ã‚„ã™ã„å‚¾å‘ãŒã¿ã‚‰ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "- **è¨ˆç®—åŠ¹ç‡ã®å‘ä¸Š**  \n",
        "  ç‰¹å¾´é‡ã‚’çµã‚‹ã“ã¨ã§å­¦ç¿’ãƒ»æ¨è«–ã®è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½æ¸›ã•ã‚Œã€å®Ÿç”¨é¢ã§ã®åˆ©ä¾¿æ€§ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ä»Šå¾Œã¯ã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®ãƒªã‚³ãƒ¼ãƒ«æ”¹å–„ã€ã•ã‚‰ãªã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ¤œè¨ã€ãŠã‚ˆã³ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®å°å…¥ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Šã‚’ç›®æŒ‡ã™ã“ã¨ãŒé‡è¦ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hua5zvQMOMbL"
      },
      "source": [
        "## 4.12 Stepwise Feature Selection from Remaining Features / æ®‹ã‚Šã®ç‰¹å¾´é‡ã‹ã‚‰1ã¤ãšã¤é¸ã‚“ã§ã‚¹ã‚³ã‚¢æ¯”è¼ƒã‚’è¡Œã†\n",
        "\n",
        "We start from the currently selected features (top 80% by importance) and evaluate the baseline model accuracy.  \n",
        "Then, we iteratively add one remaining feature at a time to see if it improves the model performance.  \n",
        "This helps us identify which additional features might be valuable to include.\n",
        "\n",
        "ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ç‰¹å¾´é‡ï¼ˆé‡è¦åº¦ä¸Šä½80%ï¼‰ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™ã€‚  \n",
        "ãã®å¾Œã€æ®‹ã‚Šã®ç‰¹å¾´é‡ã‚’ä¸€ã¤ãšã¤è¿½åŠ ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’æ¯”è¼ƒã—ã€  \n",
        "ã©ã®ç‰¹å¾´é‡ã‚’è¿½åŠ ã™ã‚‹ã¨æ”¹å–„ãŒè¦‹è¾¼ã‚ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l03ULGDPBNg"
      },
      "outputs": [],
      "source": [
        "# Currently selected features (top 80% by importance)\n",
        "# ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ç‰¹å¾´é‡ï¼ˆé‡è¦åº¦ä¸Šä½80%ï¼‰\n",
        "selected_features = ['Sex', 'Title_Mr', 'Fare', 'Fare_log', 'Age', 'Title_Miss', 'Pclass_3', 'TicketGroupSize', 'Title_Mrs', 'Family', 'Has_Cabin']\n",
        "\n",
        "# All feature columns excluding 'PassengerId' and 'Survived'\n",
        "# 'PassengerId'ã¨'ç”Ÿå­˜'åˆ—ã‚’é™¤ã„ãŸã™ã¹ã¦ã®ç‰¹å¾´é‡å\n",
        "all_features = df_fe4.drop(columns=['PassengerId', 'Survived']).columns.tolist()\n",
        "\n",
        "# Candidate features not yet used in the selected features\n",
        "# ç¾åœ¨ã®é¸æŠç‰¹å¾´é‡ã«å«ã¾ã‚Œã¦ã„ãªã„ã€æœªä½¿ç”¨ã®å€™è£œç‰¹å¾´é‡\n",
        "remaining_features = [f for f in all_features if f not in selected_features]\n",
        "\n",
        "# Original target variable / ç›®çš„å¤‰æ•°ï¼ˆç”Ÿå­˜ãƒ•ãƒ©ã‚°ï¼‰\n",
        "y = df_fe4['Survived']\n",
        "\n",
        "print(\"Number of current features:\", len(selected_features))  # ç¾åœ¨ã®ç‰¹å¾´é‡æ•°\n",
        "print(\"Number of candidate features:\", len(remaining_features))  # å€™è£œç‰¹å¾´é‡æ•°\n",
        "print()\n",
        "\n",
        "# Baseline score with currently selected features / ç¾åœ¨ã®ç‰¹å¾´é‡ã®ã¿ã§ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
        "X_base = df_fe4[selected_features]\n",
        "X_train_base, X_valid_base, y_train, y_valid = train_test_split(\n",
        "    X_base, y, test_size=0.3, random_state=42\n",
        ")\n",
        "model_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_base.fit(X_train_base, y_train)\n",
        "base_score = model_base.score(X_valid_base, y_valid)\n",
        "print(f\"Baseline score with current features: {base_score:.4f}\")\n",
        "print()\n",
        "\n",
        "# List to store results of adding each remaining feature / è¿½åŠ ç‰¹å¾´ã”ã¨ã®çµæœæ ¼ç´ãƒªã‚¹ãƒˆ\n",
        "improvement_list = []\n",
        "\n",
        "for feature in remaining_features:\n",
        "    # Combine current features with one candidate feature / ç¾åœ¨ã®ç‰¹å¾´é‡ã«1ã¤ã®å€™è£œç‰¹å¾´é‡ã‚’è¿½åŠ \n",
        "    test_features = selected_features + [feature]\n",
        "    X = df_fe4[test_features].values\n",
        "\n",
        "    # Split train and validation sets / å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train Random Forest model / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate accuracy on validation set / æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦è©•ä¾¡\n",
        "    score = model.score(X_valid, y_valid)\n",
        "\n",
        "    # Calculate score difference compared to baseline / ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®å·®åˆ†ã‚’è¨ˆç®—\n",
        "    score_diff = score - base_score\n",
        "\n",
        "    # Append results to list / çµæœã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
        "    improvement_list.append((feature, score, score_diff))\n",
        "\n",
        "    print(f\"Added feature: {feature:<20} â†’ Score: {score:.4f} (Diff: {score_diff:+.4f})\")\n",
        "\n",
        "# Sort results by score difference descending / ã‚¹ã‚³ã‚¢å·®åˆ†ã§é™é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "improvement_list.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\n=== Score Improvement Ranking / ã‚¹ã‚³ã‚¢æ”¹å–„ãƒ©ãƒ³ã‚­ãƒ³ã‚° ===\")\n",
        "for feature, score, diff in improvement_list:\n",
        "    print(f\"{feature:<20} â†’ Score: {score:.4f} (Diff: {diff:+.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6_OG0WqJ70L"
      },
      "source": [
        "### Discussion / è€ƒå¯Ÿ\n",
        "\n",
        "### 1. Model Performance Improvement  \n",
        "By adding the features `S` (Embarked), `AgeGroup_Adult` (Adult Age Group), `Deck_B` (Deck B), `Pclass_1` (First Class), and `IsGroup` (Presence of Group), the model's accuracy improved compared to the baseline using the top 80% important features.  \n",
        "Notably, adding `S` led to the largest accuracy gain of about 2.2%, confirming that the boarding location plays a significant role in survival prediction.\n",
        "\n",
        "### 1. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®å‘ä¸Š  \n",
        "`S`ï¼ˆä¹—èˆ¹åœ°ï¼‰ã€`AgeGroup_Adult`ï¼ˆæˆäººå¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã€`Deck_B`ï¼ˆBãƒ‡ãƒƒã‚­ï¼‰ã€`Pclass_1`ï¼ˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ï¼‰ã€`IsGroup`ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ã®æœ‰ç„¡ï¼‰ã‚’è¿½åŠ ã—ãŸã“ã¨ã§ã€é‡è¦åº¦ä¸Šä½80%ã®ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ã£ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚  \n",
        "ç‰¹ã«`S`ã®è¿½åŠ ã«ã‚ˆã‚‹ç´„2.2%ã®ç²¾åº¦æ”¹å–„ãŒæœ€å¤§ã§ã‚ã‚Šã€ä¹—èˆ¹åœ°ãŒç”Ÿå­˜äºˆæ¸¬ã«é‡è¦ãªå› å­ã§ã‚ã‚‹ã“ã¨ã‚’è£ä»˜ã‘ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Interpretability and Practical Implications  \n",
        "- The `S` feature likely reflects differences in rescue priority or lifeboat access based on boarding location.  \n",
        "- `AgeGroup_Adult` captures survival pattern differences between adults and other age groups.  \n",
        "- `Deck_B` and `Pclass_1` represent spatial and social status information influencing survival chances.  \n",
        "- `IsGroup` suggests that traveling in groups may affect survival likelihood.\n",
        "\n",
        "### 2. è§£é‡ˆã¨å®Ÿç”¨çš„æ„ç¾©  \n",
        "- `S`ã¯ã€æ•‘å‘½ãƒœãƒ¼ãƒˆã®é…ç½®ã‚„æ•‘åŠ©ã®å„ªå…ˆåº¦ã®é•ã„ã‚’åæ˜ ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "- `AgeGroup_Adult`ã¯ã€æˆäººã¨ãã®ä»–ã®å¹´é½¢å±¤é–“ã§ã®ç”Ÿå­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é•ã„ã‚’æ‰ãˆã¦ã„ã¾ã™ã€‚  \n",
        "- `Deck_B`ã‚„`Pclass_1`ã¯ã€èˆ¹å†…ã®ä½ç½®ã‚„ç¤¾ä¼šçš„åœ°ä½ã«é–¢ã‚ã‚‹æƒ…å ±ã§ã€ç”Ÿå­˜ç¢ºç‡ã«å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚  \n",
        "- `IsGroup`ã¯ã€ã‚°ãƒ«ãƒ¼ãƒ—ã§ã®åŒè¡ŒãŒç”Ÿå­˜ç‡ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Future Directions  \n",
        "While the model accuracy improved, there remains room to enhance recall for the survived class. Future efforts could focus on:  \n",
        "- Further feature engineering, such as detailed group or family relationships.  \n",
        "- Ensemble methods to increase model robustness.  \n",
        "- Using balanced evaluation metrics like F1-score or AUC to better capture minority class performance.\n",
        "\n",
        "### 3. ä»Šå¾Œã®æ–¹å‘æ€§  \n",
        "ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯å‘ä¸Šã—ã¾ã—ãŸãŒã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®ãƒªã‚³ãƒ¼ãƒ«æ”¹å–„ã«ã¯ã¾ã èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä»Šå¾Œã¯ä»¥ä¸‹ã®ç‚¹ã«æ³¨åŠ›ã™ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†ã€‚  \n",
        "- ã‚°ãƒ«ãƒ¼ãƒ—ã‚„å®¶æ—é–¢ä¿‚ã®è©³ç´°ã‚’å«ã‚€ã•ã‚‰ãªã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€‚  \n",
        "- ãƒ¢ãƒ‡ãƒ«ã®å …ç‰¢æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¤œè¨ã€‚  \n",
        "- å°‘æ•°ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½ã‚’é©åˆ‡ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã€F1ã‚¹ã‚³ã‚¢ã‚„AUCãªã©ã®ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè©•ä¾¡æŒ‡æ¨™ã®æ´»ç”¨ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### Summary  \n",
        "The stepwise feature selection approach successfully identified additional valuable features, leading to a more interpretable and higher-performing model. This process enhances both the predictive power and the practical usability of the model.\n",
        "\n",
        "### ã¾ã¨ã‚  \n",
        "ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ã‚¤ã‚ºç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚Šã€é‡è¦åº¦ä¸Šä½ã®ç‰¹å¾´é‡ã ã‘ã§ãªãã€æœ‰ç”¨ãªè¿½åŠ ç‰¹å¾´é‡ã‚’ç‰¹å®šã§ãã€ã‚ˆã‚Šè§£é‡ˆã—ã‚„ã™ãç²¾åº¦ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ãŒæ§‹ç¯‰ã§ãã¾ã—ãŸã€‚  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€äºˆæ¸¬æ€§èƒ½ã¨å®Ÿç”¨æ€§ã®ä¸¡æ–¹ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9JOfp9mqrVw"
      },
      "source": [
        "### 4.13 Grid Search with Extended Feature Set / æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä½¿ã£ãŸã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ\n",
        "\n",
        "We add the features that improved the score (`S`, `AgeGroup_Adult`, `Deck_B`, `Pclass_1`, `IsGroup`) to the original selected features, creating an extended feature set.  \n",
        "Using this extended set, we perform hyperparameter tuning for the Random Forest model with GridSearchCV to find the best parameters that maximize model performance.\n",
        "\n",
        "ã‚¹ã‚³ã‚¢æ”¹å–„ã«å¯„ä¸ã—ãŸç‰¹å¾´é‡ï¼ˆ`S`ã€`AgeGroup_Adult`ã€`Deck_B`ã€`Pclass_1`ã€`IsGroup`ï¼‰ã‚’å…ƒã®é¸æŠç‰¹å¾´é‡ã‚»ãƒƒãƒˆã«è¿½åŠ ã—ã€æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚  \n",
        "ã“ã®æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’GridSearchCVã§è¡Œã„ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFswPwAELZze"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define extended feature set / æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®å®šç¾©\n",
        "# =====================================================\n",
        "extended_features = selected_features + ['S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup']\n",
        "X_extended = df_fe4[extended_features]\n",
        "y = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldã®å®šç¾©ï¼ˆshuffleã‚ã‚Šï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define hyperparameter grid / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œè¨­å®š\n",
        "# =====================================================\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Grid Search with cross-validation / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ\n",
        "# =====================================================\n",
        "rfc_extended = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Fit on all data / å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢\n",
        "# =====================================================\n",
        "rfc_extended.fit(X_extended, y)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Get best estimator and parameters / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—\n",
        "# =====================================================\n",
        "best_model_rfc_extended = rfc_extended.best_estimator_\n",
        "best_params_rfc_extended = rfc_extended.best_params_\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluate model with cross-validation / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§æ€§èƒ½è©•ä¾¡\n",
        "# =====================================================\n",
        "cv_acc_extended = cross_val_score(best_model_rfc_extended, X_extended, y, cv=cv, scoring='accuracy').mean()\n",
        "cv_auc_extended = cross_val_score(best_model_rfc_extended, X_extended, y, cv=cv, scoring='roc_auc').mean()\n",
        "\n",
        "# =====================================================\n",
        "# 8. Classification report with cross-validated predictions / CVäºˆæ¸¬ã‚’ä½¿ã£ãŸåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\n",
        "# =====================================================\n",
        "y_pred_cv_extended = cross_val_predict(best_model_rfc_extended, X_extended, y, cv=cv)\n",
        "class_report_extended = classification_report(y, y_pred_cv_extended)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Best Parameters (RF extended features):\", best_params_rfc_extended)\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy (extended features):\", cv_acc_extended)\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC (extended features):\", cv_auc_extended)\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", class_report_extended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poPbKIilsNYZ"
      },
      "source": [
        "## Discussion / è€ƒå¯Ÿ\n",
        "\n",
        "### Model Performance / ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½  \n",
        "The model using the extended feature set achieved:  \n",
        "- Mean cross-validated accuracy of approximately 0.844,  \n",
        "- ROC AUC of about 0.879,  \n",
        "showing a slight improvement compared to the baseline and simpler feature sets.\n",
        "\n",
        "æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€  \n",
        "- å¹³å‡ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ãŒç´„0.844ã€  \n",
        "- ROC AUCãŒç´„0.879ã¨ãªã‚Šã€  \n",
        "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„å˜ç´”ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã¨æ¯”è¼ƒã—ã¦è‹¥å¹²ã®æ€§èƒ½å‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "### Precision and Recall / ç²¾åº¦ã¨å†ç¾ç‡  \n",
        "- For class 0 (deceased), precision was 86% and recall was 90%, indicating strong performance.  \n",
        "- For class 1 (survived), precision was 82% but recall was lower at 76%, suggesting the model still tends to miss some survivors.\n",
        "\n",
        "ã‚¯ãƒ©ã‚¹0ï¼ˆæ­»äº¡è€…ï¼‰ã«å¯¾ã—ã¦ã¯ã€ç²¾åº¦86%ã€å†ç¾ç‡90%ã¨é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã€‚  \n",
        "ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜è€…ï¼‰ã«å¯¾ã—ã¦ã¯ã€ç²¾åº¦82%ã ãŒå†ç¾ç‡ã¯76%ã¨ã‚„ã‚„ä½ãã€ç”Ÿå­˜è€…ã‚’è¦‹é€ƒã™å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "### Implications and Future Work / ä»Šå¾Œã®ç¤ºå”†  \n",
        "The feature addition stably improved the model performance, but there remains room to improve recall for the survived class.  \n",
        "Future work could explore:  \n",
        "- Further feature engineering and data augmentation,  \n",
        "- More complex models such as ensemble.  \n",
        "\n",
        "ä»Šå›ã®ç‰¹å¾´é‡è¿½åŠ ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯å®‰å®šçš„ã«å‘ä¸Šã—ã¾ã—ãŸãŒã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®ãƒªã‚³ãƒ¼ãƒ«æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "ä»Šå¾Œã¯ã€  \n",
        "- ã•ã‚‰ãªã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚„ãƒ‡ãƒ¼ã‚¿å¢—å¼·ã€  \n",
        "- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ã‚ˆã†ãªè¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvvzTUjBsqwh"
      },
      "source": [
        "## 4.14 Confusion Matrix / æ··åŒè¡Œåˆ—\n",
        "We use the confusion matrix to evaluate the classification performance of the best Random Forest model on the validation set.  \n",
        "It shows how many predictions were correct or incorrect for each class.  \n",
        "æ··åŒè¡Œåˆ—ã¯ã€æœ€è‰¯ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿\n",
        "ã„ã¾ã™ã€‚  \n",
        "å„ã‚¯ãƒ©ã‚¹ã«ã¤ã„ã¦ã€æ­£è§£ãƒ»ä¸æ­£è§£ã®äºˆæ¸¬æ•°ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeHwq_dtBkfn"
      },
      "outputs": [],
      "source": [
        "# Predict labels with the best model\n",
        "# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’å–å¾—\n",
        "y_pred = best_model_rfc_extended.predict(X_extended)\n",
        "\n",
        "# Calculate confusion matrix / æ··åŒè¡Œåˆ—ã‚’è¨ˆç®—\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "# Print confusion matrix / æ··åŒè¡Œåˆ—ã‚’è¡¨ç¤º\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualize confusion matrix / æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–ï¼ˆä»»æ„ï¼‰\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model_rfc_extended.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4UEhcfV9d9f"
      },
      "source": [
        "# Confusion Matrix Explanation / è§£èª¬\n",
        "\n",
        "- **True Negative (TN):** 508 â€” Number of actual non-survivors correctly predicted as non-survivors.  \n",
        "- **False Positive (FP):** 41 â€” Number of actual non-survivors incorrectly predicted as survivors.  \n",
        "- **False Negative (FN):** 68 â€” Number of actual survivors incorrectly predicted as non-survivors.  \n",
        "- **True Positive (TP):** 274 â€” Number of actual survivors correctly predicted as survivors.  \n",
        "\n",
        "**Insights:**  \n",
        "- The model shows strong performance detecting non-survivors (high TN).  \n",
        "- However, it misses some survivors (relatively high FN), indicating room for improvement in recall for the survivor class.  \n",
        "- This aligns with previous observations of lower recall for class 1.  \n",
        "- Addressing class imbalance and improving model calibration could help improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **True Negative (TN):** 508 â€” å®Ÿéš›ã«æ­»äº¡ã—ãŸäººã‚’æ­£ã—ãæ­»äº¡ã¨äºˆæ¸¬ã—ãŸæ•°ã€‚  \n",
        "- **False Positive (FP):** 41 â€” å®Ÿéš›ã¯æ­»äº¡è€…ã ãŒã€ç”Ÿå­˜è€…ã¨èª¤åˆ†é¡ã—ãŸæ•°ã€‚  \n",
        "- **False Negative (FN):** 68 â€” å®Ÿéš›ã¯ç”Ÿå­˜è€…ã ãŒã€æ­»äº¡è€…ã¨èª¤åˆ†é¡ã—ãŸæ•°ã€‚  \n",
        "- **True Positive (TP):** 274 â€” å®Ÿéš›ã«ç”Ÿå­˜ã—ãŸäººã‚’æ­£ã—ãç”Ÿå­˜ã¨äºˆæ¸¬ã—ãŸæ•°ã€‚  \n",
        "\n",
        "**è€ƒå¯Ÿï¼š**  \n",
        "- æ­»äº¡è€…ã‚¯ãƒ©ã‚¹ã®æ¤œå‡ºç²¾åº¦ãŒé«˜ã„ï¼ˆTNãŒå¤šã„ï¼‰ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚  \n",
        "- ä¸€æ–¹ã§ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®è¦‹é€ƒã—ï¼ˆFNï¼‰ãŒä¸€å®šæ•°ã‚ã‚Šã€ãƒªã‚³ãƒ¼ãƒ«æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "- ã“ã‚Œã¯ä»¥å‰ã®ãƒªã‚³ãƒ¼ãƒ«ãŒä½ã„çµæœã¨ã‚‚æ•´åˆã—ã¦ã„ã¾ã™ã€‚  \n",
        "- ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¸ã®å¯¾å¿œã‚„ãƒ¢ãƒ‡ãƒ«ã®èª¿æ•´ã‚’è¡Œã†ã“ã¨ã§ã€æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Smsv-KBubgG"
      },
      "source": [
        "## 4.15 Analyzing Misclassified Samples / èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®åˆ†æ  \n",
        "\n",
        "This code identifies which validation samples the model misclassified.  \n",
        "By examining these errors, we can better understand the model's weaknesses and potentially improve it.  \n",
        "The process includes preparing the data, training the model, making predictions, restoring original feature values for clarity, and extracting the misclassified cases.\n",
        "\n",
        "ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§èª¤åˆ†é¡ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®šã—ã¾ã™ã€‚  \n",
        "èª¤åˆ†é¡ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’èª¿ã¹ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ã‚’æŠŠæ¡ã—ã€æ”¹å–„ã®ãƒ’ãƒ³ãƒˆã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚  \n",
        "å‡¦ç†ã®æµã‚Œã¯ã€ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã€äºˆæ¸¬ã€å…ƒã®ç‰¹å¾´é‡ã®å¾©å…ƒï¼ˆè¦‹ã‚„ã™ã•ã®ãŸã‚ï¼‰ã€èª¤åˆ†é¡ã‚±ãƒ¼ã‚¹ã®æŠ½å‡ºã§ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwMfIOcayfny"
      },
      "outputs": [],
      "source": [
        "# Features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®è¨­å®š\n",
        "extended_features = selected_features + ['S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup']\n",
        "X = df_fe4[extended_features]\n",
        "y = df_fe4['Survived']\n",
        "\n",
        "# Split dataset while preserving index / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒã—ã¦å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest model with best parameters / æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’è¨“ç·´\n",
        "rfc = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    max_features='sqrt',\n",
        "    min_samples_leaf=4,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rfc.fit(X_train.values, y_train.values)\n",
        "\n",
        "# Predict on validation set / æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬\n",
        "y_pred = rfc.predict(X_valid.values)\n",
        "\n",
        "# Recover validation dataset with original indices / å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ãæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ\n",
        "df_valid = df_fe4.loc[X_valid.index].copy()\n",
        "\n",
        "# Function to recover original Pclass from one-hot encoding / ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸPclassã‚’å…ƒã«æˆ»ã™é–¢æ•°\n",
        "def recover_pclass(row):\n",
        "    if row.get('Pclass_1') == 1:\n",
        "        return 1\n",
        "    elif row.get('Pclass_2') == 1:\n",
        "        return 2\n",
        "    elif row.get('Pclass_3') == 1:\n",
        "        return 3\n",
        "    return None\n",
        "\n",
        "# Add recovered Pclass column if present / Pclassåˆ—ã‚’å¾©å…ƒã—ã¦è¿½åŠ ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
        "if 'Pclass_1' in df_valid.columns:\n",
        "    df_valid['Pclass'] = df_valid.apply(recover_pclass, axis=1)\n",
        "\n",
        "# Recover Sex as string for interpretability (0: male, 1: female) / æ€§åˆ¥ã‚’æ–‡å­—åˆ—ã«å¾©å…ƒï¼ˆ0: ç”·æ€§, 1: å¥³æ€§ï¼‰\n",
        "df_valid['Sex_str'] = df_valid['Sex'].map({0: 'male', 1: 'female'})\n",
        "\n",
        "# Add prediction results and correctness flag / äºˆæ¸¬çµæœã¨æ­£è§£åˆ¤å®šã‚’è¿½åŠ \n",
        "df_valid['Predicted'] = y_pred\n",
        "df_valid['Correct'] = df_valid['Survived'] == df_valid['Predicted']\n",
        "\n",
        "# Extract misclassified samples / èª¤åˆ†é¡ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡º\n",
        "mistakes = df_valid[df_valid['Correct'] == False]\n",
        "print(\"\\n=== Misclassified Validation Data / èª¤åˆ†é¡ã•ã‚ŒãŸæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ ===\")\n",
        "print(mistakes[['Survived', 'Predicted', 'Sex_str', 'Age', 'Fare', 'Pclass']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXvrZIWviRK"
      },
      "source": [
        "### Misclassification Analysis Insights / èª¤åˆ†é¡ã®å‚¾å‘åˆ†æ\n",
        "\n",
        "Among the misclassified samples, we observe the following:\n",
        "\n",
        "- Many false negatives (actual survivors predicted as non-survivors) are **young males in 3rd class with low fares**.\n",
        "- Several false positives (actual non-survivors predicted as survivors) are **females in 2nd or 3rd class**, which might indicate the model over-prioritizes gender.\n",
        "- Passengers with ambiguous feature combinations (e.g., high fare but low class) are also frequently misclassified.\n",
        "\n",
        "These patterns suggest the model might be relying too heavily on individual features like `Sex` or `Fare`, rather than more complex interactions. Introducing more sophisticated features (e.g., family grouping, social status from titles) or using more advanced models (e.g., XGBoost) may improve performance.\n",
        "\n",
        "èª¤åˆ†é¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä»¥ä¸‹ã®å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "- å½é™°æ€§ï¼ˆç”Ÿå­˜è€…ã‚’æ­»äº¡ã¨äºˆæ¸¬ï¼‰ã«ã¯ã€**3ç­‰èˆ¹å®¤ã®è‹¥ã„ç”·æ€§ã§é‹è³ƒãŒå®‰ã„ã‚±ãƒ¼ã‚¹**ãŒå¤šãè¦‹ã‚‰ã‚Œã¾ã™ã€‚\n",
        "- å½é™½æ€§ï¼ˆæ­»äº¡è€…ã‚’ç”Ÿå­˜ã¨äºˆæ¸¬ï¼‰ã«ã¯ã€**2ã€œ3ç­‰èˆ¹å®¤ã®å¥³æ€§**ãŒå¤šãã€æ€§åˆ¥ã®é‡ã¿ä»˜ã‘ãŒå¼·ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- é‹è³ƒãŒé«˜ã„ã®ã«ä½ã‚¯ãƒ©ã‚¹ã®ä¹—å®¢ãªã©ã€**ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›ãŒè¤‡é›‘ãªã‚±ãƒ¼ã‚¹**ã‚‚èª¤åˆ†é¡ã•ã‚Œã‚„ã™ã„ã§ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã®å‚¾å‘ã‹ã‚‰ã€`Sex` ã‚„ `Fare` ã¨ã„ã£ãŸå˜ä½“ã®ç‰¹å¾´é‡ã¸ã®ä¾å­˜ãŒå¼·ãã€ã‚ˆã‚Šè¤‡é›‘ãªç‰¹å¾´é‡ã‚„ä¸Šä½ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼šXGBoostï¼‰ã®æ´»ç”¨ãŒæ”¹å–„ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyu679d6YE6Z"
      },
      "source": [
        "## 4.16 Characteristics of Misclassified Samples / èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´åˆ†æ\n",
        "In this section, we explore the characteristics of the misclassified samples in more detail.\n",
        "We compare their feature distributions with those of correctly classified passengers.\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€èª¤åˆ†é¡ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®ç‰¹å¾´ã‚’è©³ã—ãåˆ†æã—ã¾ã™ã€‚\n",
        "æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸä¹—å®¢ã¨æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€èª¤åˆ†é¡ã®åŸå› ã¨ãªã‚Šã‚„ã™ã„å‚¾å‘ã‚’æ¢ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Zrsyz3e6Kq"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of Age and Fare for misclassified samples / èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®Ageã¨Fareã‚’ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¡¨ç¤º\n",
        "mistakes[['Age', 'Fare']].hist(bins=20, figsize=(10, 4))\n",
        "plt.suptitle('Distributions of Age and Fare in Misclassified Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nN8u8keMjKa"
      },
      "source": [
        "### From the plots, we observe:\n",
        "\n",
        "- Most misclassified passengers are in the 20â€“40 age range.\n",
        "- Many misclassified cases had relatively low fares (under 30), which may contribute to the model underestimating their survival chances.\n",
        "\n",
        "ã“ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ä»¥ä¸‹ã®ã‚ˆã†ãªå‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "- èª¤åˆ†é¡ã•ã‚ŒãŸä¹—å®¢ã¯20ã€œ40æ­³ã«é›†ä¸­ã—ã¦ã„ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- é‹è³ƒãŒ30æœªæº€ã®ã‚±ãƒ¼ã‚¹ãŒå¤šãã€ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿå­˜ã®å¯èƒ½æ€§ã‚’éå°è©•ä¾¡ã—ã¦ã„ã‚‹ä¸€å› ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6703uaUM0VM"
      },
      "source": [
        "## 4.17 Crosstab of Pclass and Sex in Misclassified Samples / èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®Pclassã¨æ€§åˆ¥ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ\n",
        "To understand the demographic patterns behind the misclassifications, we examine the relationship between passenger class (`Pclass`) and sex (`Sex`) among the misclassified validation samples.\n",
        "\n",
        "èª¤åˆ†é¡ã•ã‚ŒãŸæ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«ã«ãŠã„ã¦ã€ä¹—å®¢ã®ç­‰ç´šï¼ˆ`Pclass`ï¼‰ã¨æ€§åˆ¥ï¼ˆ`Sex`ï¼‰ã®é–¢ä¿‚ã‚’èª¿ã¹ã€èª¤åˆ†é¡ãŒç‰¹å®šã®å±æ€§ã«åã£ã¦ã„ãªã„ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1394BkSOsn8"
      },
      "outputs": [],
      "source": [
        "# Stacked bar chart of misclassified Pclass Ã— Sexã€€/ èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã®Pclassã¨æ€§åˆ¥ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ\n",
        "pd.crosstab(mistakes['Pclass'], mistakes['Sex_str']).plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    colormap='Set2',\n",
        "    figsize=(6,4)\n",
        ")\n",
        "plt.title('Sex Distribution in Misclassified Samples by Pclass')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Pclass')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lVHoXIVdzFp"
      },
      "source": [
        "### Interpretation of Crosstab / ã‚¯ãƒ­ã‚¹é›†è¨ˆã®è§£é‡ˆ\n",
        "\n",
        "From the misclassified samples:\n",
        "\n",
        "- In **1st class**, only **male passengers** were misclassified.\n",
        "- In **2nd class**, **mostly females** were misclassified.\n",
        "- In **3rd class**, both males and females were misclassified, with **females slightly more frequent**.\n",
        "\n",
        "ã“ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆã‹ã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "- **1ç­‰èˆ¹å®¤ã§ã¯ç”·æ€§ã®ã¿**ãŒèª¤åˆ†é¡ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "- **2ç­‰èˆ¹å®¤ã§ã¯å¥³æ€§ã®èª¤åˆ†é¡ãŒå¤šã**ã€ãƒ¢ãƒ‡ãƒ«ãŒå¥³æ€§ã®ç”Ÿå­˜ç‡ã‚’éä¿¡ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- **3ç­‰èˆ¹å®¤ã§ã¯ç”·å¥³ã¨ã‚‚ã«èª¤åˆ†é¡**ã•ã‚Œã¦ã„ã¾ã™ãŒã€**å¥³æ€§ã®æ–¹ãŒã‚„ã‚„å¤šã„**å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "These patterns suggest that the model may **overestimate survival for females**, especially in lower classes, and **underestimate survival for 1st-class males**.\n",
        "\n",
        "ã“ã®å‚¾å‘ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ãŒç‰¹ã«ä¸‹ä½ç­‰ç´šã®å¥³æ€§ã«å¯¾ã—ã¦**ç”Ÿå­˜ç¢ºç‡ã‚’éå¤§è©•ä¾¡**ã—ã€1ç­‰èˆ¹å®¤ã®ç”·æ€§ã«å¯¾ã—ã¦ã¯**éå°è©•ä¾¡**ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CUL1nGBRFQw"
      },
      "source": [
        "## 4.18 Comparison of Age and Fare between Correct and Misclassified Samples / æ­£è§£è€…ã¨èª¤åˆ†é¡è€…ã®Ageãƒ»Fareæ¯”è¼ƒ\n",
        "\n",
        "To explore whether certain numeric features differ between correctly and incorrectly classified samples, we compare the average `Age` and `Fare` values for each group.\n",
        "\n",
        "æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã¨èª¤åˆ†é¡ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã®é–“ã§ã€`Age`ï¼ˆå¹´é½¢ï¼‰ã¨`Fare`ï¼ˆé‹è³ƒï¼‰ã®å¹³å‡ã«å·®ãŒã‚ã‚‹ã‹ã‚’èª¿ã¹ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRzaeavtfJxR"
      },
      "outputs": [],
      "source": [
        "# Separate correct and misclassified samples / æ­£è§£ãƒ»èª¤åˆ†é¡ã‚µãƒ³ãƒ—ãƒ«ã‚’åˆ†ã‘ã‚‹\n",
        "correct = df_valid[df_valid['Correct'] == True]\n",
        "mistakes = df_valid[df_valid['Correct'] == False]\n",
        "\n",
        "# Compare average Age and Fare / å¹³å‡Ageã¨Fareã‚’æ¯”è¼ƒ\n",
        "print(\"Average Age - Mistakes:\", mistakes['Age'].mean(), \" / Correct:\", correct['Age'].mean())\n",
        "print(\"Average Fare - Mistakes:\", mistakes['Fare'].mean(), \" / Correct:\", correct['Fare'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rsn92Ife1R6"
      },
      "source": [
        "### ğŸ“Š Comparison Results / æ¯”è¼ƒçµæœ\n",
        "\n",
        "| Feature  | Misclassified (èª¤åˆ†é¡) | Correctly Classified (æ­£è§£) |\n",
        "| -------- | ------------------- | ------------------------- |\n",
        "| **Age**  | 30.97               | 30.02                     |\n",
        "| **Fare** | 31.22               | 33.52                     |\n",
        "\n",
        "###ğŸ§  Interpretation / è§£é‡ˆ\n",
        "- The average Age is very similar between the two groups, indicating that age alone is not a strong factor in the modelâ€™s misclassification.\n",
        "\n",
        "- The average Fare is slightly lower among the misclassified samples, suggesting that lower fares might slightly increase misclassification risk, though the difference is small.\n",
        "\n",
        "- Ageï¼ˆå¹´é½¢ï¼‰ ã®å¹³å‡å€¤ã«ã¯ å¤§ããªå·®ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ãŒå¹´é½¢å˜ç‹¬ã«å¼·ãä¾å­˜ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "- ä¸€æ–¹ã§ Fareï¼ˆé‹è³ƒï¼‰ ã®å¹³å‡å€¤ã¯èª¤åˆ†é¡ã•ã‚ŒãŸæ–¹ãŒ ã‚ãšã‹ã«ä½ãã€é‹è³ƒãŒä½ã„ä¹—å®¢ã¯ ã‚„ã‚„èª¤åˆ†é¡ã•ã‚Œã‚„ã™ã„ å‚¾å‘ãŒç¤ºå”†ã•ã‚Œã¾ã™ï¼ˆãŸã ã—ã€å·®ã¯å°ã•ã„ã§ã™ï¼‰ã€‚  \n",
        "\n",
        "###ğŸ” Conclusion / çµè«–\n",
        "While Age appears to have minimal influence on misclassification, the slightly lower Fare among misclassified passengers indicates that fare might subtly affect model decisions. This reinforces the idea that Fare, while informative, may not be sufficient in isolation and should be considered in combination with other features for robust predictions.\n",
        "\n",
        "Age ã®å½±éŸ¿ã¯é™å®šçš„ã§ã‚ã‚‹ä¸€æ–¹ã€Fare ã¯ã‚ãšã‹ãªãŒã‚‰èª¤åˆ†é¡ã«é–¢é€£ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ã“ã‚Œã¯ã€Fare å˜ä½“ã§ã¯ãªãä»–ã®ç‰¹å¾´é‡ã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ã†ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªäºˆæ¸¬ãŒå¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDdWBzY9Xex2"
      },
      "source": [
        "## 4.19 Misclassification Rate by Age Group / å¹´é½¢å¸¯ã”ã¨ã®èª¤åˆ†é¡ç‡\n",
        "\n",
        "To identify which age groups the model tends to misclassify, we calculate the misclassification rate by age range.\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ãŒã©ã®å¹´é½¢å±¤ã§èª¤åˆ†é¡ã—ã‚„ã™ã„ã‹ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã«ã€å¹´é½¢å¸¯ã”ã¨ã«èª¤åˆ†é¡ç‡ã‚’ç®—å‡ºãƒ»å¯è¦–åŒ–ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg-mD-wrfV48"
      },
      "outputs": [],
      "source": [
        "# Create age bins / å¹´é½¢ã‚’åŒºåˆ‡ã£ã¦ã‚«ãƒ†ã‚´ãƒªåŒ–\n",
        "df_valid['AgeBin'] = pd.cut(df_valid['Age'], bins=[0,10,20,30,40,50,60,70,80])\n",
        "\n",
        "# Group by age bin and calculate misclassification rate / å¹´é½¢å¸¯ã”ã¨ã«èª¤åˆ†é¡ç‡ã‚’è¨ˆç®—\n",
        "age_mis = df_valid.groupby('AgeBin')['Correct'].agg(['count', 'sum'])\n",
        "age_mis['Misclassification Rate'] = 1 - age_mis['sum'] / age_mis['count']\n",
        "\n",
        "# Plot misclassification rate / èª¤åˆ†é¡ç‡ã‚’æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–\n",
        "age_mis['Misclassification Rate'].plot(kind='bar', title='Misclassification Rate by Age')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.xlabel('Age Bin')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkB1IxovSmsi"
      },
      "source": [
        "From the plot, we observe that the **misclassification rate is highest for passengers aged 40â€“50**.\n",
        "\n",
        "This suggests that the model may be struggling to correctly classify passengers in this age range, possibly due to mixed survival patterns or underrepresented training data for this group.\n",
        "\n",
        "ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€**40ã€œ50æ­³ã®å¹´é½¢å¸¯ã§èª¤åˆ†é¡ç‡ãŒæœ€ã‚‚é«˜ã„**ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã“ã®å¹´ä»£ã®ä¹—å®¢ã¯ã€ç”Ÿå­˜ãƒ»æ­»äº¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ··åœ¨ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãåˆ†é¡ã™ã‚‹ã®ãŒé›£ã—ã„ã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã¾ãŸã€ã“ã®å±¤ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å¯èƒ½æ€§ã‚‚è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCQAUp2cTH4c"
      },
      "source": [
        "## 4.20 Misclassification Rate by Fare Range / é‹è³ƒå¸¯ã”ã¨ã®èª¤åˆ†é¡ç‡\n",
        "\n",
        "We analyze how the misclassification rate varies across fare ranges. This helps identify whether the model performs poorly for passengers in certain fare brackets.\n",
        "\n",
        "é‹è³ƒå¸¯ã”ã¨ã®èª¤åˆ†é¡ç‡ã‚’ç®—å‡ºãƒ»å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒç‰¹å®šã®ä¾¡æ ¼å¸¯ã§èª¤åˆ†é¡ã—ã‚„ã™ã„å‚¾å‘ãŒãªã„ã‹ã‚’èª¿ã¹ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRdsV-sYfZc2"
      },
      "outputs": [],
      "source": [
        "# Create fare bins every 10 units / é‹è³ƒã‚’10åˆ»ã¿ã§ãƒ“ãƒ‹ãƒ³ã‚°\n",
        "df_valid['FareBin'] = pd.cut(df_valid['Fare'], bins=range(0, 300, 10))\n",
        "\n",
        "# Calculate misclassification rate by fare bin / é‹è³ƒå¸¯ã”ã¨ã®èª¤åˆ†é¡ç‡ã‚’è¨ˆç®—\n",
        "fare_mis = df_valid.groupby('FareBin')['Correct'].agg(['count', 'sum'])\n",
        "fare_mis['Misclassification Rate'] = 1 - fare_mis['sum'] / fare_mis['count']\n",
        "\n",
        "# Plot the misclassification rate / æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–\n",
        "fare_mis['Misclassification Rate'].plot(kind='bar', title='Misclassification Rate by Fare')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.xlabel('Fare Bin')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJsSf57AWrVs"
      },
      "source": [
        "### Observation: High Misclassification in High-Fare Bins / é«˜é¡é‹è³ƒå¸¯ã§ã®èª¤åˆ†é¡ã®å‚¾å‘\n",
        "\n",
        "From the plot, we observe that misclassification rates spike in certain high-fare bins:\n",
        "\n",
        "- **130â€“140**\n",
        "- **240â€“250**\n",
        "\n",
        "This is surprising, as high fare typically correlates with higher survival (e.g. 1st-class passengers). The elevated error rate suggests that the model may:\n",
        "\n",
        "- Misclassify wealthy passengers who didn't survive\n",
        "- Overestimate survival for certain high-fare profiles\n",
        "\n",
        "These outliers might be special cases (e.g. solo 1st-class men, or passengers with rare combinations of features).\n",
        "\n",
        "ã‚°ãƒ©ãƒ•ã‹ã‚‰ã€ä»¥ä¸‹ã®é«˜é¡é‹è³ƒå¸¯ã§èª¤åˆ†é¡ç‡ãŒé«˜ããªã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸï¼š\n",
        "\n",
        "- **130ã€œ140**\n",
        "- **240ã€œ250**\n",
        "\n",
        "é«˜é‹è³ƒã®ä¹—å®¢ï¼ˆãŸã¨ãˆã°1ç­‰èˆ¹å®¤ï¼‰ã¯é€šå¸¸ç”Ÿå­˜ç‡ãŒé«˜ã„ãŸã‚ã€ã“ã‚Œã¯æ„å¤–ãªçµæœã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãŒä»¥ä¸‹ã®ã‚ˆã†ãªèª¤ã‚Šã‚’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š\n",
        "\n",
        "- å®Ÿéš›ã«ã¯æ­»äº¡ã—ãŸè£•ç¦ãªä¹—å®¢ã‚’ã€ç”Ÿå­˜ã¨èª¤åˆ†é¡ã—ã¦ã„ã‚‹\n",
        "- ç‰¹å®šã®é«˜é¡é‹è³ƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã€ç”Ÿå­˜ç¢ºç‡ã‚’éå¤§è©•ä¾¡ã—ã¦ã„ã‚‹\n",
        "\n",
        "ã“ã‚Œã‚‰ã®ã‚±ãƒ¼ã‚¹ã¯ã€ç‰¹æ®Šãªæ¡ä»¶ã‚’æŒã¤ä¾‹å¤–ï¼ˆä¾‹ï¼š1ç­‰èˆ¹å®¤ã§å˜ç‹¬ã®ç”·æ€§ãªã©ï¼‰ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HsIE4yRXftj"
      },
      "source": [
        "## 4.21 Detailed Misclassification Analysis by FareBin, Pclass, and Sex / é‹è³ƒå¸¯ Ã— ç­‰ç´š Ã— æ€§åˆ¥ã«ã‚ˆã‚‹è©³ç´°ãªèª¤åˆ†é¡åˆ†æ\n",
        "\n",
        "To detect specific passenger groups where the model performs poorly, we analyze misclassification rates grouped by `FareBin`, `Pclass`, and `Sex`.\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ãŒèª¤åˆ†é¡ã—ã‚„ã™ã„å…·ä½“çš„ãªä¹—å®¢ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«ã€`FareBinï¼ˆé‹è³ƒå¸¯ï¼‰Ã— Pclassï¼ˆä¹—å®¢ç­‰ç´šï¼‰Ã— Sexï¼ˆæ€§åˆ¥ï¼‰`ã§èª¤åˆ†é¡ç‡ã‚’ç®—å‡ºã—ã€èª¤åˆ†é¡ç‡ãŒé«˜ã„é †ã«ä¸¦ã¹ã¦ç¢ºèªã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcdxpc5GfjUd"
      },
      "outputs": [],
      "source": [
        "# Group by FareBin, Pclass, and Sex / é‹è³ƒå¸¯ Ã— ç­‰ç´š Ã— æ€§åˆ¥ã§é›†è¨ˆ\n",
        "grouped = df_valid.groupby(['FareBin', 'Pclass', 'Sex_str'])['Correct'].agg(['count', 'sum'])\n",
        "\n",
        "# Calculate misclassification rate / èª¤åˆ†é¡ç‡ = 1 - æ­£è§£ç‡\n",
        "grouped['Misclassification Rate'] = 1 - grouped['sum'] / grouped['count']\n",
        "\n",
        "# Sort by highest misclassification rate / èª¤åˆ†é¡ç‡ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º\n",
        "print(grouped.sort_values('Misclassification Rate', ascending=False).dropna())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af6NyKd1Z3Wp"
      },
      "source": [
        "### Key Insights from Misclassification by Fare, Pclass, and Sex / é‹è³ƒãƒ»ç­‰ç´šãƒ»æ€§åˆ¥ã«ã‚ˆã‚‹èª¤åˆ†é¡åˆ†æã®é‡è¦ãªæ°—ã¥ã\n",
        "The detailed analysis of misclassification rate by FareBin, Pclass, and Sex reveals specific passenger groups that the model struggles with.\n",
        "\n",
        "#### ğŸš¨ High Misclassification Groups:\n",
        "- **(130â€“140], Pclass 1, male** â†’ 100% misclassified\n",
        "- **(240â€“250], Pclass 1, male** â†’ 100% misclassified\n",
        "- **(50â€“60], Pclass 3, male** â†’ 100%\n",
        "- **(70â€“80], Pclass 1, male** â†’ 100%\n",
        "\n",
        "These are high-fare, male passengers in 1st class who the model predicted incorrectly.  \n",
        "This is surprising, as such passengers often had a higher chance of survival historically.\n",
        "\n",
        "â†’ **Hypothesis**: These may be exceptional cases (e.g., solo male travelers who perished), or the model overestimates survival based on fare/class alone.\n",
        "\n",
        "#### âœ… Low Misclassification Groups:\n",
        "- Many **high-fare females in 1st class** were correctly classified (0% error)\n",
        "- **Low-fare males in 3rd class** also had relatively low misclassification (e.g. 0.08 in (0â€“10] bin)\n",
        "\n",
        "These results suggest the model relies heavily on **Sex, Fare, and Pclass** features, and may struggle with **outliers or overlapping distributions**.\n",
        "\n",
        "---\n",
        "\n",
        "é‹è³ƒãƒ»ç­‰ç´šãƒ»æ€§åˆ¥ã”ã¨ã®èª¤åˆ†é¡ç‡ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ˜ç¢ºãªå‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "#### ğŸš¨ èª¤åˆ†é¡ç‡ãŒé«˜ã„ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ï¼‰:\n",
        "- **(130â€“140], 1ç­‰èˆ¹å®¤, ç”·æ€§** â†’ 100% èª¤åˆ†é¡\n",
        "- **(240â€“250], 1ç­‰èˆ¹å®¤, ç”·æ€§** â†’ 100% èª¤åˆ†é¡\n",
        "- **(50â€“60], 3ç­‰èˆ¹å®¤, ç”·æ€§** â†’ 100%\n",
        "- **(70â€“80], 1ç­‰èˆ¹å®¤, ç”·æ€§** â†’ 100%\n",
        "\n",
        "ã“ã‚Œã‚‰ã¯ã€Œé«˜é‹è³ƒ Ã— ç”·æ€§ Ã— 1ç­‰èˆ¹å®¤ã€ã§ã€æœ¬æ¥ãªã‚‰ç”Ÿå­˜ç‡ãŒé«˜ã„å‚¾å‘ãŒã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ãƒ¢ãƒ‡ãƒ«ãŒèª¤ã£ã¦äºˆæ¸¬ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "â†’ **ä»®èª¬**ï¼šå˜ç‹¬ã®ç”·æ€§æ—…è¡Œè€…ãªã©ã€ä»–ã®ç‰¹å¾´ã«ã‚ˆã‚Šç”Ÿå­˜ç‡ãŒä¸‹ãŒã£ã¦ã„ã‚‹ãŒã€ãƒ¢ãƒ‡ãƒ«ãŒ `Fare` ã‚„ `Pclass` ã«éåº¦ã«ä¾å­˜ã—ã¦ã—ã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "#### âœ… æ­£ã—ãåˆ†é¡ã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«ã®å¾—æ„åˆ†é‡ï¼‰:\n",
        "- é«˜é‹è³ƒã®å¥³æ€§ï¼ˆç‰¹ã«1ç­‰èˆ¹å®¤ï¼‰ã¯å¤šããŒæ­£ã—ãåˆ†é¡ã•ã‚Œã¦ã„ã‚‹\n",
        "- 3ç­‰èˆ¹å®¤ã®ä½é‹è³ƒç”·æ€§ã‚‚ä¸€éƒ¨ã§èª¤åˆ†é¡ç‡ãŒã‹ãªã‚Šä½ã„ï¼ˆä¾‹ï¼š(0â€“10]ã§ 0.08ï¼‰\n",
        "\n",
        "ã“ã‚Œã‚‰ã®å‚¾å‘ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã¯ **æ€§åˆ¥ãƒ»ç­‰ç´šãƒ»é‹è³ƒã«å¼·ãä¾å­˜**ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚  \n",
        "ãã®ä¸€æ–¹ã§ã€ã“ã‚Œã‚‰ãŒäº¤å·®ã—ã¦ã€Œä¾‹å¤–çš„ãªå±æ€§ã€ã«ãªã‚‹ã¨èª¤åˆ†é¡ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9yxak3dC6X"
      },
      "source": [
        "## 4.22 Misclassification Rate by Pclass and Sex / ç­‰ç´šã¨æ€§åˆ¥ã«ã‚ˆã‚‹èª¤åˆ†é¡ç‡ã®å¯è¦–åŒ–  \n",
        "To understand which broad demographic groups the model struggles with,  \n",
        "we calculate and plot misclassification rates by `Pclass` and `Sex`.\n",
        "\n",
        "`Pclassï¼ˆä¹—å®¢ç­‰ç´šï¼‰Ã— Sexï¼ˆæ€§åˆ¥ï¼‰` ã®çµ„ã¿åˆã‚ã›ã”ã¨ã®èª¤åˆ†é¡ç‡ã‚’ç®—å‡ºã—ã€æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ãªå±æ€§ã«å¯¾ã—ã¦èª¤åˆ†é¡ã—ã‚„ã™ã„ã‹ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM4j0VpYgJpV"
      },
      "outputs": [],
      "source": [
        "# Group by Pclass and Sex / ç­‰ç´šã¨æ€§åˆ¥ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
        "total = df_valid.groupby(['Pclass', 'Sex_str'])['Correct'].agg(['count', 'sum'])\n",
        "\n",
        "# Misclassification Rate = 1 - æ­£è§£ç‡\n",
        "total['Misclassification Rate'] = 1 - total['sum'] / total['count']\n",
        "\n",
        "# Pivot for bar chart and plot / ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆç”¨ã«ãƒ”ãƒœãƒƒãƒˆã—ã¦å¯è¦–åŒ–\n",
        "total['Misclassification Rate'].unstack().plot(kind='bar', title='Misclassification Rate by Pclass & Sex')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzWazHjbZ5z"
      },
      "source": [
        "### Class-Gender Distribution and Its Impact / ç­‰ç´š Ã— æ€§åˆ¥ã®åã‚Šã¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¸ã®å½±éŸ¿  \n",
        "Upon analyzing the misclassification rates, we notice a **class-gender imbalance**:\n",
        "\n",
        "- In **1st class**, all misclassified passengers were **male**.\n",
        "- In **2nd and 3rd class**, **females outnumber males** in the misclassified samples.\n",
        "\n",
        "ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿå­˜ç‡ã‚’åˆ¤æ–­ã™ã‚‹éš›ã«ã€Œ1ç­‰ â†’ é«˜é‹è³ƒ â†’ ç”Ÿå­˜ã€ã¨å˜ç´”ã«å­¦ç¿’ã—ã¦ã—ã¾ã„ã€  \n",
        "1ç­‰ã®ç”·æ€§ï¼ˆå®Ÿéš›ã«ã¯æ­»äº¡è€…ã‚‚å¤šã„ï¼‰ã‚’éå‰°ã«ç”Ÿå­˜ã¨äºˆæ¸¬ã—ã¦ã—ã¾ã†ä¸€å› ã¨ãªã£ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "#### Key Observations / ä¸»ãªè¦³å¯Ÿçµæœ:\n",
        "\n",
        "- **1st class passengers** in the misclassified group were predominantly male (17 males, 0 females).\n",
        "- **2nd and 3rd class** misclassified passengers had **more females than males**.\n",
        "- This gender imbalance varies **by class**, and may skew model behavior.\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã¯ `Pclass`, `Sex`, `Fare` ã‚’å¼·ãé‡è¦–ã—ã¦ã„ã‚‹ãŸã‚ã€  \n",
        "ã“ã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®åã‚ŠãŒã€ãƒ¢ãƒ‡ãƒ«ã® **å­¦ç¿’ãƒã‚¤ã‚¢ã‚¹** ã«ã¤ãªãŒã£ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "This insight suggests a need to either:\n",
        "- Engineer more nuanced features (e.g., family size, deck, group survival),\n",
        "- Or use methods that reduce overfitting to correlated features.\n",
        "\n",
        "â†’ ã¤ã¾ã‚Šã€**Pclass ã‚„ Sex ã«ä¾å­˜ã—ã™ããªã„ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**ã€  \n",
        "ã¾ãŸã¯ **äº¤å·®é …ã‚„éç·šå½¢ãƒ¢ãƒ‡ãƒ«** ã®å°å…¥ãŒæœ‰åŠ¹ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWpJaVkrjAB2"
      },
      "source": [
        "## 4.23 Misclassification Rate Heatmap by Pclass & Sex / å®¢å®¤ç­‰ç´šã¨æ€§åˆ¥ã”ã¨ã®èª¤åˆ†é¡ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
        "This heatmap visualizes the misclassification rates by passenger class (Pclass) and gender (Sex).\n",
        "Darker colors indicate higher misclassification rates, helping us identify which groups the model struggles with.\n",
        "\n",
        "ã“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ã€ä¹—å®¢ã®å®¢å®¤ç­‰ç´šï¼ˆPclassï¼‰ã¨æ€§åˆ¥ï¼ˆSexï¼‰ã”ã¨ã®èª¤åˆ†é¡ç‡ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "è‰²ãŒæ¿ƒã„ã»ã©èª¤åˆ†é¡ç‡ãŒé«˜ãã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®äºˆæ¸¬ã§è‹¦æˆ¦ã—ã¦ã„ã‚‹ã‹ã‚’è¦–è¦šçš„ã«ç†è§£ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQrmJL7fy_lM"
      },
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ”ãƒœãƒƒãƒˆã—ã¦ã€è¡Œ: Pclassã€åˆ—: Sex_str ã«å¤‰æ›\n",
        "heatmap_data = total['Misclassification Rate'].unstack()\n",
        "\n",
        "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"Reds\", linewidths=0.5)\n",
        "plt.title('Misclassification Rate Heatmap by Pclass & Sex')\n",
        "plt.ylabel('Pclass')\n",
        "plt.xlabel('Sex')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SulCj_rk-8w"
      },
      "source": [
        "### ğŸ” Interpretation of Heatmap / ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è§£é‡ˆ\n",
        "From the heatmap, we observe that 1st class males (Pclass=1, Sex=male) and 3rd class females (Pclass=3, Sex=female) have the highest misclassification rates, at 0.39 and 0.34 respectively.\n",
        "These darker cells indicate that the model struggles particularly with these two demographic groups.\n",
        "\n",
        "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‹ã‚‰ã€1ç­‰å®¢å®¤ã®ç”·æ€§ï¼ˆPclass=1, Sex=maleï¼‰ã¨3ç­‰å®¢å®¤ã®å¥³æ€§ï¼ˆPclass=3, Sex=femaleï¼‰ã®èª¤åˆ†é¡ç‡ãŒç‰¹ã«é«˜ãã€ãã‚Œãã‚Œ 0.39ã€0.34 ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "ã“ã‚Œã‚‰ã®ã‚»ãƒ«ã®è‰²ãŒæ¿ƒã„ã“ã¨ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ãŒã“ã®2ã¤ã®å±æ€§ã®ä¹—å®¢ã«å¯¾ã—ã¦ã€ç‰¹ã«äºˆæ¸¬ã‚’èª¤ã‚Šã‚„ã™ã„å‚¾å‘ãŒã‚ã‚‹ã¨åˆ†ã‹ã‚Šã¾ã™ã€‚  \n",
        "### ğŸ› ï¸ Implications for Model Improvement / ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã¸ã®ç¤ºå”†\n",
        "These findings suggest that additional feature engineering â€” such as creating interaction terms (e.g., Pclass x Sex) or incorporating survival rate priors â€” may help the model better capture these complex patterns.\n",
        "\n",
        "ã“ã‚Œã‚‰ã®çµæœã¯ã€ä¾‹ãˆã° Pclass Ã— Sex ã®ã‚ˆã†ãªäº¤äº’ä½œç”¨ç‰¹å¾´é‡ã‚’ä½œæˆã—ãŸã‚Šã€ç”Ÿå­˜ç‡ã®äº‹å‰çŸ¥è­˜ã‚’ç‰¹å¾´ã«çµ„ã¿è¾¼ã‚€ãªã©ã€ã•ã‚‰ãªã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8XNEhbbOo2S"
      },
      "source": [
        "## 4.24 ğŸ“ˆ SHAP Analysis of Top Features + Key Additions / ä¸Šä½ç‰¹å¾´é‡ï¼‹è¿½åŠ ç‰¹å¾´é‡ã®SHAPè§£æ  \n",
        "To better understand the model's decision-making process, we analyze SHAP values for the top 80% most important features, along with the following features that significantly improved the model's score:\n",
        "\n",
        "\n",
        "*   S (Embarked = S)\n",
        "*   AgeGroup_Adult\n",
        "*   Deck_B\n",
        "*   Pclass_1\n",
        "*   IsGroup\n",
        "\n",
        "ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯ã€ã‚¹ã‚³ã‚¢å‘ä¸Šã«å¤§ããè²¢çŒ®ã—ãŸãŸã‚ã€ä¸Šä½80%ã®é‡è¦ç‰¹å¾´é‡ã«åŠ ãˆã¦ SHAP å€¤ã‚’å¯è¦–åŒ–ã—ã¦åˆ†æã—ã¾ã™ã€‚\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ãŒã©ã®æƒ…å ±ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ã€ãã—ã¦å„ç‰¹å¾´ãŒäºˆæ¸¬ã«ã©ã‚Œã»ã©ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã®ãŒç›®çš„ã§ã™ã€‚\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) values allow us to:\n",
        "\n",
        "*   Visualize the magnitude and direction of each feature's contribution to theprediction\n",
        "*   Understand which features push the prediction higher (toward survival) or lower (toward not surviving) for each passenger\n",
        "\n",
        "This analysis provides interpretability and transparency, helping us not only to validate the model's reasoning but also to identify potential biases or over-reliance on specific features.  \n",
        "\n",
        "SHAPï¼ˆSHapleyåŠ æ³•çš„èª¬æ˜ï¼‰å€¤ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€æ¬¡ã®ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š\n",
        "\n",
        "*   å„ç‰¹å¾´é‡ãŒäºˆæ¸¬ã«ä¸ãˆã‚‹**å½±éŸ¿ã®å¤§ãã•ã¨æ–¹å‘ï¼ˆæ­£ã‹è² ã‹ï¼‰**ã‚’å¯è¦–åŒ–\n",
        "*   å„ä¹—å®¢ã«ãŠã„ã¦ã€ã©ã®ç‰¹å¾´ãŒç”Ÿå­˜ã®æ–¹å‘ã«äºˆæ¸¬ã‚’æŠ¼ã—ä¸Šã’ãŸã‹ã€ã¾ãŸã¯éç”Ÿå­˜ã®æ–¹å‘ã«æŠ¼ã—ä¸‹ã’ãŸã‹ã‚’ç†è§£ã™ã‚‹\n",
        "\n",
        "ã“ã®åˆ†æã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ã®åˆ¤æ–­åŸºæº–ã‚’è§£é‡ˆå¯èƒ½ã«ã—ã€å¦¥å½“ãªäºˆæ¸¬ã‚’ã—ã¦ã„ã‚‹ã‹ã®æ¤œè¨¼ã‚„ã€ç‰¹å®šã®ç‰¹å¾´ã¸ã®éåº¦ãªä¾å­˜ã‚„åã‚Šã®æ¤œå‡ºãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YtHmjW0XPhU"
      },
      "outputs": [],
      "source": [
        "pip install shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rFkfJF6sRsn"
      },
      "source": [
        "### 4.24.1 ğŸ” SHAP Value Summary (Bar Plot Interpretation) / SHAPå€¤ã‚µãƒãƒªï¼ˆãƒãƒ¼ãƒ»ãƒ—ãƒ­ãƒƒãƒˆã®è§£é‡ˆï¼‰\n",
        "The SHAP bar plot shows the mean absolute SHAP values for each feature, indicating how much each feature contributes, on average, to the modelâ€™s output (regardless of direction).\n",
        "This helps us understand which features the model relies on most heavily in making predictions.\n",
        "\n",
        "SHAP ã®ãƒãƒ¼ãƒ»ãƒ—ãƒ­ãƒƒãƒˆã¯ã€å„ç‰¹å¾´é‡ã®å¹³å‡çµ¶å¯¾SHAPå€¤ã‚’ç¤ºã—ã¦ãŠã‚Šã€äºˆæ¸¬ã«å¯¾ã—ã¦ãã®ç‰¹å¾´é‡ãŒå¹³å‡ã—ã¦ã©ã‚Œã»ã©å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’è¡¨ã—ã¦ã„ã¾ã™ï¼ˆæ­£æ–¹å‘ãƒ»è² æ–¹å‘ã‚’å•ã‚ãšï¼‰ã€‚\n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®æƒ…å ±ã«æœ€ã‚‚ä¾å­˜ã—ã¦äºˆæ¸¬ã—ã¦ã„ã‚‹ã‹ã‚’ç†è§£ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xukwEjN6aX0n"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# 1. Data Preparation / ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
        "# -------------------------\n",
        "# Final feature set including selected features and manually added impactful ones\n",
        "# é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã«åŠ ãˆã€ã‚¹ã‚³ã‚¢å‘ä¸Šã«å¯„ä¸ã—ãŸè¿½åŠ ç‰¹å¾´é‡ã‚’å«ã‚ãŸæœ€çµ‚ã‚»ãƒƒãƒˆ\n",
        "extended_features = selected_features + ['S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup']\n",
        "\n",
        "X_extended_df = df_fe4[extended_features]  # Feature matrix / ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
        "y_extended = df_fe4['Survived']           # Target variable / ç›®çš„å¤‰æ•°ï¼ˆç”Ÿå­˜ãƒ•ãƒ©ã‚°ï¼‰\n",
        "\n",
        "# -------------------------\n",
        "# 2. Model Training / ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
        "# -------------------------\n",
        "# Train a Random Forest model on the extended feature set\n",
        "# æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’å­¦ç¿’\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_extended_df, y_extended)\n",
        "\n",
        "# -------------------------\n",
        "# 3. SHAP\n",
        "# -------------------------\n",
        "# Use TreeExplainer for SHAP value calculation (compatible with tree-based models)\n",
        "# æœ¨æ§‹é€ ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ãŸ TreeExplainer ã‚’ä½¿ã£ã¦ SHAP å€¤ã‚’è¨ˆç®—\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer(X_extended_df)  # Returns a shap.Explanation object\n",
        "                                        # shap.Explanation ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã•ã‚Œã‚‹ï¼ˆã‚µãƒ³ãƒ—ãƒ«Ã—ç‰¹å¾´é‡Ã—ã‚¯ãƒ©ã‚¹ï¼‰\n",
        "\n",
        "# -------------------------\n",
        "# 4. Verification and Debug Output / æ¤œè¨¼ã¨å‡ºåŠ›\n",
        "# -------------------------\n",
        "# Print the number of SHAP value samples and the shape of the first element\n",
        "# SHAP å€¤ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã¨æœ€åˆã®è¦ç´ ã®å½¢çŠ¶ã‚’å‡ºåŠ›ã—ã¦ç¢ºèª\n",
        "print(\"âœ… shap_values shape:\", len(shap_values), shap_values[0].shape)\n",
        "\n",
        "# Print the names of the features used\n",
        "# ä½¿ç”¨ã•ã‚ŒãŸç‰¹å¾´é‡åã‚’å‡ºåŠ›\n",
        "print(\"âœ… ç‰¹å¾´é‡å:\", X_extended_df.columns.tolist())\n",
        "\n",
        "# -------------------------\n",
        "# 5. Visualization (Bar Plot) / å¯è¦–åŒ–ï¼ˆbar ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "# -------------------------\n",
        "# Since this is a binary classification, we use only the SHAP values for class 1 (Survived=1)\n",
        "# 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ãŸã‚ã€ã‚¯ãƒ©ã‚¹1ï¼ˆ=ç”Ÿå­˜ï¼‰ã® SHAP å€¤ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹\n",
        "shap.summary_plot(shap_values[..., 1], X_extended_df, plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIeof-GpMSXr"
      },
      "source": [
        "### ğŸ“Œ Top Features Identified /  ç‰¹ã«å½±éŸ¿ã®å¤§ãã‹ã£ãŸç‰¹å¾´é‡\n",
        "\n",
        "1. **`Title_Mr`** â€“ This feature had the highest SHAP value. The model strongly uses this title to identify male adults, who historically had lower survival rates.\n",
        "\n",
        "2. **`Sex`** â€“ The gender of the passenger remains one of the strongest predictors of survival. Female passengers had much higher survival rates due to the \"women and children first\" evacuation policy.\n",
        "\n",
        "3. **`Pclass_3`** â€“ Being in 3rd class negatively influences survival predictions. This suggests the model learned that lower-class passengers had fewer chances of survival.\n",
        "\n",
        "These findings validate the modelâ€™s logic and are consistent with known patterns from the Titanic dataset.  \n",
        "\n",
        "\n",
        "---\n",
        "1. **`Title_Mr`** â€“ æœ€ã‚‚SHAPå€¤ãŒé«˜ã‹ã£ãŸç‰¹å¾´ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã“ã®æ•¬ç§°ã‚’åˆ©ç”¨ã—ã¦ã€å¤§äººã®ç”·æ€§ï¼ˆæ­´å²çš„ã«ç”Ÿå­˜ç‡ãŒä½ã‹ã£ãŸï¼‰ã‚’ç‰¹å®šã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "2. **`Sex`** â€“ æ€§åˆ¥ã¯ä¾ç„¶ã¨ã—ã¦ç”Ÿå­˜äºˆæ¸¬ã«ãŠã„ã¦éå¸¸ã«å¼·åŠ›ãªç‰¹å¾´ã§ã™ã€‚å¥³æ€§ã¯ã€Œå¥³æ€§ã¨å­ä¾›ã‚’å„ªå…ˆã™ã‚‹ã€ã¨ã„ã†é¿é›£æ–¹é‡ã®å½±éŸ¿ã§ã€ç”Ÿå­˜ç‡ãŒé«˜ã‹ã£ãŸãŸã‚ã§ã™ã€‚\n",
        "\n",
        "3. **`Pclass_3`** â€“ 3ç­‰å®¢å®¤ã®ä¹—å®¢ã§ã‚ã‚‹ã“ã¨ã¯ã€ç”Ÿå­˜äºˆæ¸¬ã«ãƒã‚¤ãƒŠã‚¹ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ãŒã€Œä¸‹å±¤ã‚¯ãƒ©ã‚¹ã®ä¹—å®¢ã¯ç”Ÿå­˜ã®å¯èƒ½æ€§ãŒä½ã„ã€ã¨ã„ã†å‚¾å‘ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã®çµæœã¯ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ãŒå¦¥å½“ã§ã‚ã‚Šã€Titanicãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹å‚¾å‘ã¨ã‚‚ä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’è£ä»˜ã‘ã¦ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-i3w_OdtuHo"
      },
      "source": [
        "## 4.24.2 ğŸ“ˆ SHAP Summary Plot (Dot Plot) / SHAPã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒƒãƒˆï¼‰  \n",
        "\n",
        "This plot shows the distribution of SHAP values for each feature across all samples.  \n",
        "Each point represents a SHAP value for a single passenger, and its color reflects the feature value (e.g., red = high, blue = low).\n",
        "\n",
        "From the plot, we can observe not only which features are important, but also **how** they affect the prediction direction (positive â†’ survival, negative â†’ non-survival).  \n",
        "For example, higher values of `Sex` (i.e., male = 1) tend to push predictions toward non-survival (left), while lower values (female = 0) push toward survival (right).\n",
        "\n",
        "This helps reveal **nonlinear relationships and interaction effects** that may not be obvious from feature importance alone.  \n",
        "ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã¯ã€å„ç‰¹å¾´é‡ã«ãŠã‘ã‚‹å…¨ã‚µãƒ³ãƒ—ãƒ«ã®SHAPå€¤ã®åˆ†å¸ƒã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚  \n",
        "1ã¤1ã¤ã®ç‚¹ãŒ1äººã®ä¹—å®¢ã«ãŠã‘ã‚‹SHAPå€¤ã‚’è¡¨ã—ã¦ãŠã‚Šã€è‰²ã¯ãã®ç‰¹å¾´é‡ã®å€¤ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼ˆèµ¤ = é«˜ã„å€¤ã€é’ = ä½ã„å€¤ï¼‰ã€‚\n",
        "\n",
        "ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’é€šã—ã¦ã€é‡è¦ãªç‰¹å¾´é‡ã ã‘ã§ãªãã€ãã‚Œã‚‰ãŒ**äºˆæ¸¬ã«ã©ã®ã‚ˆã†ãªæ–¹å‘ã§å½±éŸ¿ã—ã¦ã„ã‚‹ã‹**ï¼ˆå³ = ç”Ÿå­˜æ–¹å‘ã€å·¦ = éç”Ÿå­˜æ–¹å‘ï¼‰ã‚‚ç¢ºèªã§ãã¾ã™ã€‚  \n",
        "ä¾‹ãˆã°ã€`Sex`ã®å€¤ãŒé«˜ã„ï¼ˆ= ç”·æ€§ï¼‰ã¨äºˆæ¸¬ãŒéç”Ÿå­˜å´ï¼ˆå·¦ï¼‰ã«æŠ¼ã•ã‚Œã€ä½ã„ï¼ˆ= å¥³æ€§ï¼‰ã¨ç”Ÿå­˜å´ï¼ˆå³ï¼‰ã«æŠ¼ã•ã‚Œã‚‹å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "å˜ãªã‚‹ç‰¹å¾´é‡ã®é‡è¦åº¦ã§ã¯è¦‹ãˆã«ãã„ã€**éç·šå½¢ãªé–¢ä¿‚æ€§ã‚„ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨**ã‚’ç†è§£ã™ã‚‹æ‰‹ãŒã‹ã‚Šã«ã‚‚ãªã‚Šã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfyvK9rcL7d6"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[..., 1], X_extended_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Wd-2UBwgJU"
      },
      "source": [
        "### ğŸ” Detailed Interpretation of SHAP Summary Plot / SHAPã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã®è©³ç´°ãªè§£é‡ˆ\n",
        "\n",
        "From the SHAP dot plot, we can derive how specific feature values push the modelâ€™s predictions toward survival or non-survival.\n",
        "\n",
        "- **Title_Mr**: Most of the red points (high value = Mr) are concentrated on the **left side**, indicating a strong negative impact on survival prediction. The model consistently associates this title with lower survival probability.\n",
        "\n",
        "- **Sex**: Blue points (female = 0) are mainly located on the **left**, meaning that in some cases the model predicts lower survival probability for females. Red points (male = 1) are often on the **right**, showing that being male sometimes increases the prediction. This may indicate complex interactions with other features like `Title` or `Pclass`.\n",
        "\n",
        "- **Pclass_3**: Red points (3rd class = 1) are slightly more concentrated on the **left**, meaning that being in 3rd class tends to lower the survival prediction. Blue points (not 3rd class) are more on the **right**, pushing predictions toward survival. However, the distribution is more balanced compared to the other features, suggesting that the impact of `Pclass_3` is less extreme but still significant.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "SHAPã®ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒƒãƒˆã‚’åˆ†æã™ã‚‹ã“ã¨ã§ã€ç‰¹å®šã®ç‰¹å¾´å€¤ãŒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’ã€Œç”Ÿå­˜ã€ã€Œéç”Ÿå­˜ã€ã®ã©ã¡ã‚‰ã«æŠ¼ã—ã¦ã„ã‚‹ã‹ã‚’ç†è§£ã§ãã¾ã™ã€‚\n",
        "\n",
        "- **Title_Mr**ï¼šèµ¤ã„ç‚¹ï¼ˆé«˜ã„å€¤ = Mrï¼‰ãŒ**å·¦å´**ã«å¤šãåˆ†å¸ƒã—ã¦ãŠã‚Šã€ç”Ÿå­˜äºˆæ¸¬ã«å¯¾ã—ã¦å¼·ã„ãƒã‚¤ãƒŠã‚¹ã®å½±éŸ¿ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã“ã®æ•¬ç§°ã‚’ã€Œç”Ÿå­˜ç‡ãŒä½ã„äººç‰©ã€ã¨ã—ã¦ä¸€è²«ã—ã¦æ‰±ã£ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚\n",
        "\n",
        "- **Sex**ï¼šé’ã„ç‚¹ï¼ˆå¥³æ€§ = 0ï¼‰ãŒ**å·¦å´**ã«å¤šãåˆ†å¸ƒã—ã€å¥³æ€§ãŒç”Ÿå­˜ã—ã«ãã„ã¨äºˆæ¸¬ã•ã‚ŒãŸã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚é€†ã«èµ¤ã„ç‚¹ï¼ˆç”·æ€§ = 1ï¼‰ã¯**å³å´**ã«åˆ†å¸ƒã—ã¦ãŠã‚Šã€ç”·æ€§ã§ã‚ã£ã¦ã‚‚ç”Ÿå­˜äºˆæ¸¬ãŒé«˜ããªã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€`Title` ã‚„ `Pclass` ãªã©ä»–ã®ç‰¹å¾´é‡ã¨ã®**ç›¸äº’ä½œç”¨**ã®å½±éŸ¿ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- **Pclass_3**ï¼šèµ¤ã„ç‚¹ï¼ˆ3ç­‰å®¢å®¤ = 1ï¼‰ã¯**å·¦å´**ã«ã‚„ã‚„å¤šãåˆ†å¸ƒã—ã€ç”Ÿå­˜äºˆæ¸¬ã‚’ä¸‹ã’ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚é’ã„ç‚¹ï¼ˆãã‚Œä»¥å¤–ï¼‰ã¯**å³å´**ã«å¤šãã€ç”Ÿå­˜ã®æ–¹å‘ã«å½±éŸ¿ã—ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€ä»–ã®ç‰¹å¾´ã«æ¯”ã¹ã¦åˆ†å¸ƒã¯ã‚ˆã‚Šãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ãŠã‚Šã€`Pclass_3`ã®å½±éŸ¿ã¯æ¥µç«¯ã§ã¯ãªã„ã‚‚ã®ã®ã€ä¾ç„¶ã¨ã—ã¦é‡è¦ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtwClIjTyyii"
      },
      "source": [
        "## 4.24.3 ğŸ” SHAP Waterfall Plot: Individual Prediction Breakdown / SHAPã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ—ãƒ­ãƒƒãƒˆï¼šå€‹åˆ¥äºˆæ¸¬ã®å†…è¨³\n",
        "\n",
        "To understand **how the model makes predictions for a single passenger**, we used the SHAP waterfall plot.  \n",
        "It visualizes how each feature pushed the prediction score up or down starting from the base value.\n",
        "\n",
        "Below is the waterfall plot for passenger **#300**.\n",
        "\n",
        "- Red bars show features that **increase** the predicted probability of survival.\n",
        "- Blue bars show features that **decrease** the probability.\n",
        "\n",
        "This plot clearly shows **which features were most responsible** for this particular prediction.  \n",
        "It complements the summary plot by adding a case-level explanation.  \n",
        "\n",
        "---\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ãŒ**ç‰¹å®šã®ä¹—å®¢ã«å¯¾ã—ã¦ã©ã®ã‚ˆã†ã«äºˆæ¸¬ã‚’è¡Œã£ã¦ã„ã‚‹ã‹**ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€SHAPã®ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½¿ã„ã¾ã—ãŸã€‚  \n",
        "ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã§ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå¹³å‡çš„ãªäºˆæ¸¬å€¤ï¼‰ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦ã€å„ç‰¹å¾´é‡ãŒäºˆæ¸¬ã‚’ã©ã‚Œã ã‘æŠ¼ã—ä¸Šã’ãŸï¼æŠ¼ã—ä¸‹ã’ãŸã‹ã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ã€‚\n",
        "\n",
        "ä»¥ä¸‹ã¯ã€ä¹—å®¢**#300**ã®ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ—ãƒ­ãƒƒãƒˆã§ã™ã€‚\n",
        "\n",
        "- èµ¤ã„ãƒãƒ¼ã¯ã€ç”Ÿå­˜ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’**ä¸Šã’ãŸè¦å› **  \n",
        "- é’ã„ãƒãƒ¼ã¯ã€ç”Ÿå­˜ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’**ä¸‹ã’ãŸè¦å› **\n",
        "\n",
        "ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚Šã€ã€Œãªãœã“ã®ä¹—å®¢ãŒã“ã†äºˆæ¸¬ã•ã‚ŒãŸã®ã‹ã€ã‚’**ç‰¹å¾´é‡ã”ã¨ã«æ˜ç¢ºã«å¯è¦–åŒ–**ã§ãã¾ã™ã€‚  \n",
        "å…¨ä½“å‚¾å‘ã‚’ç¤ºã™summary plotã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€åˆ†æã®æ·±ã¿ã¨ç´å¾—æ„ŸãŒå¤§ããå¢—ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR-xV0KLzuWh"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 1. Extract SHAP values for class 1 (Survived) / ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜ï¼‰ã«å¯¾ã™ã‚‹SHAPå€¤ã‚’æŠ½å‡º\n",
        "# -------------------------\n",
        "# The shape of shap_values.values is (891, 14, 2): 891 passengers Ã— 14 features Ã— 2 classes\n",
        "# shap_values.values ã®å½¢ã¯ (891, 14, 2)ï¼š891äºº Ã— 14ç‰¹å¾´é‡ Ã— 2ã‚¯ãƒ©ã‚¹\n",
        "# So we extract only the SHAP values for class 1 (survived)\n",
        "# ãã®ã†ã¡ã€Œã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜ï¼‰ã€ã«å¯¾ã™ã‚‹SHAPå€¤ã ã‘ã‚’å–ã‚Šå‡ºã™\n",
        "values = shap_values.values[:, :, 1]  # Resulting shape is (891, 14) / çµæœã®å½¢ã¯ (891, 14)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Extract base values (baseline prediction per sample) / ãƒ™ãƒ¼ã‚¹å€¤ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®äºˆæ¸¬ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ã‚’æŠ½å‡º\n",
        "# -------------------------\n",
        "# Base values represent the expected output of the model before seeing any features\n",
        "# ãƒ™ãƒ¼ã‚¹å€¤ã¯ã€ç‰¹å¾´é‡ã‚’è€ƒæ…®ã™ã‚‹å‰ã®ã€Œå¹³å‡çš„ãªäºˆæ¸¬å€¤ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ã€ã‚’è¡¨ã™\n",
        "# If the SHAP explanation is for multiple classes, we extract class 1's base values\n",
        "# è¤‡æ•°ã‚¯ãƒ©ã‚¹ã®å ´åˆã¯ã€ç”Ÿå­˜ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã®ãƒ™ãƒ¼ã‚¹å€¤ã‚’å–ã‚Šå‡ºã™\n",
        "base_values = shap_values.base_values[:, 1] if shap_values.base_values.ndim > 1 else shap_values.base_values\n",
        "\n",
        "# -------------------------\n",
        "# 3. Rebuild a SHAP Explanation object for class 1 only / ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜ï¼‰ã ã‘ã® SHAP Explanation ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†æ§‹ç¯‰\n",
        "# -------------------------\n",
        "# This allows us to use SHAP plots like waterfall for class 1 (survived) only\n",
        "# ã“ã‚Œã«ã‚ˆã‚Šã€ç”Ÿå­˜ã‚¯ãƒ©ã‚¹ã®SHAPå€¤ã ã‘ã‚’ä½¿ã£ãŸã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãªã©ã®ãƒ—ãƒ­ãƒƒãƒˆãŒå¯èƒ½ã«ãªã‚‹\n",
        "class1_shap = shap.Explanation(\n",
        "    values=values,                     # SHAP values for class 1 / ã‚¯ãƒ©ã‚¹1ã®SHAPå€¤\n",
        "    base_values=base_values,          # Base values for class 1 / ã‚¯ãƒ©ã‚¹1ã®ãƒ™ãƒ¼ã‚¹å€¤\n",
        "    data=X_extended_df,               # Original input data / å…ƒã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
        "    feature_names=X_extended_df.columns  # Feature names / ç‰¹å¾´é‡å\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4. Plot SHAP waterfall for a specific passenger / ç‰¹å®šã®ä¹—å®¢ã«å¯¾ã™ã‚‹SHAPã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”»\n",
        "# -------------------------\n",
        "# Waterfall plot shows how each feature contributes to the final prediction step by step\n",
        "# ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒ—ãƒ­ãƒƒãƒˆã¯ã€å„ç‰¹å¾´é‡ãŒæœ€çµ‚äºˆæ¸¬ã«æ®µéšçš„ã«ã©ã®ã‚ˆã†ã«å¯„ä¸ã—ãŸã‹ã‚’ç¤ºã™\n",
        "shap.plots.waterfall(class1_shap[300]) # Example: passenger #300 / ä¾‹ï¼š300ç•ªç›®ã®ä¹—å®¢"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m27GwirBy4Vf"
      },
      "source": [
        "### ğŸš© Key Positive Contributors for This Passenger / ã“ã®ä¹—å®¢ã«å¯¾ã™ã‚‹ä¸»è¦ãªãƒ—ãƒ©ã‚¹ã®å¯„ä¸ç‰¹å¾´é‡\n",
        "\n",
        "For this passenger (#300), the top features that increased the predicted survival probability were:\n",
        "\n",
        "- **Title_Mr** with a SHAP value of +0.18  \n",
        "- **Sex** with a SHAP value of +0.13  \n",
        "- **S** (embarked at Southampton) with a SHAP value of +0.08  \n",
        "\n",
        "These features pushed the modelâ€™s prediction towards survival.\n",
        "\n",
        "---\n",
        "\n",
        "ã“ã®ä¹—å®¢ï¼ˆ#300ï¼‰ã«ãŠã„ã¦ã€ç”Ÿå­˜äºˆæ¸¬ã‚’æŠ¼ã—ä¸Šã’ãŸä¸»ãªç‰¹å¾´é‡ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n",
        "\n",
        "- **Title_Mr**ï¼ˆæ•¬ç§°Mrï¼‰: SHAPå€¤ +0.18  \n",
        "- **Sex**ï¼ˆæ€§åˆ¥ï¼‰: SHAPå€¤ +0.13  \n",
        "- **S**ï¼ˆã‚µã‚¦ã‚µãƒ³ãƒ—ãƒˆãƒ³ä¹—èˆ¹ï¼‰: SHAPå€¤ +0.08  \n",
        "\n",
        "ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ãŒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’ç”Ÿå­˜æ–¹å‘ã¸ã¨å¾ŒæŠ¼ã—ã—ã¾ã—ãŸã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xb79dZl1ooB"
      },
      "source": [
        "## 4.24.4 SHAP Dependence Plots / SHAPä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "\n",
        "For each feature in our extended set, we plotted SHAP dependence plots.  \n",
        "These plots show how the value of a feature relates to its impact on the prediction, helping us understand feature interactions and nonlinear effects.\n",
        "\n",
        "æ‹¡å¼µç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®å„ç‰¹å¾´é‡ã«ã¤ã„ã¦ã€SHAPä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚  \n",
        "ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã¯ã€ç‰¹å¾´é‡ã®å€¤ãŒäºˆæ¸¬ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’ç¤ºã—ã€ç‰¹å¾´é–“ã®ç›¸äº’ä½œç”¨ã‚„éç·šå½¢åŠ¹æœã®ç†è§£ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQz5kTZK2E6B"
      },
      "outputs": [],
      "source": [
        "# Extract only SHAP values for class 1 (Survived=1)\n",
        "# ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜=1ï¼‰ã«å¯¾ã™ã‚‹SHAPå€¤ã ã‘ã‚’æŠ½å‡º\n",
        "class1_shap = shap_values[..., 1]\n",
        "\n",
        "# Create dependence plots for each feature in extended_features\n",
        "# extended_features ã«å«ã¾ã‚Œã‚‹å„ç‰¹å¾´é‡ã«ã¤ã„ã¦ã€SHAPä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ\n",
        "for feat in extended_features:\n",
        "    shap.dependence_plot(\n",
        "        feat,                         # Feature name / ç‰¹å¾´é‡å\n",
        "        shap_values=class1_shap.values,  # SHAP values for class 1 / ã‚¯ãƒ©ã‚¹1ã®SHAPå€¤\n",
        "        features=X_extended_df,          # Original input features / å…ƒã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿\n",
        "        feature_names=X_extended_df.columns  # Feature names / ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nefWbEhI2Qr9"
      },
      "source": [
        "### Interaction Effects Observed / è¦³å¯Ÿã•ã‚ŒãŸç‰¹å¾´é‡ã®ç›¸äº’ä½œç”¨åŠ¹æœ\n",
        "\n",
        "- **Sex Ã— Pclass_3**: Males in 3rd class have relatively higher survival rates compared to others.  \n",
        "  This indicates that the **interaction between gender and passenger class** strongly influences the prediction.\n",
        "\n",
        "- **Title_Mr Ã— Family**: Females with family members present tend to have lower survival rates.  \n",
        "  This suggests a strong **interaction between title (gender/age) and family presence** affecting survival predictions.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **æ€§åˆ¥ Ã— 3ç­‰å®¢å®¤ï¼ˆSex Ã— Pclass_3ï¼‰**: 3ç­‰å®¢å®¤ã®ç”·æ€§ã¯ä»–ã¨æ¯”ã¹ã¦æ¯”è¼ƒçš„ç”Ÿå­˜ç‡ãŒé«˜ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "  ã“ã‚Œã¯ã€**æ€§åˆ¥ã¨å®¢å®¤éšç´šã®ç›¸äº’ä½œç”¨**ãŒäºˆæ¸¬ã«å¼·ãå½±éŸ¿ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- **æ•¬ç§°Mr Ã— å®¶æ—ã®æœ‰ç„¡ï¼ˆTitle_Mr Ã— Familyï¼‰**: å®¶æ—ãŒã„ã‚‹å¥³æ€§ã¯ç”Ÿå­˜ç‡ãŒä½ã„å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚  \n",
        "  ã“ã‚Œã¯ã€**æ•¬ç§°ï¼ˆæ€§åˆ¥ã‚„å¹´é½¢ï¼‰ã¨å®¶æ—ã®æœ‰ç„¡ã®ç›¸äº’ä½œç”¨**ãŒç”Ÿå­˜äºˆæ¸¬ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-uB7iWy2evt"
      },
      "source": [
        "## 4.24.5 SHAP Dependence of Family by Title / ã‚¿ã‚¤ãƒˆãƒ«åˆ¥ã®å®¶æ—ç‰¹å¾´é‡ã®SHAPä¾å­˜é–¢ä¿‚\n",
        "\n",
        "We plotted the SHAP values of the 'Family' feature separately for passengers with titles Mr, Mrs, and Miss.  \n",
        "This allows us to compare how family size affects survival predictions differently depending on age and gender group.\n",
        "\n",
        "å®¶æ—ã®ç‰¹å¾´é‡ï¼ˆFamilyï¼‰ã®SHAPå€¤ã‚’ã€æ•¬ç§°ï¼ˆTitleï¼‰ãŒMrã€Mrsã€Missã®ä¹—å®¢åˆ¥ã«åˆ†ã‘ã¦æç”»ã—ã¾ã—ãŸã€‚  \n",
        "ã“ã‚Œã«ã‚ˆã‚Šã€å¹´é½¢ãƒ»æ€§åˆ¥ã®ç•°ãªã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã§å®¶æ—æ§‹æˆãŒç”Ÿå­˜äºˆæ¸¬ã«ã©ã®ã‚ˆã†ã«å½±éŸ¿ã™ã‚‹ã‹ã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txM93P_tnjmY"
      },
      "outputs": [],
      "source": [
        "for title in ['Title_Mr', 'Title_Mrs', 'Title_Miss']:\n",
        "    mask = X_extended_df[title] == 1  # Filter samples by title / æ•¬ç§°ã§ãƒ•ã‚£ãƒ«ã‚¿\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(\n",
        "        X_extended_df.loc[mask, 'Family'],  # x: Family size / xè»¸ï¼šå®¶æ—äººæ•°\n",
        "        class1_shap.values[mask, X_extended_df.columns.get_loc('Family')],  # y: SHAP value for Family / yè»¸ï¼šFamilyã®SHAPå€¤\n",
        "        alpha=0.6,\n",
        "        c='skyblue',  # è‰²ã‚’çµ±ä¸€ã™ã‚‹ã¨è¦‹ã‚„ã™ã„\n",
        "        edgecolor='k'\n",
        "    )\n",
        "    plt.title(f'SHAP Dependence of Family - {title}')  # ã‚¿ã‚¤ãƒˆãƒ«\n",
        "    plt.xlabel('Family Size')     # xè»¸ãƒ©ãƒ™ãƒ«\n",
        "    plt.ylabel('SHAP Value')      # yè»¸ãƒ©ãƒ™ãƒ«\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3bGpfDE3syb"
      },
      "source": [
        "### Observed Trends in Family Size SHAP Values by Title / ã‚¿ã‚¤ãƒˆãƒ«åˆ¥ã®å®¶æ—ã‚µã‚¤ã‚ºSHAPå€¤ã®å‚¾å‘\n",
        "\n",
        "- For **Mr**, family sizes between 1 and 3 are most common and have higher SHAP values than family sizes 4 to 10.  \n",
        "- For **Mrs**, family sizes between 1 and 4 are most common and have higher SHAP values than family sizes 5 to 10.  \n",
        "- For **Miss**, family sizes between 1 and 4 are most common and have higher SHAP values than family sizes 5 to 10.  \n",
        "\n",
        "This suggests that smaller family sizes tend to contribute more positively to survival predictions across these titles.  \n",
        "\n",
        "---\n",
        "\n",
        "- **Mr**ã§ã¯ã€å®¶æ—äººæ•°ãŒ1ã€œ3ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¤šãã€4ã€œ10ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚ˆã‚Šã‚‚é«˜ã„SHAPå€¤ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚  \n",
        "- **Mrs**ã§ã¯ã€å®¶æ—äººæ•°ãŒ1ã€œ4ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¤šãã€5ã€œ10ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚ˆã‚Šé«˜ã„SHAPå€¤ã§ã™ã€‚  \n",
        "- **Miss**ã§ã‚‚ã€å®¶æ—äººæ•°1ã€œ4ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¤šãã€5ã€œ10ã‚ˆã‚Šé«˜ã„SHAPå€¤ã®å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚  \n",
        "\n",
        "ã“ã‚Œã¯ã€ã“ã‚Œã‚‰ã®æ•¬ç§°ã®ä¹—å®¢ã«ãŠã„ã¦ã€å®¶æ—äººæ•°ãŒå°‘ãªã„æ–¹ãŒç”Ÿå­˜äºˆæ¸¬ã«ã‚ˆã‚Šãƒã‚¸ãƒ†ã‚£ãƒ–ã«å¯„ä¸ã—ã‚„ã™ã„ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQ3dmINwwDN"
      },
      "source": [
        "## 4.25 Combined Feature: Sex_Pclass / æ€§åˆ¥ã¨Pclassã®çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡\n",
        "\n",
        "We created a new categorical feature by combining `Sex` and `Pclass`, named `Sex_Pclass` (e.g., `female_P1`, `male_P3`).  \n",
        "This feature captures the interaction between gender and passenger class, which is known to be strongly related to survival probability on the Titanic.  \n",
        "To evaluate its effectiveness, we examined the prediction accuracy for each group and found noticeable differences.\n",
        "\n",
        "æ€§åˆ¥ (`Sex`) ã¨ãƒã‚±ãƒƒãƒˆã‚¯ãƒ©ã‚¹ (`Pclass`) ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ–°ãŸãªã‚«ãƒ†ã‚´ãƒªå¤‰æ•° `Sex_Pclass`ï¼ˆä¾‹: `female_P1`, `male_P3`ï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚  \n",
        "ã“ã®ç‰¹å¾´é‡ã¯ã€æ€§åˆ¥ã¨ç¤¾ä¼šéšç´šã®ç›¸äº’ä½œç”¨ã‚’è¡¨ç¾ã—ã¦ãŠã‚Šã€ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯å·ã«ãŠã‘ã‚‹ç”Ÿå­˜ç‡ã¨å¼·ãé–¢é€£ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚  \n",
        "æœ‰åŠ¹æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã®äºˆæ¸¬æ­£è§£ç‡ã‚’èª¿ã¹ãŸã¨ã“ã‚ã€é¡•è‘—ãªé•ã„ãŒç¢ºèªã§ãã¾ã—ãŸã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxwII-5n74tX"
      },
      "outputs": [],
      "source": [
        "# Create a combined categorical feature 'Sex_Pclass' by combining Sex and Pclass\n",
        "# æ€§åˆ¥ã¨ãƒã‚±ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªå¤‰æ•° 'Sex_Pclass' ã‚’ä½œæˆ\n",
        "df_valid['Sex_Pclass'] = df_valid['Sex_str'] + '_P' + df_valid['Pclass'].astype(str)\n",
        "\n",
        "# Calculate the accuracy (percentage of correct predictions) for each Sex_Pclass group\n",
        "# å„ 'Sex_Pclass' ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«äºˆæ¸¬ã®æ­£è§£ç‡ï¼ˆCorrectåˆ—ã®å¹³å‡ï¼‰ã‚’ç®—å‡º\n",
        "group_accuracy = df_valid.groupby('Sex_Pclass')['Correct'].mean().sort_values()\n",
        "\n",
        "# Display the sorted accuracy by group\n",
        "# ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®æ­£è§£ç‡ã‚’æ˜‡é †ã§è¡¨ç¤º\n",
        "print(group_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_dPnJfydkH"
      },
      "source": [
        "##4.26 Heatmap: Accuracy by Sex and Pclass / æ€§åˆ¥ã¨Pclassã”ã¨ã®æ­£è§£ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—  \n",
        "To visualize the overall pattern, we created a heatmap showing accuracy across Sex and Pclass.\n",
        "\n",
        "æ¬¡ã«ã€å…¨ä½“ã®å‚¾å‘ã‚’è¦–è¦šçš„ã«ç¢ºèªã™ã‚‹ãŸã‚ã€æ€§åˆ¥ã¨Pclassã”ã¨ã®æ­£è§£ç‡ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–ã—ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viwz9h9-uoUa"
      },
      "outputs": [],
      "source": [
        "heatmap_df = df_valid.pivot_table(index='Sex_str', columns='Pclass', values='Correct', aggfunc='mean')\n",
        "\n",
        "sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.title(\"Accuracy by Sex and Pclass\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqlWF1q8I-YE"
      },
      "source": [
        "### Observations on Prediction Accuracy by Sex_Pclass / Sex_Pclassã”ã¨ã®æ­£è§£ç‡ã«é–¢ã™ã‚‹è€ƒå¯Ÿ\n",
        "\n",
        "The prediction accuracy varies significantly depending on the combination of Sex and Pclass:\n",
        "\n",
        "- `female_P1`: **100% accuracy**, suggesting the model perfectly predicts survival for first-class female passengers.\n",
        "- `male_P2`: **96% accuracy**, which is surprisingly high for male passengers, indicating that second-class males may have clearer survival patterns in the data.\n",
        "- `male_P3`: **88.8% accuracy**, indicating good performance even in third class.\n",
        "- `female_P2`: **81.5% accuracy**, still quite strong, possibly due to both gender and class advantages.\n",
        "- `female_P3`: **65.9% accuracy**, showing that third-class females are harder to predict correctly.\n",
        "- `male_P1`: **61.4% accuracy**, the lowest among all groups, suggesting that first-class males have less consistent survival patterns from the model's perspective.\n",
        "\n",
        "These results show that the combination of gender and class has a strong influence on the model's ability to make accurate predictions. Particularly, female passengers in higher classes are the most predictable, while male passengers in first class are the least.\n",
        "\n",
        "---\n",
        "\n",
        "æ€§åˆ¥ã¨Pclassã‚’çµ„ã¿åˆã‚ã›ãŸ `Sex_Pclass` ã«ã‚ˆã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬æ­£è§£ç‡ã«ã¯å¤§ããªå·®ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- `female_P1` ã¯ **100%ã®æ­£è§£ç‡** ã§ã€1ç­‰èˆ¹å®¤ã®å¥³æ€§ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ãŒéå¸¸ã«å¼·ãå­¦ç¿’ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "- `male_P2` ã¯ **96%ã®æ­£è§£ç‡** ã§ã€ç”·æ€§ã¨ã—ã¦ã¯éå¸¸ã«é«˜ã„ç²¾åº¦ã§äºˆæ¸¬ã•ã‚Œã¦ã„ã¾ã™ã€‚2ç­‰èˆ¹å®¤ã®ç”·æ€§ã¯æ¯”è¼ƒçš„äºˆæ¸¬ã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "- `male_P3` ã‚‚ **88.8%** ã¨é«˜ç²¾åº¦ã§ã€äºˆæ¸¬ãŒå®‰å®šã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "- `female_P2` ã¯ **81.5%** ã§ã€æ€§åˆ¥ãƒ»ã‚¯ãƒ©ã‚¹ä¸¡æ–¹ã®å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "- `female_P3` ã¯ **65.9%** ã¨ã‚„ã‚„ä½ã‚ã§ã€3ç­‰èˆ¹å®¤ã®å¥³æ€§ã®äºˆæ¸¬ã¯ã‚„ã‚„é›£ã—ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- `male_P1` ã¯ **61.4%** ã¨æœ€ã‚‚ä½ãã€1ç­‰èˆ¹å®¤ã®ç”·æ€§ã¯ãƒ‡ãƒ¼ã‚¿ä¸Šã§ç”Ÿå­˜ãƒ»éç”Ÿå­˜ã®å‚¾å‘ãŒã¯ã£ãã‚Šã—ãªã„ã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã®çµæœã‹ã‚‰ã€**æ€§åˆ¥ã¨ã‚¯ãƒ©ã‚¹ã®çµ„ã¿åˆã‚ã›ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹**ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ç‰¹ã«é«˜ç­‰ã‚¯ãƒ©ã‚¹ã®å¥³æ€§ã¯äºˆæ¸¬ã—ã‚„ã™ãã€1ç­‰èˆ¹å®¤ã®ç”·æ€§ã¯æœ€ã‚‚é›£ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã§ã‚ã‚‹ã¨ã„ãˆã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXKYRrPeAD2O"
      },
      "source": [
        "## 4.27 Prediction Accuracy by Title_Mr Ã— Family Size / Mræ•¬ç§°ã¨å®¶æ—äººæ•°ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹äºˆæ¸¬æ­£è§£ç‡\n",
        "\n",
        "To explore whether family size affects prediction performance differently for male passengers (Title = Mr),  \n",
        "we created a new combined categorical feature: `Title_Family`, which concatenates the `Title_Mr` flag (1 if Mr, 0 otherwise) and the number of family members.  \n",
        "We then calculated the prediction accuracy for each group defined by this combined feature.\n",
        "\n",
        "ç”·æ€§ä¹—å®¢ï¼ˆæ•¬ç§°ãŒMrï¼‰ã®äºˆæ¸¬ç²¾åº¦ãŒå®¶æ—äººæ•°ã«ã‚ˆã£ã¦ç•°ãªã‚‹ã‹ã‚’èª¿ã¹ã‚‹ãŸã‚ã€  \n",
        "`Title_Mr`ï¼ˆMrã§ã‚ã‚Œã°1ã€ãã‚Œä»¥å¤–ã¯0ï¼‰ã¨`Family`ï¼ˆå®¶æ—äººæ•°ï¼‰ã‚’çµåˆã—ãŸäº¤å·®ç‰¹å¾´é‡ `Title_Family` ã‚’ä½œæˆã—ã¾ã—ãŸã€‚\n",
        "\n",
        "æ¬¡ã«ã€ã“ã®æ–°ã—ã„çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡ã”ã¨ã«äºˆæ¸¬ã®æ­£è§£ç‡ã‚’ç®—å‡ºã—ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxDMQL2p8IIb"
      },
      "outputs": [],
      "source": [
        "# Create a new categorical feature 'Title_Family' by combining Title_Mr (1 or 0) and Family size\n",
        "# Title_Mrï¼ˆæ•¬ç§°ãŒMrã‹ã©ã†ã‹ï¼‰ã¨Familyï¼ˆå®¶æ—äººæ•°ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ã€æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªå¤‰æ•° 'Title_Family' ã‚’ä½œæˆ\n",
        "df_valid['Title_Family'] = df_valid['Title_Mr'].astype(str) + '_F' + df_valid['Family'].astype(str)\n",
        "\n",
        "# Calculate prediction accuracy (mean of 'Correct') for each Title_Family group\n",
        "# å„ 'Title_Family' ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«äºˆæ¸¬ã®æ­£è§£ç‡ï¼ˆCorrectåˆ—ã®å¹³å‡ï¼‰ã‚’ç®—å‡º\n",
        "group_accuracy = df_valid.groupby('Title_Family')['Correct'].mean().sort_values()\n",
        "\n",
        "# Display the accuracy by group in ascending order\n",
        "# ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®æ­£è§£ç‡ã‚’æ˜‡é †ã§è¡¨ç¤º\n",
        "print(group_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIDHR_YiIm-Z"
      },
      "source": [
        "## 4.28 Misclassification Rate (Bar Plot) / èª¤åˆ†é¡ç‡ã®æ£’ã‚°ãƒ©ãƒ•\n",
        "\n",
        "To better understand which Title-Family groups are most challenging for the model,  \n",
        "we calculated and visualized the **misclassification rate** (1 - accuracy) for each `Title_Family` category.  \n",
        "This visualization clearly highlights where the model is struggling the most.  \n",
        "\n",
        "ã©ã® `Title_Family`ï¼ˆæ•¬ç§°Mrã¨å®¶æ—äººæ•°ã®çµ„ã¿åˆã‚ã›ï¼‰ã‚°ãƒ«ãƒ¼ãƒ—ãŒãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦é›£ã—ã„ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã«ã€  \n",
        "å„ã‚°ãƒ«ãƒ¼ãƒ—ã®**èª¤åˆ†é¡ç‡ï¼ˆ1 - æ­£è§£ç‡ï¼‰**ã‚’ç®—å‡ºã—ã€æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã—ã¾ã—ãŸã€‚  \n",
        "ã“ã®å¯è¦–åŒ–ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®äºˆæ¸¬ã«æœ€ã‚‚è‹¦æˆ¦ã—ã¦ã„ã‚‹ã‹ãŒä¸€ç›®ã§ã‚ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNlpP94HAcdT"
      },
      "outputs": [],
      "source": [
        "# Calculate misclassification rate for each group\n",
        "# ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®èª¤åˆ†é¡ç‡ã‚’è¨ˆç®—\n",
        "title_family_error = 1 - df_valid.groupby('Title_Family')['Correct'].mean()\n",
        "\n",
        "# Sort by misclassification rate in descending order\n",
        "# èª¤åˆ†é¡ç‡ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "title_family_error = title_family_error.sort_values(ascending=False)\n",
        "\n",
        "# Visualize the misclassification rates with a bar plot\n",
        "# èª¤åˆ†é¡ç‡ã‚’æ£’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 6))\n",
        "title_family_error.plot(kind='bar', color='tomato')\n",
        "plt.title('Misclassification Rate by Title_Mr Ã— Family')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.xlabel('Title_Mr_Family')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikIZiKkKIrGc"
      },
      "source": [
        "### Observations / è€ƒå¯Ÿ\n",
        "\n",
        "- The group **`1_F4`** (Mr with 4 family members) had the **highest misclassification rate (50%)**,  \n",
        "  suggesting that this group may have more ambiguous or mixed survival signals.\n",
        "- Other Mr groups like `1_F2` and `1_F1` also showed moderate misclassification rates,  \n",
        "  indicating that male passengers with small families are more prone to incorrect predictions.\n",
        "- In contrast, non-Mr groups (`0_F4`, `0_F6`, etc.) generally had **very low error rates**,  \n",
        "  further supporting the idea that male title groups are harder for the model to classify.  \n",
        "\n",
        "This kind of group-wise error analysis helps identify areas where the model might need additional features, adjustments, or focused attention.\n",
        "\n",
        "---\n",
        "\n",
        "- **`1_F4`**ï¼ˆå®¶æ—4äººã®Mrï¼‰ãŒ**èª¤åˆ†é¡ç‡50%**ã¨æœ€ã‚‚é«˜ãã€  \n",
        "  ã“ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®ç”Ÿå­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ›–æ˜§ã§ã€ãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦é›£ã—ã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "- ä»–ã«ã‚‚ã€`1_F2`, `1_F1` ãªã©ã®å°‘äººæ•°ã®Mrã‚°ãƒ«ãƒ¼ãƒ—ã§ã‚‚èª¤åˆ†é¡ç‡ãŒã‚„ã‚„é«˜ãã€  \n",
        "  å°è¦æ¨¡ãªå®¶æ—ã‚’æŒã¤ç”·æ€§ä¹—å®¢ã®äºˆæ¸¬ãŒé›£ã—ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- ä¸€æ–¹ã§ã€`0_F4`, `0_F6` ãªã©ã®**éMrã‚°ãƒ«ãƒ¼ãƒ—ã¯èª¤åˆ†é¡ç‡ãŒéå¸¸ã«ä½ã**ã€  \n",
        "  ã‚„ã¯ã‚Š**ç”·æ€§ä¹—å®¢ã®åˆ†é¡ãŒç›¸å¯¾çš„ã«é›£ã—ã„**ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚  \n",
        "  \n",
        "ã“ã®ã‚ˆã†ãªã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®èª¤åˆ†é¡åˆ†æã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„ä½™åœ°ã‚„è¿½åŠ ã®ç‰¹å¾´é‡è¨­è¨ˆã®ãƒ’ãƒ³ãƒˆã‚’ä¸ãˆã¦ãã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx75NAerO_Vl"
      },
      "source": [
        "## ğŸ”· 4.29 Feature Selection for Final Model / æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã«å‘ã‘ãŸç‰¹å¾´é‡é¸å®š\n",
        "\n",
        "To build a more interpretable and efficient model, we reduce the feature set to only the most impactful ones.\n",
        "\n",
        "ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ã¨åŠ¹ç‡æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚’**å½±éŸ¿ã®å¤§ãã„ã‚‚ã®ã®ã¿ã«çµã‚Šè¾¼ã¿**ã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Top Features Based on SHAP / SHAPã«ã‚ˆã‚‹ä¸»è¦ç‰¹å¾´é‡ã®é¸å®š\n",
        "\n",
        "We start by selecting the top **~80% of cumulative importance** from SHAP values and/or feature importances.  \n",
        "This forms the core of our model, containing features with the strongest direct impact on prediction.\n",
        "\n",
        "ã¾ãšã€SHAPå€¤ã‚„ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ã„ã¦ã€ç´¯ç©ã§ç´„**ä¸Šä½80%**ã‚’å ã‚ã‚‹ä¸»è¦ãªç‰¹å¾´é‡ã‚’é¸å®šã—ã¾ã™ã€‚  \n",
        "ã“ã‚ŒãŒãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã¨ãªã‚Šã€**äºˆæ¸¬ã«ç›´æ¥çš„ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ç‰¹å¾´**ã‚’ä¸­å¿ƒã«æ§‹æˆã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Add Selected High-Impact Interaction Features / åŠ¹æœçš„ãªçµ„åˆã›ç‰¹å¾´ã®è¿½åŠ \n",
        "\n",
        "In addition to the top features, we add a **small number of carefully chosen interaction features**,  \n",
        "based on their effectiveness observed in misclassification analysis and SHAP dependence plots.\n",
        "Examples of these include:\n",
        "\n",
        "- `Sex_Pclass_male_1`: 1st class males with distinct survival patterns  \n",
        "- `Sex_Pclass_female_3`: 3rd class females with lower survival accuracy  \n",
        "- `Title_Mr_Family_4`: Mr passengers with 4 family members â€“ a high-error group\n",
        "\n",
        "By doing this, we preserve the model's simplicity while still incorporating insights from deeper error analysis.  \n",
        "\n",
        "ã¾ãŸã€SHAPä¾å­˜ãƒ—ãƒ­ãƒƒãƒˆã‚„èª¤åˆ†é¡åˆ†æã§**åŠ¹æœçš„ã ã£ãŸäº¤å·®ç‰¹å¾´é‡**ã®ä¸­ã‹ã‚‰ã€  \n",
        "**ç‰¹ã«å¼·ã„å‚¾å‘ãŒè¦‹ã‚‰ã‚ŒãŸå°‘æ•°ã®çµ„ã¿åˆã‚ã›**ã ã‘ã‚’é¸æŠœã—ã¦è¿½åŠ ã—ã¾ã™ã€‚\n",
        "\n",
        "è¿½åŠ ã•ã‚ŒãŸå…·ä½“ä¾‹ï¼š\n",
        "\n",
        "- `Sex_Pclass_male_1`ï¼šæ˜ç¢ºãªç”Ÿå­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤1ç­‰ç”·æ€§  \n",
        "- `Sex_Pclass_female_3`ï¼šç”Ÿå­˜äºˆæ¸¬ãŒé›£ã—ã‹ã£ãŸ3ç­‰å¥³æ€§  \n",
        "- `Title_Mr_Family_4`ï¼šèª¤åˆ†é¡ç‡ãŒç‰¹ã«é«˜ã‹ã£ãŸã€å®¶æ—4äººã®Mr  \n",
        "\n",
        "ã“ã®ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã€**ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ç¶­æŒã—ãªãŒã‚‰ã€èª¤åˆ†é¡ã‚„ç›¸äº’ä½œç”¨ã®åˆ†æçµæœã‚’é©åˆ‡ã«åæ˜ **ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dIYKZGAqgTI"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1. ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦æ§‹ç¯‰é–‹å§‹\n",
        "# =========================\n",
        "df_fe5 = df_fe4.copy()\n",
        "df_fe5_test = df_fe4_test.copy()\n",
        "\n",
        "# å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆçµåˆï¼ˆç‰¹å¾´é‡æ•´åˆã®ãŸã‚ï¼‰\n",
        "df_fe5_all = pd.concat([df_fe5, df_fe5_test], axis=0).reset_index(drop=True)\n",
        "\n",
        "# =========================\n",
        "# 2. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã®è¿½åŠ \n",
        "# =========================\n",
        "df_fe5_all['male_1_flag'] = ((df_fe5_all['Sex'] == 0) & (df_fe5_all['Pclass_1'] == 1)).astype(int)\n",
        "df_fe5_all['female_3_flag'] = ((df_fe5_all['Sex'] == 1) & (df_fe5_all['Pclass_3'] == 1)).astype(int)\n",
        "df_fe5_all['mr_family_4_flag'] = ((df_fe5_all['Title_Mr'] == 1) & (df_fe5_all['Family'] == 4)).astype(int)\n",
        "\n",
        "# =========================\n",
        "# 3. ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®å®šç¾©\n",
        "# =========================\n",
        "selected_features = [\n",
        "    'Sex', 'Title_Mr', 'Fare', 'Fare_log', 'Age',\n",
        "    'Title_Miss', 'Pclass_3', 'TicketGroupSize', 'Title_Mrs',\n",
        "    'Family', 'Has_Cabin'\n",
        "]\n",
        "\n",
        "new_features = [\n",
        "    'S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup',\n",
        "    'male_1_flag', 'female_3_flag', 'mr_family_4_flag'\n",
        "]\n",
        "\n",
        "final_features = list(dict.fromkeys(selected_features + new_features))\n",
        "\n",
        "# =========================\n",
        "# 4. ãƒ‡ãƒ¼ã‚¿å†åˆ†å‰²\n",
        "# =========================\n",
        "df_fe5 = df_fe5_all.iloc[:len(df_fe4)].reset_index(drop=True)\n",
        "df_fe5_test = df_fe5_all.iloc[len(df_fe4):].reset_index(drop=True)\n",
        "\n",
        "X_selected_plus = df_fe5[final_features]\n",
        "y_selected_plus = df_fe5['Survived']\n",
        "\n",
        "# =========================\n",
        "# 5. StratifiedKFold ã®å®šç¾©\n",
        "# =========================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =========================\n",
        "# 6. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰å®šç¾©\n",
        "# =========================\n",
        "param_grid_rf_plus = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 7. GridSearchCV ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
        "# =========================\n",
        "grid_search_rf_plus = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid_rf_plus,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 8. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼‰\n",
        "# =========================\n",
        "grid_search_rf_plus.fit(X_selected_plus, y_selected_plus)\n",
        "\n",
        "# =========================\n",
        "# 9. æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—\n",
        "# =========================\n",
        "best_model_rf_plus = grid_search_rf_plus.best_estimator_\n",
        "best_params_rf_plus = grid_search_rf_plus.best_params_\n",
        "\n",
        "# =========================\n",
        "# 10. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆAccuracy / ROC-AUCï¼‰\n",
        "# =========================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "scores_rf_plus = evaluate_model_cv(best_model_rf_plus, X_selected_plus, y_selected_plus, cv)\n",
        "\n",
        "# =========================\n",
        "# 11. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ & ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›\n",
        "# =========================\n",
        "y_pred_cv_rf_plus = cross_val_predict(best_model_rf_plus, X_selected_plus, y_selected_plus, cv=cv)\n",
        "class_report_rf_plus = classification_report(y_selected_plus, y_pred_cv_rf_plus)\n",
        "\n",
        "# =========================\n",
        "# 12. çµæœã®å‡ºåŠ›\n",
        "# =========================\n",
        "print(\"ğŸ“Œ Best Parameters (with interaction features):\", best_params_rf_plus)\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", scores_rf_plus['accuracy'])\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", scores_rf_plus['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", class_report_rf_plus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0deivh02pTzn"
      },
      "source": [
        "## Insights / è€ƒå¯Ÿ\n",
        "\n",
        "### 1. Recall for Survived Class is Relatively Low  \n",
        "- The recall for survivors (class 1) is 0.74, meaning about 26% of survivors are misclassified as non-survivors.  \n",
        "- This may be caused by class imbalance or insufficient feature representation.\n",
        "\n",
        "### 1. ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®ãƒªã‚³ãƒ¼ãƒ«ãŒã‚„ã‚„ä½ã„  \n",
        "- ç”Ÿå­˜è€…ã®ãƒªã‚³ãƒ¼ãƒ«ãŒ0.74ã§ã€ç´„26%ã®ç”Ÿå­˜è€…ãŒéç”Ÿå­˜ã¨èª¤åˆ†é¡ã•ã‚Œã¦ã„ã¾ã™ã€‚  \n",
        "- ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã‚„ç‰¹å¾´é‡ã®ä¸è¶³ãŒåŸå› ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### 2. High Precision and Recall for Non-Survived Class  \n",
        "- The model detects non-survivors well, with precision 0.85 and recall 0.91.  \n",
        "- It is effective at minimizing false negatives for the non-survivor class.\n",
        "\n",
        "### 2. éç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®ç²¾åº¦ãƒ»å†ç¾ç‡ã¯é«˜ã„  \n",
        "- éç”Ÿå­˜è€…ã®ç²¾åº¦ãŒ0.85ã€å†ç¾ç‡ãŒ0.91ã¨é«˜ãã€èª¤ã£ã¦ç”Ÿå­˜è€…ã¨åˆ¤å®šã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒå°‘ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Excellent ROC AUC  \n",
        "- The ROC AUC of 0.8768 indicates strong discrimination between survivors and non-survivors.  \n",
        "- Adding interaction features improved the modelâ€™s ability to distinguish classes.\n",
        "\n",
        "### 3. ROC AUCãŒéå¸¸ã«å„ªç§€  \n",
        "- ROC AUC 0.8768 ã¯ã€ç”Ÿå­˜è€…ã¨éç”Ÿå­˜è€…ã‚’ã‚ˆãè­˜åˆ¥ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚  \n",
        "- äº¤å·®ç‰¹å¾´é‡ã®è¿½åŠ ãŒåˆ¤åˆ¥èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã¾ã—ãŸã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Strengths and Areas for Improvement / å¼·ã¿ã¨æ”¹å–„ç‚¹\n",
        "\n",
        "| Strengths (å¼·ã¿)                          | Areas for Improvement (æ”¹å–„ç‚¹)            |\n",
        "|-----------------------------------------|-----------------------------------------|\n",
        "| âœ… High discrimination power (é«˜ã„è­˜åˆ¥èƒ½åŠ›ãƒ»AUCãŒé«˜ã„) | ğŸ”º Recall for survivors can be improved (ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã®ãƒªã‚³ãƒ¼ãƒ«æ”¹å–„ãŒèª²é¡Œ) |\n",
        "| âœ… Effective use of interaction features (äº¤å·®ç‰¹å¾´é‡ã®åŠ¹æœçš„ãªæ´»ç”¨) | ğŸ”º Consider exploring more auxiliary features (ã•ã‚‰ãªã‚‹è£œåŠ©ç‰¹å¾´é‡ã®æ¢ç´¢ãŒå¿…è¦) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jrik6eTImdbs"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 1. Create dictionary of model results\n",
        "#    ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’è¾æ›¸å½¢å¼ã§å®šç¾©\n",
        "# =============================================\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\n",
        "        \"Top 80% Features Only\",              # ä¸Šä½80%ã®ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
        "        \"Top 80% + Interaction Features\"       # ä¸Šä½80% + äº¤å·®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
        "    ],\n",
        "    \"Best Parameters\": [\n",
        "        best_params_rf_sel,                    # ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "        best_params_rf_plus                    # Interactionç‰¹å¾´è¿½åŠ ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "    ],\n",
        "    \"Mean CV Accuracy\": [\n",
        "        scores_rf_sel['accuracy'],             # ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡Accuracy\n",
        "        scores_rf_plus['accuracy']             # Interactionè¿½åŠ ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡Accuracy\n",
        "    ],\n",
        "    \"Mean CV ROC AUC\": [\n",
        "        scores_rf_sel['roc_auc'],              # ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡ROC AUC\n",
        "        scores_rf_plus['roc_auc']              # Interactionè¿½åŠ ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡ROC AUC\n",
        "    ]\n",
        "}\n",
        "\n",
        "# =============================================\n",
        "# 2. Convert dictionary to DataFrame\n",
        "#    çµæœã‚’DataFrameå½¢å¼ã«å¤‰æ›\n",
        "# =============================================\n",
        "\n",
        "comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# =============================================\n",
        "# 3. Display comparison table\n",
        "#    æ¯”è¼ƒè¡¨ã‚’è¡¨ç¤º\n",
        "# =============================================\n",
        "\n",
        "print(\"ğŸ” Model Comparison Table / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨ï¼š\\n\")\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0PAvcoYqjwe"
      },
      "source": [
        "### Analysis / è€ƒå¯Ÿ\n",
        "\n",
        "- Both models achieved similar hyperparameters, indicating that model complexity optimality did not change with the addition of interaction features.\n",
        "- The \"Top 80% Features Only\" model showed a slightly higher mean CV accuracy (84.7%) compared to the model with interaction features (84.2%), suggesting that interaction features did not improve accuracy in this case.\n",
        "- However, the model with interaction features achieved a marginally better mean CV ROC AUC (0.8768 vs 0.8760), which indicates a slight improvement in the modelâ€™s ability to discriminate between classes.\n",
        "- This implies that while interaction features might not boost overall accuracy, they can help the model better rank predictions by confidence, potentially improving decision thresholds.\n",
        "- Further feature engineering or tuning might be needed to better exploit the potential of interaction terms for accuracy gains.\n",
        "\n",
        "---\n",
        "\n",
        "- ä¸¡ãƒ¢ãƒ‡ãƒ«ã¯åŒã˜æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¤ºã—ã¦ãŠã‚Šã€äº¤å·®ç‰¹å¾´é‡ã®è¿½åŠ ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã®æœ€é©è§£ãŒå¤‰ã‚ã‚‰ãªã‹ã£ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "- ã€Œä¸Šä½80%ã®ç‰¹å¾´é‡ã®ã¿ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ãšã‹ã«é«˜ã„å¹³å‡CVç²¾åº¦ï¼ˆç´„84.7%ï¼‰ã‚’ç¤ºã—ã€äº¤å·®ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆç´„84.2%ï¼‰ã‚ˆã‚Šç²¾åº¦ãŒè‹¥å¹²è‰¯å¥½ã§ã—ãŸã€‚ã¤ã¾ã‚Šã€ä»Šå›ã®æ¡ä»¶ä¸‹ã§ã¯äº¤å·®ç‰¹å¾´é‡ãŒç²¾åº¦å‘ä¸Šã«ã¯ã¤ãªãŒã‚‰ãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "- ä¸€æ–¹ã§ã€äº¤å·®ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯å¹³å‡CV ROC AUCãŒã‚ãšã‹ã«é«˜ãï¼ˆ0.8768 vs 0.8760ï¼‰ã€ã‚¯ãƒ©ã‚¹é–“ã®åˆ¤åˆ¥åŠ›ã¯å°‘ã—å‘ä¸Šã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "- ã“ã‚Œã¯ã€äº¤å·®ç‰¹å¾´é‡ãŒå…¨ä½“ã®ç²¾åº¦å‘ä¸Šã«ã¯å¯„ä¸ã—ãªãã¨ã‚‚ã€äºˆæ¸¬ã®ä¿¡é ¼åº¦ã®ãƒ©ãƒ³ã‚¯ä»˜ã‘ã«ã¯å½¹ç«‹ã¡ã€é–¾å€¤è¨­å®šãªã©ã®èª¿æ•´ã§æ€§èƒ½æ”¹å–„ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n",
        "- ä»Šå¾Œã¯ã€ã•ã‚‰ã«ç‰¹å¾´é‡ã®å·¥å¤«ã‚„ãƒ¢ãƒ‡ãƒ«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€äº¤å·®ç‰¹å¾´é‡ã®åŠ¹æœã‚’ã‚ˆã‚Šå¼•ãå‡ºã™ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæœ›ã¾ã‚Œã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTswIdPCR2WV"
      },
      "source": [
        "## 4.30 Evaluating Contribution of Additional Features / è¿½åŠ ç‰¹å¾´é‡ã®è²¢çŒ®åº¦è©•ä¾¡\n",
        "\n",
        "We evaluated the impact of each newly added feature by removing them one at a time  \n",
        "and measuring the change in validation accuracy compared to the baseline model.\n",
        "\n",
        "æ–°ãŸã«è¿½åŠ ã—ãŸå„ç‰¹å¾´é‡ã«ã¤ã„ã¦ã€1ã¤ãšã¤é™¤å¤–ã—ã¦ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦ã®å¤‰åŒ–ã‚’ç¢ºèªã—ã€  \n",
        "ãƒ¢ãƒ‡ãƒ«ã¸ã®è²¢çŒ®åº¦ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2TLouyvGB-v"
      },
      "outputs": [],
      "source": [
        "y_fe5 = df_fe5['Survived'] # Target variableã€€/ ç›®çš„å¤‰æ•°\n",
        "\n",
        "# Split the data into training and validation sets / ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ç”¨ã¨æ¤œè¨¼ç”¨ã«åˆ†å‰²\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(df_fe5[final_features], y_fe5, test_size=0.3, random_state=42)\n",
        "\n",
        "# Baseline model using all features / ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’ä½¿ã£ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
        "base_model = RandomForestClassifier(\n",
        "    n_estimators=100,          # Number of trees in the forest / æ£®ã®æœ¨ã®æ•°\n",
        "    max_depth=None,            # No limit on the depth of the trees / æœ¨ã®æœ€å¤§æ·±ã•ã¯åˆ¶é™ãªã—\n",
        "    max_features='sqrt',       # Number of features to consider at each split / åˆ†å‰²æ™‚ã«è€ƒæ…®ã™ã‚‹ç‰¹å¾´é‡ã®æ•°\n",
        "    min_samples_leaf=3,        # Minimum samples required at leaf nodes / è‘‰ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "    min_samples_split=2,       # Minimum samples required to split a node / åˆ†å‰²ã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "    random_state=42            # Random seed for reproducibility / å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰\n",
        ")\n",
        "base_model.fit(X_train, y_train)  # Train the model / ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
        "base_score = accuracy_score(y_valid, base_model.predict(X_valid))  # Calculate accuracy on validation set / æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ç²¾åº¦ã‚’è¨ˆç®—\n",
        "\n",
        "# List of newly added handcrafted features / æ–°ã—ãè¿½åŠ ã—ãŸæ‰‹ä½œã‚Šç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ\n",
        "new_features = [\n",
        "    'S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup',\n",
        "    'male_1_flag','female_3_flag', 'mr_family_4_flag'\n",
        "]\n",
        "\n",
        "# Compare accuracy by removing one new feature at a time\n",
        "# 1ã¤ãšã¤æ–°ã—ã„ç‰¹å¾´é‡ã‚’é™¤å¤–ã—ã¦ç²¾åº¦ã‚’æ¯”è¼ƒ\n",
        "results = []\n",
        "\n",
        "for feature in new_features:\n",
        "    # Features excluding the current one / ç¾åœ¨é™¤å¤–ä¸­ã®ç‰¹å¾´é‡ä»¥å¤–ã‚’é¸æŠ\n",
        "    reduced_features = [f for f in final_features if f != feature]\n",
        "    X_train_reduced = X_train[reduced_features]\n",
        "    X_valid_reduced = X_valid[reduced_features]\n",
        "\n",
        "    # Train model on reduced feature set / é™¤å¤–ã—ãŸç‰¹å¾´é‡ãªã—ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        max_features='sqrt',\n",
        "        min_samples_leaf=3,\n",
        "        min_samples_split=2,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_reduced, y_train)\n",
        "    score = accuracy_score(y_valid, model.predict(X_valid_reduced))  # Evaluate accuracy / ç²¾åº¦è©•ä¾¡\n",
        "    results.append((feature, score, score - base_score))  # Save feature name, accuracy, and difference from baseline / çµæœã‚’ä¿å­˜\n",
        "\n",
        "# Display the results sorted by difference from baseline (descending)\n",
        "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®å·®åˆ†ã§ã‚½ãƒ¼ãƒˆã—ã¦çµæœã‚’è¡¨ç¤º\n",
        "results_df = pd.DataFrame(results, columns=[\"Removed Feature\", \"Accuracy\", \"Difference\"])\n",
        "print(results_df.sort_values(by=\"Difference\", ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrT-uDtPCjVN"
      },
      "source": [
        "### Feature Selection and Model Performance Analysis / ç‰¹å¾´é‡é¸æŠã¨ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®åˆ†æ\n",
        "\n",
        "We tested adding several handcrafted features suggested by SHAP and misclassification analysis,  \n",
        "but the validation accuracy decreased compared to using only the top 80% of features selected by feature importance.\n",
        "\n",
        "Removing features one by one showed that some new features, including `AgeGroup_Adult` and `mr_family_4_flag`,  \n",
        "actually decreased model accuracy, suggesting that not all handcrafted features contribute positively.\n",
        "\n",
        "These results indicate that adding too many candidate features may introduce noise or redundancy,  \n",
        "and careful feature selection focusing on the most important features leads to better model performance.\n",
        "\n",
        "Therefore, we decided to use only the top 80% features selected by importance for the final model.\n",
        "\n",
        "---\n",
        "\n",
        "SHAPã‚„èª¤åˆ†é¡åˆ†æã§ç¤ºå”†ã•ã‚ŒãŸè¤‡æ•°ã®æ‰‹ä½œã‚Šç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¦æ¤œè¨¼ã—ã¾ã—ãŸãŒã€  \n",
        "ç‰¹å¾´é‡é‡è¦åº¦ã§é¸ã‚“ã ä¸Šä½80%ã®ç‰¹å¾´é‡ã ã‘ã‚’ä½¿ç”¨ã—ãŸå ´åˆã¨æ¯”ã¹ã¦ã€æ¤œè¨¼ç²¾åº¦ã¯ä½ä¸‹ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "ç‰¹å¾´é‡ã‚’ä¸€ã¤ãšã¤é™¤å¤–ã—ãªãŒã‚‰æ¤œè¨¼ã—ãŸçµæœã€`AgeGroup_Adult`ã‚„`mr_family_4_flag`ãªã©ã®æ–°ã—ã„ç‰¹å¾´é‡ã¯  \n",
        "ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ä¸‹ã’ã¦ãŠã‚Šã€ã™ã¹ã¦ã®æ‰‹ä½œã‚Šç‰¹å¾´é‡ãŒåŠ¹æœçš„ã§ã‚ã‚‹ã¨ã¯é™ã‚‰ãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚\n",
        "\n",
        "ã“ã®ã“ã¨ã‹ã‚‰ã€å¤šãã®å€™è£œç‰¹å¾´é‡ã‚’è¿½åŠ ã™ã‚‹ã¨ãƒã‚¤ã‚ºã‚„å†—é•·æ€§ãŒå¢—ãˆã€  \n",
        "é‡è¦ãªç‰¹å¾´é‡ã«çµã£ã¦é¸æŠã™ã‚‹ã“ã¨ãŒã‚ˆã‚Šè‰¯ã„ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã«ã¤ãªãŒã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "ã‚ˆã£ã¦ã€æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«ã«ã¯ç‰¹å¾´é‡é‡è¦åº¦ã§é¸ã‚“ã ä¸Šä½80%ã®ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ã†ã“ã¨ã«æ±ºã‚ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqzI6lbYaVrA"
      },
      "source": [
        "## ğŸ“Œ 4.31 Feature Selection by Feature Importance / ç‰¹å¾´é‡é‡è¦åº¦ã«ã‚ˆã‚‹ç‰¹å¾´é¸æŠã®æ¤œè¨¼\n",
        "We tested a simple incremental feature selection strategy: starting from the most important feature and adding one at a time based on the feature importances from a Random Forest model.\n",
        "The goal was to observe how validation accuracy changes as more features are introduced.\n",
        "\n",
        "We trained a model with 1 to all features, recorded the validation accuracy at each step, and plotted the results.\n",
        "This approach can help determine an optimal number of features based solely on their importance scores.\n",
        "\n",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ã„ã¦ã€ã‚‚ã£ã¨ã‚‚é‡è¦ãªç‰¹å¾´é‡ã‹ã‚‰1ã¤ãšã¤è¿½åŠ ã—ã€æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã§ã®ç²¾åº¦ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n",
        "1å€‹ã‹ã‚‰å…¨ç‰¹å¾´é‡ã¾ã§æ®µéšçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ãã‚Œãã‚Œã®æ­£è§£ç‡ã‚’è¨˜éŒ²ã—ã¦å¯è¦–åŒ–ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ã§é¸ã‚“ã å ´åˆã«æœ€ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è‰¯ã„ç‰¹å¾´é‡æ•°ãŒã©ã®ã‚ãŸã‚Šã‹ã‚’åˆ¤æ–­ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XhfwvUADM8"
      },
      "outputs": [],
      "source": [
        "# Define features and target variable / ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’å®šç¾©\n",
        "X = df_fe5[final_features]\n",
        "y = y_fe5\n",
        "\n",
        "# Train a Random Forest to get initial feature importances\n",
        "# åˆæœŸã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¾—ã‚‹ãŸã‚ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’å­¦ç¿’\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Sort features by their importance (from highest to lowest) / ç‰¹å¾´é‡ã‚’é‡è¦åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "sorted_features = feat_importances.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "# Fix the train/validation split to ensure consistency / ä¸€è²«æ€§ã®ã‚ã‚‹æ¯”è¼ƒã®ãŸã‚ã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã‚’å›ºå®š\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Prepare to record accuracy as we increase the number of features / ç‰¹å¾´é‡æ•°ã‚’å¢—ã‚„ã—ãªãŒã‚‰ç²¾åº¦ã‚’è¨˜éŒ²ã™ã‚‹ãƒªã‚¹ãƒˆã‚’æº–å‚™\n",
        "num_features_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Loop through 1 to all features and evaluate model accuracy\n",
        "# 1å€‹ã‹ã‚‰å…¨ã¦ã®ç‰¹å¾´é‡ã¾ã§æ®µéšçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ç²¾åº¦ã‚’è©•ä¾¡\n",
        "for i in range(1, len(sorted_features) + 1):\n",
        "    selected_feats = sorted_features[:i]  # Use top i features / ä¸Šä½iå€‹ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train[selected_feats], y_train)  # Train model / ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’\n",
        "    y_pred = model.predict(X_valid[selected_feats])  # Predict on validation set / æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬\n",
        "    acc = accuracy_score(y_valid, y_pred)  # Calculate accuracy / ç²¾åº¦ã‚’ç®—å‡º\n",
        "    num_features_list.append(i)  # Number of features used / ä½¿ç”¨ã—ãŸç‰¹å¾´é‡ã®æ•°\n",
        "    accuracy_list.append(acc)    # Accuracy result / ç²¾åº¦ã®çµæœ\n",
        "\n",
        "# Plot the accuracy as a function of number of features used / ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°ã”ã¨ã®ç²¾åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(num_features_list, accuracy_list, marker='o')  # Line plot with markers / ãƒãƒ¼ã‚«ãƒ¼ä»˜ãæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•\n",
        "plt.xlabel(\"Number of Features Used\")  # Xè»¸ãƒ©ãƒ™ãƒ«ï¼šä½¿ç”¨ã—ãŸç‰¹å¾´é‡ã®æ•°\n",
        "plt.ylabel(\"Validation Accuracy\")     # Yè»¸ãƒ©ãƒ™ãƒ«ï¼šæ¤œè¨¼ç²¾åº¦\n",
        "plt.title(\"Accuracy vs Number of Features (Feature Importance Order)\")  # ã‚¿ã‚¤ãƒˆãƒ«ï¼šç‰¹å¾´é‡æ•°ã¨ç²¾åº¦ã®é–¢ä¿‚\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_JHFEif_fr"
      },
      "source": [
        "## 4.32 Best Accuracy and Number of Features Used / æœ€é«˜ç²¾åº¦ã¨ä½¿ç”¨ã—ãŸç‰¹å¾´é‡ã®æ•°  \n",
        "In this section, we identify the number of features and the corresponding accuracy that achieved the highest performance as we gradually increased the number of features. This is an important step in finding the optimal number of features, which helps in understanding which feature set maximizes the model's performance.  \n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç‰¹å¾´é‡æ•°ã‚’å¢—ã‚„ã—ã¦ã„ãéç¨‹ã§å¾—ã‚‰ã‚ŒãŸç²¾åº¦çµæœã‹ã‚‰ã€æœ€é«˜ç²¾åº¦ã‚’é”æˆã—ãŸç‰¹å¾´é‡ã®æ•°ã¨ãã®ç²¾åº¦ã‚’ç‰¹å®šã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚Šã€ã©ã®ç‰¹å¾´é‡æ•°ãŒãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-mRC0-OGRUZ"
      },
      "outputs": [],
      "source": [
        "# Find the highest accuracy from the recorded accuracies\n",
        "# è¨˜éŒ²ã—ãŸç²¾åº¦ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é«˜ã„ç²¾åº¦ã‚’è¦‹ã¤ã‘ã‚‹\n",
        "best_acc = max(accuracy_list)\n",
        "\n",
        "# Find the number of features corresponding to the highest accuracy\n",
        "# æœ€é«˜ç²¾åº¦ãŒå‡ºãŸã¨ãã®ç‰¹å¾´é‡ã®æ•°ã‚’å–å¾—ã™ã‚‹\n",
        "best_num_feats = num_features_list[accuracy_list.index(best_acc)]\n",
        "\n",
        "# Print the best accuracy and the number of features used\n",
        "# ãƒ™ã‚¹ãƒˆãªç²¾åº¦ã¨ãã®æ™‚ã®ç‰¹å¾´é‡ã®æ•°ã‚’è¡¨ç¤ºã™ã‚‹\n",
        "print(f\"âœ… Best Accuracy: {best_acc:.4f} at {best_num_feats} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l024owd7K1Dn"
      },
      "source": [
        "### Best Accuracy and Number of Features Used / æœ€é«˜ç²¾åº¦ã¨ä½¿ç”¨ã—ãŸç‰¹å¾´é‡ã®æ•°\n",
        "\n",
        "### Discussion / è€ƒå¯Ÿ\n",
        "\n",
        "In this section, we identified the optimal number of features for the model based on the highest validation accuracy achieved. The best accuracy of **0.8209** was obtained when **14 features** were used. This suggests that **14 features** strike the best balance between model complexity and predictive performance.\n",
        "\n",
        "- **Key Insights:**\n",
        "  - The model's accuracy increased as more features were added, but after a certain point, the addition of new features did not lead to a significant improvement.\n",
        "  - Using **14 features** appears to provide the best predictive performance, indicating that these features contain the most relevant information for predicting the target variable.\n",
        "  - This finding suggests that using more than **14 features** could lead to overfitting or unnecessarily increased complexity without adding meaningful improvements to model performance.  \n",
        "\n",
        "The optimal number of features, based on the highest achieved accuracy, is **14**. By selecting these top features, we can achieve a more efficient and effective model without unnecessarily increasing the computational cost or risk of overfitting.  \n",
        "\n",
        "---\n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç‰¹å¾´é‡æ•°ã‚’å¢—ã‚„ã—ã¦ã„ãéç¨‹ã§å¾—ã‚‰ã‚ŒãŸç²¾åº¦çµæœã‹ã‚‰ã€æœ€é«˜ç²¾åº¦ã‚’é”æˆã—ãŸç‰¹å¾´é‡ã®æ•°ã¨ãã®ç²¾åº¦ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚æœ€é©ãªç²¾åº¦ **0.8209** ã¯ **14å€‹ã®ç‰¹å¾´é‡** ã§å¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã¯ã€**14å€‹ã®ç‰¹å¾´é‡** ãŒãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã¨äºˆæ¸¬ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- **é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆ:**\n",
        "  - ç‰¹å¾´é‡ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯å‘ä¸Šã—ã¾ã—ãŸãŒã€ä¸€å®šã®ç‰¹å¾´é‡æ•°ã‚’è¶…ãˆã‚‹ã¨ã€ç²¾åº¦å‘ä¸Šã¯ã»ã¨ã‚“ã©è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n",
        "  - **14å€‹ã®ç‰¹å¾´é‡** ã¯äºˆæ¸¬ç²¾åº¦ã‚’æœ€ã‚‚é«˜ã‚ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«æœ€ã‚‚é–¢é€£æ€§ã®ã‚ã‚‹æƒ…å ±ã‚’å«ã‚“ã§ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "  - ã“ã®çµæœã¯ã€**14å€‹ä»¥ä¸Šã®ç‰¹å¾´é‡** ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€éå­¦ç¿’ã‚„ä¸å¿…è¦ã«è¤‡é›‘åŒ–ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ”¹å–„ã«ã¯ã¤ãªãŒã‚‰ãªã„ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "### çµè«–\n",
        "æœ€é©ãªç‰¹å¾´é‡æ•°ã¯ã€æœ€é«˜ç²¾åº¦ã‚’é”æˆã—ãŸ **14å€‹ã®ç‰¹å¾´é‡** ã§ã™ã€‚ã“ã®ç‰¹å¾´é‡ã‚’é¸å®šã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚„éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã‚’æŠ‘ãˆãªãŒã‚‰ã€åŠ¹ç‡çš„ã§åŠ¹æœçš„ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbh7UAJ98qNJ"
      },
      "source": [
        "### 4.33 Train and evaluate a Random Forest model using the top 14 features by importance / ç‰¹å¾´é‡é‡è¦åº¦ä¸Šä½14å€‹ã‚’ç”¨ã„ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡  \n",
        "We trained a model using the top 14 features based on feature importance, and compared it with a model using the top 80% of cumulative importance to examine how different selection strategies affect predictive performance.  \n",
        "\n",
        "ç‰¹å¾´é‡é‡è¦åº¦ã®ä¸Šä½14å€‹ã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€ç´¯ç©é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ã§é¸ã‚“ã ä¸Šä½80%ã®ç‰¹å¾´é‡ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€é¸æŠæ–¹æ³•ã®é•ã„ãŒäºˆæ¸¬æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W7wDrF9oKBp"
      },
      "outputs": [],
      "source": [
        "# Use the top 14 features selected based on feature importance for training the model.\n",
        "# ç‰¹å¾´é‡é‡è¦åº¦ã§é¸ã°ã‚ŒãŸä¸Šä½14å€‹ã®ç‰¹å¾´é‡ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã€‚\n",
        "top14_features = sorted_features[:14]\n",
        "print(top14_features)\n",
        "\n",
        "# =====================================================\n",
        "# 1. Define features and target variable / ä¸Šä½14å€‹ã®ç‰¹å¾´é‡ã ã‘ã‚’ç‰¹å¾´é‡ã«å«ã‚ã‚‹\n",
        "# =====================================================\n",
        "X_top14 = df_fe5[top14_features]\n",
        "y_top14 = df_fe5['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldã®å®šç¾©ï¼ˆshuffleã‚ã‚Šï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Hyperparameter grid / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œè¨­å®š\n",
        "# =====================================================\n",
        "param_grid_rf_top14 = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Grid Search with cross-validation / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ\n",
        "# =====================================================\n",
        "grid_search_rf_top14 = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid_rf_top14,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Fit on all data / å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ï¼‰\n",
        "# =====================================================\n",
        "grid_search_rf_top14.fit(X_top14, y_top14)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Get best estimator and parameters / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—\n",
        "# =====================================================\n",
        "best_model_rf_top14 = grid_search_rf_top14.best_estimator_\n",
        "best_params_rf_top14 = grid_search_rf_top14.best_params_\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluation function definition / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡é–¢æ•°å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluate model with cross-validation / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_rf_top14 = evaluate_model_cv(best_model_rf_top14, X_top14, y_top14, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Classification report with cross-validated predictions / CVäºˆæ¸¬ã‚’ä½¿ã£ãŸåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
        "# =====================================================\n",
        "y_pred_cv_rf_top14 = cross_val_predict(best_model_rf_top14, X_top14, y_top14, cv=cv)\n",
        "class_report_rf_top14 = classification_report(y_top14, y_pred_cv_rf_top14)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Best Parameters (RF selected features):\", best_params_rf_top14)\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", scores_rf_top14['accuracy'])\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", scores_rf_top14['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", class_report_rf_top14)\n",
        "\n",
        "# =============================================\n",
        "# 11. Create dictionary of model results\n",
        "#    ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’è¾æ›¸å½¢å¼ã§å®šç¾©\n",
        "# =============================================\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\n",
        "        \"Top 80% Features\",              # ä¸Šä½80%ã®ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
        "        \"Top14ã€€ Features\"                     # ä¸Šä½14ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
        "    ],\n",
        "    \"Best Parameters\": [\n",
        "        best_params_rf_sel,                    # ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "        best_params_rf_top14                    # Interactionç‰¹å¾´è¿½åŠ ãƒ¢ãƒ‡ãƒ«ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "    ],\n",
        "    \"Mean CV Accuracy\": [\n",
        "        scores_rf_sel['accuracy'],             # ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡Accuracy\n",
        "        scores_rf_top14['accuracy']             # Interactionè¿½åŠ ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡Accuracy\n",
        "    ],\n",
        "    \"Mean CV ROC AUC\": [\n",
        "        scores_rf_sel['roc_auc'],              # ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡ROC AUC\n",
        "        scores_rf_top14['roc_auc']              # Interactionè¿½åŠ ãƒ¢ãƒ‡ãƒ«ã®CVå¹³å‡ROC AUC\n",
        "    ]\n",
        "}\n",
        "\n",
        "# =============================================\n",
        "# 12. Convert dictionary to DataFrame\n",
        "#    çµæœã‚’DataFrameå½¢å¼ã«å¤‰æ›\n",
        "# =============================================\n",
        "comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# =============================================\n",
        "# 13. Display comparison table\n",
        "#    æ¯”è¼ƒè¡¨ã‚’è¡¨ç¤º\n",
        "# =============================================\n",
        "\n",
        "print(\"ğŸ” Model Comparison Table / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨ï¼š\\n\")\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrkPHxPmxeUS"
      },
      "source": [
        "###ğŸ” Model Comparison and Evaluation / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã¨è©•ä¾¡\n",
        "Feature Set: Top 80% Features vs Top 14 Features  \n",
        "ç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼šä¸Šä½80%ç‰¹å¾´é‡ vs ä¸Šä½14ç‰¹å¾´é‡  \n",
        "\n",
        "| Model / ãƒ¢ãƒ‡ãƒ«      | Mean CV Accuracy | Mean CV ROC AUC | Best Parameters / æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿         |\n",
        "| ---------------- | ---------------- | --------------- | --------------------------------- |\n",
        "| Top 80% Features | **0.8473**       | **0.8760**      | `max_depth=7, min_samples_leaf=4` |\n",
        "| Top 14 Features  | 0.8440           | 0.8758          | `max_depth=7, min_samples_leaf=2` |\n",
        "\n",
        "- The Top 80% Features model slightly outperformed the Top 14 model in both accuracy and ROC AUC.\n",
        "\n",
        "- The classification report also showed a marginally higher recall for the Top 80% model.\n",
        "\n",
        "- ä¸Šä½80%ç‰¹å¾´é‡ãƒ¢ãƒ‡ãƒ«ã¯ã€ç²¾åº¦ã¨ROC AUCã®ä¸¡æ–¹ã§ä¸Šä½14ç‰¹å¾´é‡ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ãšã‹ã«ä¸Šå›ã‚Šã¾ã—ãŸã€‚\n",
        "\n",
        "- åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã® recallï¼ˆå†ç¾ç‡ï¼‰ã‚‚ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒã‚„ã‚„é«˜ã„å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚  \n",
        "\n",
        "###ğŸ’¡ Considerations / è€ƒå¯Ÿ\n",
        "\n",
        "- The **Top14 model** uses 14 features selected by descending feature importance, which is more than the **Top 80% model (11 features)**.\n",
        "- However, based on cross-validation, the **Top14 model performed slightly worse** in both accuracy (0.844 vs 0.847) and ROC-AUC.\n",
        "- This suggests that **the additional lower-importance features (12â€“14) may have introduced noise**, slightly reducing generalization performance.\n",
        "- In contrast, the **Top 80% model achieved higher accuracy with fewer features**, indicating a more efficient use of relevant information and better regularization.\n",
        "- The fact that `min_samples_leaf=4` was optimal for the Top 80% model further supports the idea that **stronger regularization helped prevent overfitting with fewer features**.\n",
        "\n",
        "âœ… In conclusion, while the Top14 model is still solid, the **Top 80% model offers better performance with fewer features**, making it a more practical and robust choice.\n",
        "\n",
        "\n",
        "- **Top14ãƒ¢ãƒ‡ãƒ«**ã¯ã€ç‰¹å¾´é‡é‡è¦åº¦ã®é«˜ã„é †ã«14å€‹ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€**ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ï¼ˆ11å€‹ï¼‰ã‚ˆã‚Šã‚‚å¤šãã®ç‰¹å¾´é‡ã‚’ä½¿ã£ã¦ã„ã¾ã™**ã€‚\n",
        "- ã—ã‹ã—ã€äº¤å·®æ¤œè¨¼ã®çµæœã€**Top14ãƒ¢ãƒ‡ãƒ«ã¯è‹¥å¹²ç²¾åº¦ãŒåŠ£ã‚Šï¼ˆAccuracy: 0.844 vs 0.847ï¼‰**ã€**ROC-AUCã‚‚ã‚ãšã‹ã«ä½ããªã‚Šã¾ã—ãŸ**ã€‚\n",
        "- ã“ã‚Œã¯ã€**è¿½åŠ ã•ã‚ŒãŸä¸‹ä½ã®ç‰¹å¾´é‡ï¼ˆ12ã€œ14ä½ï¼‰ãŒãƒã‚¤ã‚ºã«ãªã£ã¦ã—ã¾ã„ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’ã‚ãšã‹ã«æãªã£ãŸå¯èƒ½æ€§**ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "- ä¸€æ–¹ã€**ä¸Šä½80%ãƒ¢ãƒ‡ãƒ«ã¯å°‘ãªã„ç‰¹å¾´é‡ã§ã‚ˆã‚Šé«˜ã„ç²¾åº¦**ã‚’é”æˆã—ã¦ãŠã‚Šã€**å¿…è¦ãªæƒ…å ±ã‚’åŠ¹ç‡ã‚ˆãæ‰ãˆã¤ã¤éå­¦ç¿’ã‚’é˜²ã„ã§ã„ã‚‹**ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "- ã¾ãŸã€æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ `min_samples_leaf=4` ãŒé¸ã°ã‚ŒãŸã“ã¨ã‚‚ã€**ç‰¹å¾´é‡ãŒå°‘ãªã„åˆ†ã€ã‚ˆã‚Šå¼·ã‚ã®æ­£å‰‡åŒ–ãŒåŠ¹æœçš„ã ã£ãŸ**å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "âœ… çµè«–ã¨ã—ã¦ã€**Top14ã¯æ€§èƒ½ãŒæ‚ªã„ã‚ã‘ã§ã¯ãªã„ãŒã€Top 80%ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒå°‘ãªã„ç‰¹å¾´é‡ã§ã‚ˆã‚Šé«˜ç²¾åº¦ã‹ã¤å®‰å®šã—ãŸçµæœ**ã‚’å‡ºã—ã¦ãŠã‚Šã€ã‚ˆã‚Šå®Ÿç”¨çš„ã§ã‚ã‚‹ã¨è©•ä¾¡ã§ãã¾ã™ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLxJRf0Jig6V"
      },
      "source": [
        "## 4.34 Feature Selection and Model Accuracy Evaluation / ç‰¹å¾´é‡é¸æŠã¨ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®è©•ä¾¡  \n",
        "1. ğŸ“Œ Initial Feature Selection (Cumulative Importance)  \n",
        "We initially selected features based on cumulative contribution using SHAP values and feature importance scores from a Random Forest model. We set the threshold at 80% of cumulative importance, which resulted in the best validation accuracy of 84.73%.\n",
        "\n",
        "2. ğŸ§ª Testing Handcrafted Features  \n",
        "We then introduced handcrafted features inspired by domain knowledge and misclassification analysisâ€”such as male_1_flag, female_3_flag, and mr_family_4_flag.\n",
        "However, when we added all these features to the base model, validation accuracy slightly decreased.\n",
        "\n",
        "3. ğŸ§® Evaluating Top 14 Features by Importance  \n",
        "We also tested a model using the top 14 features ranked by Random Forest's feature importance. The validation accuracy was 84.4%, close to the best but still slightly below the cumulative importance model.\n",
        "\n",
        "4. âœ… Conclusion and Final Feature Set  \n",
        "Based on both predictive performance and model simplicity, we selected the feature set derived from the top 80% cumulative importance as our final choice.\n",
        "This approach avoids unnecessary complexity and potential overfitting introduced by less effective handcrafted features.\n",
        "\n",
        "---\n",
        "1. ğŸ“Œ åˆæœŸé¸æŠï¼ˆç´¯ç©é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ï¼‰  \n",
        "ã¾ãšã€SHAPå€¤ã‚„ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ãã€ç´¯ç©é‡è¦åº¦ãŒ80ï¼…ã«é”ã™ã‚‹ã¾ã§ã®ç‰¹å¾´é‡ã‚’é¸æŠã—ã¾ã—ãŸã€‚ã“ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆãŒã€84.73ï¼…ã®æœ€é«˜æ¤œè¨¼ç²¾åº¦ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "2. ğŸ§ª æ‰‹ä½œã‚Šç‰¹å¾´é‡ã®è¿½åŠ ã¨æ¤œè¨¼  \n",
        "æ¬¡ã«ã€ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚„èª¤åˆ†é¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç€æƒ³ã‚’å¾—ã¦ä½œæˆã—ãŸæ‰‹ä½œã‚Šç‰¹å¾´é‡ï¼ˆä¾‹ï¼šmale_1_flag, female_3_flag, mr_family_4_flagï¼‰ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚\n",
        "ã—ã‹ã—ã€ã“ã‚Œã‚‰ã‚’ã™ã¹ã¦åŠ ãˆãŸå ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ç²¾åº¦ã¯ã‚ãšã‹ã«ä½ä¸‹ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "3. ğŸ§® ç‰¹å¾´é‡é‡è¦åº¦ä¸Šä½14å€‹ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡  \n",
        "ã•ã‚‰ã«ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ç‰¹å¾´é‡é‡è¦åº¦ã§ä¸Šä½14å€‹ã‚’é¸ã‚“ã ãƒ¢ãƒ‡ãƒ«ã‚‚è©•ä¾¡ã—ã¾ã—ãŸã€‚ã“ã¡ã‚‰ã®ç²¾åº¦ã¯84.4ï¼…ã¨ã€ã‹ãªã‚Šé«˜ã‹ã£ãŸã‚‚ã®ã®ã€ç´¯ç©é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ã‚ãšã‹ã«å±Šãã¾ã›ã‚“ã§ã—ãŸã€‚\n",
        "\n",
        "4. âœ… çµè«–ã¨æœ€çµ‚çš„ãªç‰¹å¾´é‡é¸æŠ  \n",
        "äºˆæ¸¬ç²¾åº¦ã¨ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒ³ãƒ—ãƒ«ã•ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ã€ç´¯ç©é‡è¦åº¦80ï¼…ã«åŸºã¥ãç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ¡ç”¨ã—ã¾ã—ãŸã€‚\n",
        "ã“ã®é¸æŠã«ã‚ˆã‚Šã€åŠ¹æœã®ä¸ç¢ºã‹ãªç‰¹å¾´é‡ã‚’æ’é™¤ã—ã€éå­¦ç¿’ã‚„è¤‡é›‘åŒ–ã‚’å›é¿ã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkQddlqfwPSW"
      },
      "source": [
        "# ğŸ¤– 5. Model Comparison / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\n",
        "In this section, we compared several machine learning models using the final feature set (based on the top 80% cumulative importance).\n",
        "We tried each model one by one with hyperparameter tuning, and evaluated their performance using accuracy, AUC, and classification report on the validation set.\n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€æœ€çµ‚çš„ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆï¼ˆç´¯ç©é‡è¦åº¦ä¸Šä½80ï¼…ï¼‰ã‚’ç”¨ã„ã¦ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ®µéšçš„ã«æ¯”è¼ƒã—ã¾ã—ãŸã€‚\n",
        "ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’è¡Œã„ã€ç²¾åº¦ï¼ˆAccuracyï¼‰ã€AUCã‚¹ã‚³ã‚¢ã€ãŠã‚ˆã³åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆClassification Reportï¼‰ã‚’ç”¨ã„ã¦æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptd0nb45X1v6"
      },
      "source": [
        "##ğŸ§ª 5.1 Models Tried / è©¦ã—ãŸãƒ¢ãƒ‡ãƒ«ä¸€è¦§\n",
        "âœ… Random Forest (RF)\n",
        "â†’ This model was already tested during feature engineering.  / ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®éç¨‹ã§æ—¢ã«ä½¿ç”¨ãƒ»æ¤œè¨¼æ¸ˆã¿\n",
        "\n",
        "1.ğŸ”¶ XGBoost  \n",
        " A powerful gradient boosting algorithm known for its performance in structured data.\n",
        "æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¼·ãã€ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã‚‚å®šç•ªã®ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã€‚\n",
        "\n",
        "2.ğŸ”· LightGBM  \n",
        "A lightweight and fast gradient boosting framework developed by Microsoft.\n",
        "é«˜é€Ÿã§è»½é‡ãªå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚XGBoostã‚ˆã‚Šé«˜é€Ÿãªã“ã¨ã‚‚ã€‚\n",
        "\n",
        "3.ğŸ”¸ Support Vector Machine (SVM)  \n",
        "Effective in high-dimensional spaces. Requires careful feature scaling.\n",
        "é«˜æ¬¡å…ƒç©ºé–“ã«å¼·ã„ãŒã€ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé‡è¦ã€‚\n",
        "\n",
        "4.ğŸ”º Voting Classifier (Hard Voting)  \n",
        "An ensemble method combining multiple base models using majority rule.\n",
        "è¤‡æ•°ã®åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦å¤šæ•°æ±ºã§äºˆæ¸¬ã‚’æ±ºå®šã™ã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã€‚\n",
        "\n",
        "5.ğŸ”° Stacking Classifier (with RF, XGB, LightGBM, SVM.)  \n",
        "A meta-model that learns to combine the predictions of several base models (RF, XGB, LightGBM, SVM).\n",
        "è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆRF, XGB, LightGBM, SVMï¼‰ã®å‡ºåŠ›ã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§çµ±åˆã™ã‚‹é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmkgnE32d_g"
      },
      "source": [
        "### âš™ï¸ **5.2 Evaluation Metrics / è©•ä¾¡æŒ‡æ¨™**\n",
        "\n",
        "To evaluate and compare the performance of each machine learning model, we used the following metrics calculated through **cross-validation (CV)**:\n",
        "\n",
        "- **CV Accuracy**: The average proportion of correct predictions across all folds. This gives an estimate of the modelâ€™s generalization performance.\n",
        "- **CV ROC AUC (Receiver Operating Characteristic - Area Under the Curve)**: Measures the modelâ€™s ability to distinguish between the two classes (Survived / Not Survived) across CV folds. This is especially important when the class distribution is imbalanced.\n",
        "- **Classification Report (based on CV predictions)**: Reports class-specific metrics such as precision, recall, and F1-score. This helps evaluate how well the model performs on each class (0 = Not Survived, 1 = Survived), beyond overall accuracy.\n",
        "\n",
        "ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ãƒ»æ¯”è¼ƒã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®æŒ‡æ¨™ã‚’**äº¤å·®æ¤œè¨¼ï¼ˆCVï¼‰**ã«åŸºã¥ã„ã¦ä½¿ç”¨ã—ã¾ã—ãŸï¼š\n",
        "\n",
        "- **CV Accuracyï¼ˆäº¤å·®æ¤œè¨¼ã®æ­£è§£ç‡ï¼‰**ï¼šå„åˆ†å‰²ã§ã®æ­£è§£ç‡ã®å¹³å‡ã‚’ç®—å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚\n",
        "- **CV ROC AUCï¼ˆäº¤å·®æ¤œè¨¼ã«ãŠã‘ã‚‹ROCæ›²ç·šä¸‹é¢ç©ï¼‰**ï¼šç”Ÿå­˜è€…ãƒ»éç”Ÿå­˜è€…ã®è­˜åˆ¥æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã§ã‚ã‚Šã€ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãŒã‚ã‚‹å ´é¢ã§ã‚‚æœ‰åŠ¹ã§ã™ã€‚\n",
        "- **åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆCVäºˆæ¸¬ã«åŸºã¥ãï¼‰**ï¼šå„ã‚¯ãƒ©ã‚¹ï¼ˆ0 = éç”Ÿå­˜ã€1 = ç”Ÿå­˜ï¼‰ã”ã¨ã«ã€**é©åˆç‡ï¼ˆPrecisionï¼‰**ã€**å†ç¾ç‡ï¼ˆRecallï¼‰**ã€**F1ã‚¹ã‚³ã‚¢**ã‚’å‡ºåŠ›ã—ã€å˜ãªã‚‹ç²¾åº¦ã§ã¯è¦‹ãˆã«ãã„ãƒ¢ãƒ‡ãƒ«ã®åã‚Šã‚„ã‚¯ãƒ©ã‚¹ã”ã¨ã®æ€§èƒ½ã‚’è©³ç´°ã«åˆ†æã—ã¾ã—ãŸã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-z4XChz3Z9Z"
      },
      "source": [
        "## 5.3 Model Training and Evaluation / ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡  \n",
        "### 5.3.1 XGBoost (Before Hyperparameter Tuning) / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å‰ã®XGBoost\n",
        "\n",
        "We started by training an XGBoost classifier using its default hyperparameters, except for two settings:\n",
        "- `use_label_encoder=False` (to suppress the deprecation warning)\n",
        "- `eval_metric='logloss'` (for binary classification)\n",
        "\n",
        "åˆæœŸæ®µéšã§ã¯ã€XGBoostã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¤ã¤ã€ä»¥ä¸‹ã®2ç‚¹ã®ã¿æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¾ã—ãŸï¼š\n",
        "- `use_label_encoder=False`ï¼ˆéæ¨å¥¨è­¦å‘Šã‚’å›é¿ã™ã‚‹ãŸã‚ï¼‰  \n",
        "- `eval_metric='logloss'`ï¼ˆ2å€¤åˆ†é¡ã®ãŸã‚ã®è©•ä¾¡æŒ‡æ¨™ï¼‰  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haW39S745DbN"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use top 80% important features / ä¸Šä½80%é‡è¦åº¦ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable: Survived / ç›®çš„å¤‰æ•°ï¼šç”Ÿå­˜è€…\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define Stratified K-Fold CV / StratifiedKFoldã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®šç¾©ï¼ˆå±¤åŒ–æŠ½å‡ºï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Initialize XGBoost classifier (Before Hyperparameter Tuning) / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å‰ã®XGBooståˆæœŸåŒ–\n",
        "# =====================================================\n",
        "model_xgb = XGBClassifier(\n",
        "    use_label_encoder=False,  # Suppress deprecation warning / éæ¨å¥¨è­¦å‘Šã‚’æŠ‘åˆ¶\n",
        "    eval_metric='logloss',    # Use logloss for binary classification / 2å€¤åˆ†é¡ã®æå¤±é–¢æ•°ã¨ã—ã¦loglossã‚’æŒ‡å®š\n",
        "    random_state=42           # Ensure reproducibility / çµæœã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Evaluate model with cross-validation / äº¤å·®æ¤œè¨¼ã§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆAccuracyã¨ROC AUCï¼‰\n",
        "# =====================================================\n",
        "accuracy = cross_val_score(model_xgb, X_selected, y_selected, cv=cv, scoring='accuracy').mean()\n",
        "roc_auc = cross_val_score(model_xgb, X_selected, y_selected, cv=cv, scoring='roc_auc').mean()\n",
        "\n",
        "print(f\"ğŸ“Œ Default XGBoost Evaluation\")\n",
        "print(f\"ğŸ“ˆ Mean CV Accuracy: {accuracy:.4f}\")  # å¹³å‡æ­£è§£ç‡ã‚’è¡¨ç¤º\n",
        "print(f\"ğŸ“ˆ Mean CV ROC AUC: {roc_auc:.4f}\")    # å¹³å‡ROC AUCã‚’è¡¨ç¤º\n",
        "\n",
        "# =====================================================\n",
        "# 5. Get cross-validated predictions and classification report / äº¤å·®æ¤œè¨¼äºˆæ¸¬å€¤ã‚’å–å¾—ã—åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º\n",
        "# =====================================================\n",
        "y_pred = cross_val_predict(model_xgb, X_selected, y_selected, cv=cv)\n",
        "print(\"ğŸ“ Classification Report:\\n\", classification_report(y_selected, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvaiUFYl51Kc"
      },
      "source": [
        "### Analysis of Default XGBoost Performance / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆXGBoostã®æ€§èƒ½ã«é–¢ã™ã‚‹è€ƒå¯Ÿ\n",
        "\n",
        "The default XGBoost classifier achieves a strong performance on this dataset, with a mean cross-validated accuracy of 82.6% and a ROC AUC score of 0.8741. These results indicate that the model is effective at distinguishing between the classes.\n",
        "\n",
        "Looking at the classification report:\n",
        "- Precision and recall are balanced fairly well between the classes, with class 0 (majority class) showing slightly higher precision and recall (0.85 and 0.87) compared to class 1 (minority class) with precision 0.78 and recall 0.76.\n",
        "- The F1-scores, which balance precision and recall, also reflect this trend (0.86 for class 0 and 0.77 for class 1).\n",
        "- Overall, the model shows robust predictive performance, but there is still room for improvement, especially for the minority class (class 1), which tends to have lower recall.\n",
        "\n",
        "These results provide a solid baseline before hyperparameter tuning and further optimization.\n",
        "\n",
        "---\n",
        "\n",
        "ã“ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®XGBooståˆ†é¡å™¨ã¯ã€äº¤å·®æ¤œè¨¼ã§å¹³å‡æ­£è§£ç‡82.6%ã€ROC AUCã‚¹ã‚³ã‚¢0.8741ã¨è‰¯å¥½ãªæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã‚¯ãƒ©ã‚¹ã®è­˜åˆ¥ã«åŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¦‹ã‚‹ã¨ï¼š\n",
        "- ã‚¯ãƒ©ã‚¹0ï¼ˆå¤šæ•°æ´¾ã‚¯ãƒ©ã‚¹ï¼‰ã¯ç²¾åº¦ï¼ˆ0.85ï¼‰ã¨å†ç¾ç‡ï¼ˆ0.87ï¼‰ãŒã‚„ã‚„é«˜ã„ã®ã«å¯¾ã—ã€ã‚¯ãƒ©ã‚¹1ï¼ˆå°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ï¼‰ã¯ç²¾åº¦0.78ã€å†ç¾ç‡0.76ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "- ç²¾åº¦ã¨å†ç¾ç‡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ç¤ºã™F1ã‚¹ã‚³ã‚¢ã‚‚åŒæ§˜ã«ã€ã‚¯ãƒ©ã‚¹0ã¯0.86ã€ã‚¯ãƒ©ã‚¹1ã¯0.77ã§ã™ã€‚\n",
        "- å…¨ä½“çš„ã«å …ç‰¢ãªäºˆæ¸¬æ€§èƒ½ã‚’æŒã£ã¦ã„ã¾ã™ãŒã€ç‰¹ã«å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹å†ç¾ç‡å‘ä¸Šã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‰ã®çµæœã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚„ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã‚’è¡Œã†å‰ã®è‰¯ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ãªã‚Šã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l32HClrJ5-8V"
      },
      "source": [
        "### ğŸ“ˆ 5.3.2 XGBoost (After Hyperparameter Tuning) / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å¾Œã®XGBoost\n",
        "\n",
        "We improved the XGBoost model by performing grid search over several hyperparameters, including tree depth, learning rate, regularization terms, and `scale_pos_weight` to handle class imbalance.\n",
        "\n",
        "XGBoostãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€æœ¨ã®æ·±ã•ãƒ»å­¦ç¿’ç‡ãƒ»æ­£å‰‡åŒ–é …ãƒ»`scale_pos_weight`ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œï¼‰ãªã©ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’è¡Œã„ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_7capfE5IIA"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use top 80% important features / ä¸Šä½80%é‡è¦åº¦ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable: Survived / ç›®çš„å¤‰æ•°ï¼šç”Ÿå­˜è€…\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define Stratified K-Fold CV / StratifiedKFoldã®å®šç¾©ï¼ˆå±¤åŒ–æŠ½å‡ºï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Initialize XGBoost classifier / XGBooståˆ†é¡å™¨ã‚’åˆæœŸåŒ–\n",
        "# =====================================================\n",
        "model_xgb = XGBClassifier(\n",
        "    use_label_encoder=False,  # Suppress warning / éæ¨å¥¨è­¦å‘Šã‚’æŠ‘åˆ¶\n",
        "    eval_metric='logloss',    # Binary classification metric / 2å€¤åˆ†é¡ã®æå¤±é–¢æ•°\n",
        "    random_state=42           # Seed for reproducibility / å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define hyperparameter grid including scale_pos_weight / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.7],\n",
        "    'colsample_bytree': [0.7],\n",
        "    'gamma': [0, 1],\n",
        "    'scale_pos_weight': [1, 2]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 5. Perform Grid Search with cross-validation / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©åŒ–ï¼ˆäº¤å·®æ¤œè¨¼ï¼‰\n",
        "# =====================================================\n",
        "xgb_gs_fe4 = GridSearchCV(\n",
        "    estimator=model_xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',       # Optimize ROC AUC / ROC AUCã§æœ€é©åŒ–\n",
        "    cv=cv,\n",
        "    n_jobs=-1,               # Use all processors / ä¸¦åˆ—å‡¦ç†\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Fit the model with GridSearchCV / å…¨ãƒ‡ãƒ¼ã‚¿ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ\n",
        "# =====================================================\n",
        "xgb_gs_fe4.fit(X_selected , y_selected )  # Train with hyperparameter tuning / å­¦ç¿’ï¼‹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
        "\n",
        "# =====================================================\n",
        "# 7. Get best estimator and parameters / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
        "# =====================================================\n",
        "best_model_xgb = xgb_gs_fe4.best_estimator_  # Best model after grid search / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå¾Œã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
        "best_params_xgb = xgb_gs_fe4.best_params_    # Best hyperparameters / æœ€è‰¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "xgb_best_cv_acc = xgb_gs_fe4.best_score_     # Best cross-validation accuracy / æœ€è‰¯äº¤å·®æ¤œè¨¼ç²¾åº¦\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluation function (shared with XGBoost) / è©•ä¾¡é–¢æ•°ï¼ˆXGBoostã¨å…±é€šåŒ–ï¼‰\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡ã™ã‚‹é–¢æ•°\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Accuracy score / ç²¾åº¦ã‚¹ã‚³ã‚¢\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # ROC AUC score / ROC AUCã‚¹ã‚³ã‚¢\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}  # Return mean scores / å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¿”ã™\n",
        "\n",
        "# =====================================================\n",
        "# 9. Evaluate best LGBM model with CV / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_xgb = evaluate_model_cv(best_model_xgb, X_selected, y_selected, cv)  # Evaluate the best LightGBM model / æœ€è‰¯LightGBMãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡\n",
        "\n",
        "# =====================================================\n",
        "# 10. Classification report with cross-validated predictions / CVäºˆæ¸¬ã«ã‚ˆã‚‹åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
        "# =====================================================\n",
        "y_pred_cv_xgb = cross_val_predict(best_model_xgb, X_selected, y_selected, cv=cv)  # CVäºˆæ¸¬ï¼ˆäºˆæ¸¬å€¤ã‚’å–å¾—ï¼‰\n",
        "class_report_xgb = classification_report(y_selected, y_pred_cv_xgb)  # Classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Best Parameters (LGBM):\", best_params_xgb)  # Best parameters output / æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", scores_xgb['accuracy'])  # Mean CV accuracy output / å¹³å‡CVç²¾åº¦ã®è¡¨ç¤º\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", scores_xgb['roc_auc'])  # Mean CV ROC AUC output / å¹³å‡CV ROC AUCã®è¡¨ç¤º\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", class_report_xgb)  # Classification report output / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Š Model Performance Comparison / ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒï¼ˆXGBoostï¼‰\n",
        "\n",
        "| Evaluation Metric / è©•ä¾¡æŒ‡æ¨™            | â‘  Default XGBoost / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š | â‘¡ Tuned XGBoost / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ |\n",
        "| ----------------------------------- | --------------------------- | ------------------------- |\n",
        "| ğŸ“ˆ Mean CV Accuracy / å¹³å‡æ­£è§£ç‡         | 0.8260                      | **0.8395**                |\n",
        "| ğŸ“ˆ Mean CV ROC AUC / å¹³å‡ROC AUC      | 0.8741                      | **0.8868**                |\n",
        "| ğŸ§® Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰  | 0.78                        | **0.81**                     |\n",
        "| ğŸ§® Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰     | 0.76                        | 0.76                      |\n",
        "| ğŸ§® F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ | 0.77                        | **0.78**    \n",
        "\n",
        "\n",
        "### ğŸ“Œ XGBoost Performance Comparison / XGBoostã®æ€§èƒ½æ¯”è¼ƒ\n",
        "\n",
        "**English:**After tuning the XGBoost hyperparameters using Grid Search with a reduced parameter grid (16 combinations), we observed improvements in both the mean cross-validation accuracy (from 0.8260 to 0.8395) and the ROC AUC (from 0.8741 to 0.8868). The F1-score for the minority class (class 1) also slightly improved, indicating better overall balance in classification performance. The gain in precision for class 1 (from 0.78 to 0.81) suggests reduced false positives.\n",
        "\n",
        "\n",
        "XGBoostã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ï¼ˆçµ„ã¿åˆã‚ã›æ•°ã‚’çµã£ã¦ï¼‰ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§èª¿æ•´ã—ãŸçµæœã€å¹³å‡æ­£è§£ç‡ã¯0.8260ã‹ã‚‰0.8395ã«ã€ROC AUCã¯0.8741ã‹ã‚‰0.8868ã«å‘ä¸Šã—ã¾ã—ãŸã€‚ç‰¹ã«ã€ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜è€…ï¼‰ã®F1ã‚¹ã‚³ã‚¢ã¨é©åˆç‡ãŒã‚ãšã‹ã«æ”¹å–„ã—ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šãƒãƒ©ãƒ³ã‚¹ã‚ˆãã‚¯ãƒ©ã‚¹åˆ†é¡ã§ãã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚èª¤åˆ†é¡ï¼ˆFalse Positiveï¼‰ã®æ¸›å°‘ã‚‚æœŸå¾…ã•ã‚Œã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "J7OOE6OP4IFg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPAJYMYU2wJH"
      },
      "source": [
        "### 5.3.3 LightGBM (Before Hyperparameter Tuning) / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å‰ã®LightGBM  \n",
        "Before tuning, we trained a basic LightGBM model with fixed parameters to establish a performance baseline.  \n",
        "Although these are not the full defaults (e.g., `max_depth=3`, `learning_rate=0.2`), no tuning was performed and the setup was consistent with our initial XGBoost test.\n",
        "\n",
        "ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã€LightGBMãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹æˆã§å­¦ç¿’ã—ã¾ã—ãŸã€‚  \n",
        "ã“ã“ã§ã¯ `max_depth=3` ã‚„ `learning_rate=0.2` ãªã©ä¸€éƒ¨ã®è¨­å®šã¯ã—ã¦ã„ã¾ã™ãŒã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯è¡Œã£ã¦ã„ã¾ã›ã‚“ã€‚XGBoostã®åˆæœŸè¨­å®šã¨åŒæ§˜ã®æ¡ä»¶ã§è©•ä¾¡ã™ã‚‹ã“ã¨ãŒç›®çš„ã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsN8brHgmZZ0"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable / ç›®çš„å¤‰æ•°\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define Stratified K-Fold CV / StratifiedKFoldã®å®šç¾©ï¼ˆå±¤åŒ–æŠ½å‡ºï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Initialize LightGBM classifier with basic parameters / LightGBMåˆ†é¡å™¨ã‚’åˆæœŸåŒ–ï¼ˆåŸºæœ¬çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š)\n",
        "# =====================================================\n",
        "model_lgb = lgb.LGBMClassifier(\n",
        "    max_depth=3,               # Tree max depth / æœ¨ã®æœ€å¤§æ·±ã•\n",
        "    learning_rate=0.2,         # Learning rate / å­¦ç¿’ç‡\n",
        "    n_estimators=100,          # Number of trees / æœ¨ã®æ•°\n",
        "    random_state=42            # å†ç¾æ€§ã‚’ç¢ºä¿\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Evaluate model with cross-validation (Accuracy and ROC AUC) / äº¤å·®æ¤œè¨¼ã§ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆæ­£è§£ç‡ã¨ROC AUCï¼‰\n",
        "# =====================================================\n",
        "accuracy = cross_val_score(model_lgb, X_selected, y_selected, cv=cv, scoring='accuracy').mean()\n",
        "roc_auc = cross_val_score(model_lgb, X_selected, y_selected, cv=cv, scoring='roc_auc').mean()\n",
        "\n",
        "print(f\"Mean CV Accuracy: {accuracy:.4f}\")  # å¹³å‡æ­£è§£ç‡ã‚’è¡¨ç¤º\n",
        "print(f\"Mean CV ROC AUC: {roc_auc:.4f}\")    # å¹³å‡ROC AUCã‚’è¡¨ç¤º\n",
        "\n",
        "# =====================================================\n",
        "# 5. Get cross-validated predictions and classification report / äº¤å·®æ¤œè¨¼äºˆæ¸¬å€¤ã‚’å–å¾—ã—ã€åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º\n",
        "# =====================================================\n",
        "y_pred = cross_val_predict(model_lgb, X_selected, y_selected, cv=cv)\n",
        "print(\"Classification Report:\\n\", classification_report(y_selected, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c9HcrK47cDM"
      },
      "source": [
        "### ğŸ“ Observations / è€ƒå¯Ÿ  \n",
        "Even without hyperparameter tuning, the model achieved solid performance with ~80.6% accuracy.\n",
        "\n",
        "Class 1 (survived) had slightly lower recall, a common issue in imbalanced datasets.\n",
        "\n",
        "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ã„ãªã„æ®µéšã§ã‚‚ã€80%ã‚’è¶…ãˆã‚‹ç²¾åº¦ã‚’é”æˆã€‚\n",
        "\n",
        "ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ï¼ˆ1ï¼‰ã®å†ç¾ç‡ã¯ã‚„ã‚„ä½ã„ãŒã€ä»Šå¾Œã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§æ”¹å–„ã®ä½™åœ°ã‚ã‚Šã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zHV4mQrAiyH"
      },
      "source": [
        "### 5.3.4 LightGBM (After Hyperparameter Tuning) / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å¾Œã®LightGBM  \n",
        "To improve LightGBMâ€™s performance, we conducted a grid search over multiple hyperparameters including tree depth, learning rate, and sampling strategies. Additionally, `scale_pos_weight` was tuned to mitigate class imbalance.\n",
        "\n",
        "LightGBMã®æ€§èƒ½ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€æœ¨ã®æ·±ã•ã€å­¦ç¿’ç‡ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æˆ¦ç•¥ãªã©ã€è¤‡æ•°ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’è¡Œã„ã¾ã—ãŸã€‚ã¾ãŸã€ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡ã«å¯¾å¿œã™ã‚‹ãŸã‚ `scale_pos_weight` ã‚‚èª¿æ•´å¯¾è±¡ã¨ã—ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymyCHEVp6aZi"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use top 80% important features / ä¸Šä½80%é‡è¦åº¦ã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable: Survived / ç›®çš„å¤‰æ•°ï¼šç”Ÿå­˜è€…\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldã®å®šç¾©ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# 5åˆ†å‰²äº¤å·®æ¤œè¨¼ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šï¼‰ï¼šãƒ‡ãƒ¼ã‚¿ã‚’5ã¤ã«åˆ†ã‘ã€å„éƒ¨åˆ†ã§è¨“ç·´ã¨æ¤œè¨¼ã‚’è¡Œã„ã¾ã™ã€‚\n",
        "# ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ãŒãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ãƒã‚¤ã‚¢ã‚¹ã‚’æ¸›å°‘ã•ã›ã¾ã™ã€‚\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define hyperparameter grid for LightGBM / LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
        "# =====================================================\n",
        "lgb_param_grid = {\n",
        "    'max_depth': [3, 5, 7],               # Depth of the trees / æœ¨ã®æ·±ã•\n",
        "    'learning_rate': [0.05, 0.1, 0.2],    # Learning rate / å­¦ç¿’ç‡\n",
        "    'n_estimators': [50, 100, 200],       # Number of trees / æœ¨ã®æ•°\n",
        "    'subsample': [0.8, 1.0],               # Fraction of samples used per tree / å„æœ¨ã®å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®å‰²åˆ\n",
        "    'colsample_bytree': [0.8, 1.0],        # Fraction of features used per tree / å„æœ¨ã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®å‰²åˆ\n",
        "    'scale_pos_weight': [1, 5, 10]         # Weight for positive class (class imbalance adjustment) / æ­£ã®ã‚¯ãƒ©ã‚¹ã®é‡ã¿ï¼ˆä¸å‡è¡¡èª¿æ•´ï¼‰\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Initialize LightGBM model / ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
        "# =====================================================\n",
        "lgb_model_fe4 = LGBMClassifier(random_state=42)  # LightGBMã®ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆå†ç¾æ€§ã®ãŸã‚random_stateã‚’è¨­å®šï¼‰\n",
        "\n",
        "# =====================================================\n",
        "# 5. Grid Search with CV / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆäº¤å·®æ¤œè¨¼ä»˜ãï¼‰\n",
        "# =====================================================\n",
        "\n",
        "lgb_gs_fe4 = GridSearchCV(\n",
        "    estimator=lgb_model_fe4,              # Estimator / ãƒ¢ãƒ‡ãƒ«\n",
        "    param_grid=lgb_param_grid,             # Hyperparameter grid / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç¯„å›²\n",
        "    cv=cv,                                 # Cross-validation strategy / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥\n",
        "    scoring='accuracy',                    # Evaluation metric: Accuracy / è©•ä¾¡æŒ‡æ¨™ï¼šç²¾åº¦\n",
        "    n_jobs=-1,                             # Use all available CPUs / åˆ©ç”¨å¯èƒ½ãªCPUã‚’ã™ã¹ã¦ä½¿ç”¨\n",
        "    verbose=1,                             # Show progress of grid search / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®é€²æ—ã‚’è¡¨ç¤º\n",
        "    error_score='raise'                    # Raise error if any occurs / ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Fit the model with GridSearchCV / å…¨ãƒ‡ãƒ¼ã‚¿ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ\n",
        "# =====================================================\n",
        "lgb_gs_fe4.fit(X_selected, y_selected)  # Fit the model using grid search / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’\n",
        "\n",
        "# =====================================================\n",
        "# 7. Get best estimator and parameters / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
        "# =====================================================\n",
        "best_model_lgb = lgb_gs_fe4.best_estimator_  # Best model after grid search / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå¾Œã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
        "best_params_lgb = lgb_gs_fe4.best_params_    # Best hyperparameters / æœ€è‰¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "lgb_best_cv_acc = lgb_gs_fe4.best_score_     # Best cross-validation accuracy / æœ€è‰¯äº¤å·®æ¤œè¨¼ç²¾åº¦\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluation function (shared with XGBoost) / è©•ä¾¡é–¢æ•°ï¼ˆXGBoostã¨å…±é€šåŒ–ï¼‰\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡ã™ã‚‹é–¢æ•°\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Accuracy score / ç²¾åº¦ã‚¹ã‚³ã‚¢\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # ROC AUC score / ROC AUCã‚¹ã‚³ã‚¢\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}  # Return mean scores / å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¿”ã™\n",
        "\n",
        "# =====================================================\n",
        "# 9. Evaluate best LGBM model with CV / ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_lgb = evaluate_model_cv(best_model_lgb, X_selected, y_selected, cv)  # Evaluate the best LightGBM model / æœ€è‰¯LightGBMãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡\n",
        "\n",
        "# =====================================================\n",
        "# 10. Classification report with cross-validated predictions / CVäºˆæ¸¬ã«ã‚ˆã‚‹åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
        "# =====================================================\n",
        "y_pred_cv_lgb = cross_val_predict(best_model_lgb, X_selected, y_selected, cv=cv)  # CVäºˆæ¸¬ï¼ˆäºˆæ¸¬å€¤ã‚’å–å¾—ï¼‰\n",
        "class_report_lgb = classification_report(y_selected, y_pred_cv_lgb)  # Classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Best Parameters (LGBM):\", best_params_lgb)  # Best parameters output / æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", scores_lgb['accuracy'])  # Mean CV accuracy output / å¹³å‡CVç²¾åº¦ã®è¡¨ç¤º\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", scores_lgb['roc_auc'])  # Mean CV ROC AUC output / å¹³å‡CV ROC AUCã®è¡¨ç¤º\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", class_report_lgb)  # Classification report output / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“‹ LightGBM Performance Comparison Table / LightGBMæ€§èƒ½æ¯”è¼ƒè¡¨   \n",
        "\n",
        "| Evaluation Metric / è©•ä¾¡æŒ‡æ¨™         | Before Tuning / èª¿æ•´å‰ | After Tuning / èª¿æ•´å¾Œ |\n",
        "| -------------------------------- | ------------------- | ------------------ |\n",
        "| Mean CV Accuracy / å¹³å‡ç²¾åº¦          | 0.8316              | 0.8384             |\n",
        "| Mean ROC AUC / å¹³å‡ROC AUC         | 0.8798              | 0.8803             |\n",
        "| Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰  | 0.79                | 0.81               |\n",
        "| Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰     | 0.76                | 0.76               |\n",
        "| F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ | 0.79                | 0.78               |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qr7x8IY26Ufx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrRy4OUy_MtQ"
      },
      "source": [
        "### ğŸ“Š LightGBM Performance Comparison (Before vs After Hyperparameter Tuning) / LightGBMã®æ€§èƒ½æ¯”è¼ƒï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ å‰å¾Œï¼‰  \n",
        "\n",
        "\n",
        "After hyperparameter tuning, LightGBM showed a slight improvement in performance:\n",
        "\n",
        "- Mean CV Accuracy improved from 0.8316 to 0.8384\n",
        "\n",
        "- Mean ROC AUC increased slightly from 0.8798 to 0.8803\n",
        "\n",
        "- F1-score for class 1 (survivors) remained steady at 0.78, but precision improved (from 0.79 to 0.81)\n",
        "\n",
        "These results suggest that the tuning helped the model make more confident and accurate predictions for the positive class, especially by slightly reducing false positives.\n",
        "\n",
        "Overall, the performance gain is modest but meaningful, especially considering class imbalance. The optimized model maintains strong performance for both classes.  \n",
        "\n",
        "---\n",
        "\n",
        "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å¾Œã®LightGBMã¯ã€ã‚ãšã‹ã«æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã—ãŸï¼š\n",
        "\n",
        "- Mean CV Accuracyã¯ 0.8316 â†’ 0.8384 ã«æ”¹å–„\n",
        "\n",
        "- Mean ROC AUCã‚‚ 0.8798 â†’ 0.8803 ã«å¾®å¢—\n",
        "\n",
        "- ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜è€…ï¼‰ã®F1ã‚¹ã‚³ã‚¢ã¯ 0.78ã§å®‰å®šã—ã¦ã„ã‚‹ã‚‚ã®ã®ã€precisionã¯0.79 â†’ 0.81ã¨æ”¹å–„\n",
        "\n",
        "ã“ã‚Œã‚‰ã®çµæœã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ãŒ ã‚¯ãƒ©ã‚¹1ã®äºˆæ¸¬ã«ãŠã„ã¦ã€ã‚ˆã‚Šç¢ºä¿¡ã‚’ã‚‚ã£ã¦æ­£ã—ãåˆ¤å®šã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸï¼ˆå½é™½æ€§ã®æ¸›å°‘ï¼‰ ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "å…¨ä½“ã¨ã—ã¦ã€æ”¹å–„å¹…ã¯å°ã•ã„ãªãŒã‚‰ã‚‚æ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦ã‚‚å®‰å®šã—ãŸæ€§èƒ½ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJEXzw2sEifG"
      },
      "source": [
        "### 5.3.5 Default SVM Model Evaluation / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆSVMãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ç¢ºèª\n",
        "\n",
        "To evaluate the baseline performance of a Support Vector Machine (SVM), we trained a default SVM model using selected features. Because SVMs are sensitive to feature scales, we standardized all numerical variables (Fare_log, Age, Family, TicketGroupSize) using StandardScaler.\n",
        "\n",
        "SVMã®åŸºæœ¬æ€§èƒ½ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ç”¨ã„ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®SVMãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚  \n",
        "SVMã¯ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æ•æ„Ÿã§ã‚ã‚‹ãŸã‚ã€æ•°å€¤ç‰¹å¾´é‡ï¼ˆFare_log, Age, Family, TicketGroupSizeï¼‰ã‚’StandardScalerã§æ¨™æº–åŒ–ã—ã¾ã—ãŸã€‚  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable / ç›®çš„å¤‰æ•°\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features / æ•°å€¤ç‰¹å¾´é‡\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡\n",
        "\n",
        "# =====================================================\n",
        "# 3. Create ColumnTransformer to scale only numerical features / æ•°å€¤ç‰¹å¾´é‡ã®ã¿ã‚’æ¨™æº–åŒ–ï¼ˆã‚«ãƒ†ã‚´ãƒªã¯ãã®ã¾ã¾é€šã™ï¼‰\n",
        "# =====================================================\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),   # Apply StandardScaler to numerical features / æ•°å€¤ç‰¹å¾´é‡ã«StandardScalerã‚’é©ç”¨\n",
        "        ('cat', 'passthrough', categorical_features)     # Leave categorical features unchanged / ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾\n",
        "    ]\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Set up Stratified K-Fold Cross-Validation / StratifiedKFoldã®è¨­å®šï¼ˆã‚¯ãƒ©ã‚¹ã®å‰²åˆã‚’ä¿ã¤ï¼‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV with shuffling / ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šã®5åˆ†å‰²äº¤å·®æ¤œè¨¼\n",
        "\n",
        "# =====================================================\n",
        "# 5. Create pipeline with LinearSVC / LinearSVCã‚’ä½¿ã£ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ\n",
        "# =====================================================\n",
        "pipeline_linear_svc = Pipeline([\n",
        "    ('preprocessor', preprocessor),                         # Preprocessing step / å‰å‡¦ç†ï¼ˆæ¨™æº–åŒ–ï¼‹ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ï¼‰\n",
        "    ('linear_svc', LinearSVC(random_state=42, max_iter=10000))  # LinearSVC classifier / ç·šå½¢SVMåˆ†é¡å™¨\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 6. Define evaluation function / ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–¢æ•°ã®å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Mean accuracy / å¹³å‡æ­£è§£ç‡\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # Mean ROC AUC / å¹³å‡ROC AUCã‚¹ã‚³ã‚¢\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluate LinearSVC with cross-validation / LinearSVCã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_linear_svc = evaluate_model_cv(pipeline_linear_svc, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 8. Get CV predictions and generate classification report / äº¤å·®æ¤œè¨¼ã®äºˆæ¸¬å€¤ã‹ã‚‰åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\n",
        "# =====================================================\n",
        "y_pred_cv_linear_svc = cross_val_predict(pipeline_linear_svc, X_selected, y_selected, cv=cv)  # Cross-validated predictions / äº¤å·®æ¤œè¨¼äºˆæ¸¬å€¤\n",
        "report_linear_svc = classification_report(y_selected, y_pred_cv_linear_svc)                   # Generate report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\n",
        "\n",
        "# =====================================================\n",
        "# 9. Output evaluation results / è©•ä¾¡çµæœã‚’å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"\\n==== Default LinearSVC ====\")\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", scores_linear_svc['accuracy'])  # Mean accuracy score / å¹³å‡æ­£è§£ç‡\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", scores_linear_svc['roc_auc'])    # Mean ROC AUC score / å¹³å‡ROC AUC\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_linear_svc)       # Classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "BEi6vvglytf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWWh63oRg-iF"
      },
      "source": [
        "## ğŸ“Œ Default SVM Evaluation (Pipeline) - Consideration / è€ƒå¯Ÿ\n",
        "\n",
        "The default LinearSVC model achieved a mean cross-validated accuracy of approximately 81.5% and a ROC AUC score of about 0.87. These results indicate a strong performance for this baseline model on the Titanic dataset.\n",
        "\n",
        "Looking at the classification report, the model shows balanced precision and recall for both classes. The precision and recall for the non-survivor class (class 0) are particularly high, suggesting the model is effective at identifying passengers who did not survive. For the survivor class (class 1), precision and recall are slightly lower but still satisfactory, indicating the model reasonably predicts survivors.\n",
        "\n",
        "Given these results, the default LinearSVC provides a reliable baseline. Future improvements may come from hyperparameter tuning. Overall, this model can serve as a solid foundation for further experiments.\n",
        "\n",
        "---\n",
        "\n",
        "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LinearSVCãƒ¢ãƒ‡ãƒ«ã¯ã€äº¤å·®æ¤œè¨¼ã§ç´„81.5ï¼…ã®æ­£è§£ç‡ã¨ROC AUCã‚¹ã‚³ã‚¢ç´„0.87ã‚’é”æˆã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€Titanicãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã‘ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ååˆ†ãªæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¦‹ã‚‹ã¨ã€éç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹0ï¼‰ã«å¯¾ã—ã¦ã¯ç²¾åº¦ã¨å†ç¾ç‡ãŒé«˜ãã€ã“ã®ã‚¯ãƒ©ã‚¹ã®è­˜åˆ¥ã«å„ªã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ä¸€æ–¹ã§ã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã«ã¤ã„ã¦ã¯ç²¾åº¦ãƒ»å†ç¾ç‡ãŒã‚„ã‚„ä½ã„ã‚‚ã®ã®ã€å¦¥å½“ãªäºˆæ¸¬ãŒã§ãã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ã“ã®çµæœã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LinearSVCã¯ä¿¡é ¼ã§ãã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨è¨€ãˆã¾ã™ã€‚ä»Šå¾Œã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã‚’é€šã˜ã¦ã€ã•ã‚‰ãªã‚‹æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ã€‚å…¨ä½“ã¨ã—ã¦ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®å®Ÿé¨“ã®è‰¯ã„å‡ºç™ºç‚¹ã¨ãªã‚Šã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph772VJwiEeV"
      },
      "source": [
        "## 5.3.6 SVM with Class Weight Adjustment / ã‚¯ãƒ©ã‚¹é‡ã¿èª¿æ•´ä»˜ãSVM\n",
        "To mitigate the effects of class imbalance in the Titanic dataset, we added class_weight='balanced' as one of the parameters during hyperparameter tuning of the SVM model.\n",
        "This setting instructs the model to automatically assign higher weights to minority classes, such as survivors, which helps improve recall and F1-score for that class.\n",
        "We compared models with and without class weight adjustment using GridSearchCV and selected the best-performing configuration based on cross-validation accuracy.\n",
        "\n",
        "ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ï¼ˆç”Ÿå­˜è€…ã¨éç”Ÿå­˜è€…ã®æ¯”ç‡ã®å·®ï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€SVMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã« class_weight='balanced' ã‚’å€™è£œã¨ã—ã¦è¿½åŠ ã—ã¾ã—ãŸã€‚\n",
        "ã“ã®è¨­å®šã«ã‚ˆã‚Šã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ï¼ˆç”Ÿå­˜è€…ãªã©ï¼‰ã«è‡ªå‹•ã§é‡ã¿ãŒä¸ãˆã‚‰ã‚Œã€å†ç¾ç‡ã‚„F1ã‚¹ã‚³ã‚¢ã®æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™ã€‚\n",
        "GridSearchCV ã«ã‚ˆã£ã¦é‡ã¿ã®æœ‰ç„¡ã‚’å«ã‚ãŸè¤‡æ•°ã®è¨­å®šã‚’æ¯”è¼ƒã—ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ç²¾åº¦ã«åŸºã¥ã„ã¦æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã—ãŸã€‚  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features for training / é¸æŠã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable: survival status / ç›®çš„å¤‰æ•°ï¼šç”Ÿå­˜ã®æœ‰ç„¡\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize'] # æ•°å€¤ç‰¹å¾´é‡ï¼ˆæ¨™æº–åŒ–å¯¾è±¡ï¼‰\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # ãã®ä»–ã‚’ã‚«ãƒ†ã‚´ãƒªæ‰±ã„\n",
        "\n",
        "# =====================================================\n",
        "# 3. ColumnTransformer (only scale numerical features) / æ•°å€¤ç‰¹å¾´é‡ã®ã¿ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "# =====================================================\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),     # æ•°å€¤ç‰¹å¾´é‡ã«StandardScalerã‚’é©ç”¨\n",
        "    ('cat', 'passthrough', categorical_features)       # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾é€šã™\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define Stratified K-Fold / StratifiedKFold ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # å±¤åŒ–5åˆ†å‰²äº¤å·®æ¤œè¨¼ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šï¼‰\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define LinearSVC pipeline / LinearSVC ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆé«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼‰\n",
        "# =====================================================\n",
        "pipeline_linsvc = Pipeline([\n",
        "    ('preprocessor', preprocessor),                             # å‰å‡¦ç†ï¼ˆæ¨™æº–åŒ–ï¼‹ã‚«ãƒ†ã‚´ãƒªé€šéï¼‰\n",
        "    ('svm', LinearSVC(random_state=42, max_iter=5000))          # LinearSVC ãƒ¢ãƒ‡ãƒ«\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 6. Define hyperparameter grid / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "param_grid_linsvc = {\n",
        "    'svm__C': [0.1, 1, 10],                      # æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿C\n",
        "    'svm__class_weight': [None, 'balanced']      # ã‚¯ãƒ©ã‚¹é‡ã¿ã®æœ‰ç„¡ï¼ˆä¸å‡è¡¡å¯¾ç­–ï¼‰\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 7. Grid Search with ROC AUC scoring / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆROC AUCã§è©•ä¾¡ï¼‰\n",
        "# =====================================================\n",
        "grid_linsvc = GridSearchCV(\n",
        "    pipeline_linsvc,\n",
        "    param_grid=param_grid_linsvc,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',     # ROC AUC ã‚’æŒ‡æ¨™ã«æœ€é©åŒ–\n",
        "    n_jobs=-1,             # CPUã™ã¹ã¦ä½¿ã†\n",
        "    verbose=1              # é€²æ—è¡¨ç¤ºã‚ã‚Š\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 8. Fit GridSearchCV / ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ\n",
        "# =====================================================\n",
        "grid_linsvc.fit(X_selected, y_selected)  # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢ã—ãªãŒã‚‰å­¦ç¿’\n",
        "\n",
        "# =====================================================\n",
        "# 9. Evaluate best model / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡\n",
        "# =====================================================\n",
        "best_model_svm = grid_linsvc.best_estimator_   # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«\n",
        "best_params_svm = grid_linsvc.best_params_     # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "\n",
        "# =====================================================\n",
        "# 10. Evaluation function / è©•ä¾¡é–¢æ•°\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡ã™ã‚‹é–¢æ•°\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Accuracy\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’å–å¾—\n",
        "cv_results = evaluate_model_cv(best_model_svm, X_selected, y_selected, cv=cv)\n",
        "cv_accuracy_svm = cv_results['accuracy']\n",
        "cv_roc_auc_svm = cv_results['roc_auc']\n",
        "\n",
        "# =====================================================\n",
        "# 11. Classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆäº¤å·®æ¤œè¨¼äºˆæ¸¬ã‚’ç”¨ã„ã¦ï¼‰\n",
        "# =====================================================\n",
        "y_pred_cv = cross_val_predict(best_model_svm, X_selected, y_selected, cv=cv)\n",
        "report_svm = classification_report(y_selected, y_pred_cv)\n",
        "\n",
        "# =====================================================\n",
        "# 12. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Tuned LinearSVC Evaluation\")\n",
        "print(\"ğŸ“Œ Best Parameters:\", best_params_svm)               # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º\n",
        "print(\"ğŸ“ˆ Mean CV Accuracy:\", cv_accuracy_svm)              # å¹³å‡Accuracy\n",
        "print(\"ğŸ“ˆ Mean CV ROC AUC:\", cv_roc_auc_svm)                # å¹³å‡ROC AUC\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", report_svm)  # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ"
      ],
      "metadata": {
        "id": "aee54xdmDZre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“‹ SVM Performance Comparison Table / SVMæ€§èƒ½æ¯”è¼ƒè¡¨  \n",
        "\n",
        "| Evaluation Metric / è©•ä¾¡æŒ‡æ¨™             | Before Tuning / èª¿æ•´å‰ | After Tuning / èª¿æ•´å¾Œ |\n",
        "| ------------------------------------ | ------------------- | ------------------ |\n",
        "| **Mean CV Accuracy / å¹³å‡ç²¾åº¦**          | 0.8148              | 0.8114             |\n",
        "| **Mean ROC AUC / å¹³å‡ROC AUC**         | 0.8657              | 0.8667             |  \n",
        "| **Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰**  | 0.77                | 0.74               |\n",
        "| **Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰**     | 0.73                | 0.78               |\n",
        "| **F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰** | 0.75                | 0.76               |\n",
        "\n",
        "\n",
        "###ğŸ” Interpretation / è€ƒå¯Ÿ\n",
        "- Specifying class_weight='balanced' improved recall for the minority class (Class 1: survivors).\n",
        "\n",
        "- While the overall accuracy remained nearly the same, the F1-score for Class 1 increased, indicating better sensitivity to the minority class.\n",
        "\n",
        "- The ROC AUC score also improved slightly, showing a more balanced overall discrimination ability of the model.  \n",
        "\n",
        "- class_weight='balanced' ã‚’æŒ‡å®šã—ãŸã“ã¨ã§ã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹1ï¼šç”Ÿå­˜è€…ï¼‰ã®å†ç¾ç‡ãŒå‘ä¸Šã—ã¾ã—ãŸã€‚\n",
        "\n",
        "- å…¨ä½“ã®ç²¾åº¦ï¼ˆaccuracyï¼‰ã¯ã»ã¼åŒã˜ã§ã—ãŸãŒã€ã‚¯ãƒ©ã‚¹1ã®F1ã‚¹ã‚³ã‚¢ãŒæ”¹å–„ã—ã¦ãŠã‚Šã€å°‘æ•°æ´¾ã‚¯ãƒ©ã‚¹ã¸ã®æ„Ÿåº¦ãŒé«˜ã¾ã‚Šã¾ã—ãŸã€‚\n",
        "\n",
        "- ã¾ãŸã€ROC AUC ã‚¹ã‚³ã‚¢ã‚‚ã‚ãšã‹ã«æ”¹å–„ã—ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®è­˜åˆ¥æ€§èƒ½ãŒã‚ˆã‚Šãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚‚ã®ã«ãªã£ã¦ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "3ue8TCCCAxlx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDni4bfrzfrp"
      },
      "source": [
        "### Tuned SVM Performance Summary / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®SVMãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½\n",
        "\n",
        "By adjusting the class weight, the model becomes more sensitive to the minority class. Although accuracy slightly dropped, recall and F1-score for the survivors improved, which is crucial in imbalanced classification tasks like this one.\n",
        "\n",
        "ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’èª¿æ•´ã—ãŸã“ã¨ã§ã€ç”Ÿå­˜è€…ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹æ„Ÿåº¦ãŒé«˜ã¾ã‚Šã€å†ç¾ç‡ãƒ»F1ã‚¹ã‚³ã‚¢ãŒæ”¹å–„ã—ã¾ã—ãŸã€‚ã‚ãšã‹ãªç²¾åº¦ã®ä½ä¸‹ã¯è¨±å®¹ç¯„å›²å†…ã§ã‚ã‚Šã€å°‘æ•°ã‚¯ãƒ©ã‚¹ã®èª¤åˆ¤å®šã‚’æ¸›ã‚‰ã›ãŸã“ã¨ã¯å®Ÿç”¨ä¸Šé‡è¦ã§ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3M81UE5xizg"
      },
      "source": [
        "##5.3.7 Ensemble Models: Voting & Stacking / ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼šVotingã¨Stacking\n",
        "To enhance prediction performance, we implemented two ensemble learning methods: Voting and Stacking.\n",
        "\n",
        "- Voting combines predictions from multiple base models (e.g., SVM, Random Forest) using majority voting or averaged predicted probabilities.\n",
        "\n",
        "- Stacking is a method that uses the output predictions from base models as input features for a meta-model, which makes the final prediction. In this study, we compared four types of meta-models: (1) Logistic Regression, (2) Random Forest, (3) XGBoost, and (4) LightGBM.\n",
        "\n",
        "Since tree-based models such as Random Forest, XGBoost, and LightGBM are not affected by feature scaling, we applied standardization only to the SVM model via a pipeline. This allowed us to maintain consistency across models while preserving optimal preprocessing for each algorithm.\n",
        "\n",
        "äºˆæ¸¬æ€§èƒ½ã®å‘ä¸Šã‚’ç›®çš„ã¨ã—ã¦ã€Voting ãŠã‚ˆã³ Stacking ã®2ç¨®é¡ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "- Voting ã¯ã€è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼šSVMã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰ã®äºˆæ¸¬ã‚’ã€å¤šæ•°æ±ºã¾ãŸã¯ç¢ºç‡ã®å¹³å‡ã«ã‚ˆã‚Šçµ±åˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚\n",
        "\n",
        "- Stacking ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›çµæœã‚’ç‰¹å¾´é‡ã¨ã—ã¦ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã€æœ€çµ‚çš„ãªäºˆæ¸¬ã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚ä»Šå›ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ï¼ˆ1ï¼‰ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€ï¼ˆ2ï¼‰ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆ3ï¼‰XGBoostã€(4)LightGBM ã®4ç¨®é¡ã‚’æ¯”è¼ƒã—ã¾ã—ãŸã€‚\n",
        "\n",
        "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€XGBoostã€LightGBMãªã©ã®æ±ºå®šæœ¨ç³»ãƒ¢ãƒ‡ãƒ«ã¯ã€ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«å½±éŸ¿ã•ã‚Œãªã„ãŸã‚ã€SVMãƒ¢ãƒ‡ãƒ«ã®ã¿ã«æ¨™æº–åŒ–å‡¦ç†ã‚’é©ç”¨ã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã«å¿œã˜ãŸå‰å‡¦ç†ã‚’è¡Œã„ã¤ã¤ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å…¨ä½“ã¨ã—ã¦ã®æ•´åˆæ€§ã‚’ç¢ºä¿ã—ã¾ã—ãŸã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9MJYSjLGHuq"
      },
      "source": [
        "### 5.3.8 ğŸ§  Voting Ensemble (Soft Voting with Standardized SVM)\n",
        "To fairly combine SVM with tree-based models (RF, XGBoost, LGBM), standardization was applied only to the SVM model using a pipeline. The final prediction is made by averaging the predicted probabilities (soft voting).\n",
        "\n",
        "ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆRFã€XGBoostã€LGBMï¼‰ã¨SVMã‚’é©åˆ‡ã«çµ±åˆã™ã‚‹ãŸã‚ã€SVMã®ã¿ã«æ¨™æº–åŒ–å‡¦ç†ã‚’æ–½ã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚æœ€çµ‚çš„ãªäºˆæ¸¬ã¯ã€å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡ã®å¹³å‡ï¼ˆã‚½ãƒ•ãƒˆæŠ•ç¥¨ï¼‰ã«ã‚ˆã£ã¦è¡Œã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Feature and Target Setup / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®æº–å‚™\n",
        "# =====================================================\n",
        "# Define numerical and categorical features\n",
        "# æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’å®šç¾©\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# Select features and target from preprocessed DataFrame\n",
        "# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’æŠ½å‡º\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Calibrated LinearSVC Pipeline / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ã SVM ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "# =====================================================\n",
        "# Build a preprocessing pipeline:\n",
        "# æ•°å€¤ç‰¹å¾´é‡ã¯æ¨™æº–åŒ–ã—ã€ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾ä½¿ã†\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),   # Apply standardization to numerical features / æ•°å€¤ç‰¹å¾´é‡ã«æ¨™æº–åŒ–ã‚’é©ç”¨\n",
        "    ('cat', 'passthrough', categorical_features)     # Use categorical features as is / ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾é€šé\n",
        "])\n",
        "\n",
        "# Build a pipeline with Calibrated LinearSVC\n",
        "# CalibratedClassifierCV ã§ SVM ã«ç¢ºç‡å‡ºåŠ›ã‚’ä»˜åŠ ï¼ˆå¿…è¦ï¼šVotingClassifier ã® soft voting å¯¾å¿œï¼‰\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),  # Preprocessing pipeline / å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "    ('svc', CalibratedClassifierCV(      # Calibration wrapper for probability estimates / ç¢ºç‡æ¨å®šç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
        "        LinearSVC(random_state=42, max_iter=5000),  # Base model: LinearSVC / ç·šå½¢ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³\n",
        "        method='sigmoid',                           # Sigmoid calibration / ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "        cv=5                                        # Inner CV for calibration / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å†…éƒ¨äº¤å·®æ¤œè¨¼\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 3. VotingClassifier (Soft Voting) / ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆã‚½ãƒ•ãƒˆæŠ•ç¥¨ï¼‰\n",
        "# =====================================================\n",
        "# Combine pre-trained and optimized models using soft voting\n",
        "# æœ€é©åŒ–æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚½ãƒ•ãƒˆæŠ•ç¥¨ã§çµåˆã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚’æ§‹ç¯‰\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', best_model_rf_sel),             # Random Forest model / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«\n",
        "        ('xgb', best_model_xgb),               # XGBoost model / XGBoost ãƒ¢ãƒ‡ãƒ«\n",
        "        ('lgb', best_model_lgb),               # LightGBM model / LightGBM ãƒ¢ãƒ‡ãƒ«\n",
        "        ('svm', svm_pipeline_calibrated)       # Calibrated SVM pipeline / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿SVM\n",
        "    ],\n",
        "    voting='soft',   # Soft voting: use predicted probabilities / ã‚½ãƒ•ãƒˆæŠ•ç¥¨ï¼ˆç¢ºç‡ã§å¹³å‡åŒ–ï¼‰\n",
        "    n_jobs=-1        # Use all CPU cores / å…¨CPUã‚³ã‚¢ä½¿ç”¨\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define Cross-Validation and Evaluation / äº¤å·®æ¤œè¨¼ã¨è©•ä¾¡é–¢æ•°ã®å®šç¾©\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified 5-fold CV / å±¤åŒ–5åˆ†å‰²äº¤å·®æ¤œè¨¼\n",
        "\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡ã™ã‚‹é–¢æ•°\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()     # Accuracy / ç²¾åº¦\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()       # ROC AUC / AUCã‚¹ã‚³ã‚¢\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 5. Run Evaluation / ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "scores_voting = evaluate_model_cv(voting_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Generate Classification Report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "# =====================================================\n",
        "# Generate predictions using cross-validation / äº¤å·®æ¤œè¨¼ã‚’ç”¨ã„ãŸäºˆæ¸¬ã‚’å–å¾—\n",
        "y_pred_voting = cross_val_predict(voting_clf, X_selected, y_selected, cv=cv)\n",
        "\n",
        "# Create classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ\n",
        "report_voting = classification_report(y_selected, y_pred_voting)\n",
        "\n",
        "# =====================================================\n",
        "# 7. Output Results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ VotingClassifier with Calibrated LinearSVC\")\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_voting['accuracy'])  # å¹³å‡ç²¾åº¦\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_voting['roc_auc'])    # å¹³å‡ROC AUC\n",
        "print(\"ğŸ“ Classification Report (CV predictions):\\n\", report_voting)  # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"
      ],
      "metadata": {
        "id": "UNZZ6QVYot7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCnl4SH8E4Xj"
      },
      "source": [
        "### Voting Ensemble Model Evaluation / Votingã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨è€ƒå¯Ÿ\n",
        "\n",
        "\n",
        "The VotingClassifier with Calibrated LinearSVC achieved strong performance with a mean cross-validated accuracy of approximately 84.2% and a ROC AUC of 0.88. This indicates the model effectively distinguishes between survivors and non-survivors in the Titanic dataset.\n",
        "\n",
        "The classification report shows:\n",
        "- High precision and recall for the majority class (non-survivors), with an F1-score of 0.87.\n",
        "- Reasonably good performance on the minority class (survivors), with a precision of 0.82 and recall of 0.76, resulting in an F1-score of 0.79.\n",
        "\n",
        "Overall, the model balances well between sensitivity and specificity, providing reliable predictions for both classes. This suggests that the ensemble approach combining Random Forest, XGBoost, LightGBM, and calibrated SVM successfully leverages complementary strengths of each model.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "VotingClassifierï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿LinearSVCã‚’å«ã‚€ï¼‰ã¯ã€å¹³å‡äº¤å·®æ¤œè¨¼ç²¾åº¦ç´„84.2ï¼…ã€ROC AUCã¯0.88ã¨é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ã®ç”Ÿå­˜è€…äºˆæ¸¬ã«ãŠã„ã¦ã€è‰¯å¥½ã«ã‚¯ãƒ©ã‚¹ã‚’è­˜åˆ¥ã§ãã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
        "\n",
        "åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã¯ä»¥ä¸‹ãŒèª­ã¿å–ã‚Œã¾ã™ï¼š\n",
        "- éç”Ÿå­˜è€…ï¼ˆå¤šæ•°ã‚¯ãƒ©ã‚¹ï¼‰ã«å¯¾ã—ã¦é«˜ã„ç²¾åº¦ã¨å†ç¾ç‡ã‚’æŒã¡ã€F1ã‚¹ã‚³ã‚¢ã¯0.87ã€‚\n",
        "- ç”Ÿå­˜è€…ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ï¼‰ã«å¯¾ã—ã¦ã‚‚æ¯”è¼ƒçš„è‰¯ã„æ€§èƒ½ã§ã€ç²¾åº¦0.82ã€å†ç¾ç‡0.76ã€F1ã‚¹ã‚³ã‚¢ã¯0.79ã§ã—ãŸã€‚\n",
        "\n",
        "å…¨ä½“ã¨ã—ã¦ã€æ„Ÿåº¦ï¼ˆå†ç¾ç‡ï¼‰ã¨ç‰¹ç•°åº¦ï¼ˆç²¾åº¦ï¼‰ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€ä¸¡ã‚¯ãƒ©ã‚¹ã«å¯¾ã—ã¦ä¿¡é ¼ã§ãã‚‹äºˆæ¸¬ãŒå¯èƒ½ã§ã™ã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Random Forestã€XGBoostã€LightGBMã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿SVMã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã¯ã€ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ã‚’åŠ¹æœçš„ã«æ´»ã‹ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRo8XTHXx02g"
      },
      "source": [
        "### ğŸ“š 5.4.9 Stacking Ensemble (Meta-model: Logistic Regression, passthrough=False) / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼šãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€å…ƒç‰¹å¾´é‡ã¯ä½¿ç”¨ã—ãªã„ï¼‰\n",
        "\n",
        "In this stacking approach, the predictions of base models (Random Forest, XGBoost, LightGBM, SVM with standardization) are used as inputs to a logistic regression meta-model. The original raw features are not passed to the meta-model (passthrough=False), so the meta-model relies purely on the outputs of the base learners. This setup helps focus learning on the interactions among model predictions rather than original feature values.\n",
        "\n",
        "ã“ã®ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã§ã¯ã€å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRFã€XGBoostã€LGBMã€æ¨™æº–åŒ–æ¸ˆã¿SVMï¼‰ã®äºˆæ¸¬çµæœã®ã¿ã‚’ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã€æœ€çµ‚äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚passthrough=False ã«ã‚ˆã‚Šã€ç”Ÿã®ç‰¹å¾´é‡ã¯ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œãšã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®ã¿ã«åŸºã¥ã„ã¦å­¦ç¿’ãŒè¡Œã‚ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ç”Ÿç‰¹å¾´é‡ã®ãƒã‚¤ã‚ºã®å½±éŸ¿ã‚’é¿ã‘ã¤ã¤ã€å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã®é–¢ä¿‚æ€§ã‚’æ´»ã‹ã™æ§‹æˆã§ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / é¸æŠã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable / ç›®çš„å¤‰æ•°ï¼šç”Ÿå­˜ã‹å¦ã‹\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / æ•°å€¤ç‰¹å¾´é‡ï¼ˆæ¨™æº–åŒ–å¯¾è±¡ï¼‰\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / æ®‹ã‚Šã¯ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ã™ã‚‹\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVMç”¨ã®å‰å‡¦ç†ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),   # Standardize numerical features / æ•°å€¤ç‰¹å¾´é‡ã«æ¨™æº–åŒ–ã‚’é©ç”¨\n",
        "    ('cat', 'passthrough', categorical_features)     # Pass categorical features without transformation / ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾é€šã™\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),  # Preprocessing step / å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—\n",
        "    ('svc', CalibratedClassifierCV(      # Calibration wrapper for probability output / ç¢ºç‡å‡ºåŠ›ç”¨ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ãƒƒãƒ‘ãƒ¼\n",
        "        LinearSVC(random_state=42, max_iter=5000),  # Linear SVM model / ç·šå½¢SVMãƒ¢ãƒ‡ãƒ«\n",
        "        method='sigmoid',                           # Sigmoid calibration / ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "        cv=5                                        # Inner CV for calibration / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å†…éƒ¨CV\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ãƒ™ãƒ¼ã‚¹å­¦ç¿’å™¨ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),             # Random Forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ\n",
        "    ('xgb', best_model_xgb),               # XGBoost / XGBoost\n",
        "    ('lgb', best_model_lgb),               # LightGBM / LightGBM\n",
        "    ('svm', svm_pipeline_calibrated)       # Calibrated SVM pipeline / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿SVM\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "final_model = LogisticRegression(max_iter=1000, random_state=42)  # Meta-model: Logistic Regression / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼šãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã®æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified 5-Fold Cross Validation / å±¤åŒ–5åˆ†å‰²äº¤å·®æ¤œè¨¼\n",
        "\n",
        "stacking_LR_clf = StackingClassifier(\n",
        "    estimators=estimators,          # Base models / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤\n",
        "    final_estimator=final_model,    # Meta-model / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«\n",
        "    passthrough=False,              # Do not pass raw features / ç”Ÿã®ç‰¹å¾´é‡ã¯æ¸¡ã•ãªã„\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluation function / è©•ä¾¡ç”¨é–¢æ•°\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model using cross-validation / ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()     # Mean accuracy / å¹³å‡æ­£è§£ç‡\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()       # Mean ROC AUC / å¹³å‡ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / è©•ä¾¡ã®å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "scores_stacking_LR = evaluate_model_cv(stacking_LR_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "# =====================================================\n",
        "y_pred_stacking_LR = cross_val_predict(stacking_LR_clf, X_selected, y_selected, cv=cv)  # Cross-validated predictions / äº¤å·®æ¤œè¨¼äºˆæ¸¬\n",
        "report_stacking_LR = classification_report(y_selected, y_pred_stacking_LR)              # Create report / ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ StackingClassifier with Calibrated LinearSVC (Logistic Regression meta-model)\")\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_LR['accuracy'])   # Print mean accuracy / å¹³å‡æ­£è§£ç‡ã®è¡¨ç¤º\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_LR['roc_auc'])     # Print mean ROC AUC / å¹³å‡ROC AUCã®è¡¨ç¤º\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_LR)          # Print classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"
      ],
      "metadata": {
        "id": "qvyOEjAdZWPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5sH3et3yK_2"
      },
      "source": [
        "### ğŸ” Analysis: Stacking with Logistic Regression (passthrough=False)\n",
        "\n",
        "The stacking ensemble using a logistic regression meta-model and calibrated LinearSVC as one of the base learners achieved a mean cross-validated accuracy of about 84.1% and an ROC AUC of approximately 0.88. This indicates strong overall predictive performance.\n",
        "\n",
        "Looking at the classification report, the model shows slightly better precision and recall for the majority class (label 0), achieving 0.85 precision and 0.90 recall, while for the minority class (label 1), precision is 0.82 and recall is 0.75. This suggests the model is more confident and accurate at identifying non-survivors but still performs well in detecting survivors.\n",
        "\n",
        "The macro-averaged F1 score of 0.83 confirms a balanced performance across classes, making this stacking approach effective in leveraging complementary strengths of different base models.\n",
        "\n",
        "---\n",
        "\n",
        "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãLinearSVCã‚’å«ã‚€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¯ã€å¹³å‡äº¤å·®æ¤œè¨¼ç²¾åº¦ãŒç´„84.1ï¼…ã€ROC AUCãŒç´„0.88ã¨é«˜ã„äºˆæ¸¬æ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚\n",
        "\n",
        "åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’è¦‹ã‚‹ã¨ã€å¤šæ•°ã‚¯ãƒ©ã‚¹ï¼ˆ0ï¼‰ã®ç²¾åº¦ã¨å†ç¾ç‡ã¯ãã‚Œãã‚Œ0.85ã¨0.90ã§ã‚ã‚Šã€å°‘æ•°ã‚¯ãƒ©ã‚¹ï¼ˆ1ï¼‰ã¯ç²¾åº¦0.82ã€å†ç¾ç‡0.75ã¨ãªã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿå­˜ã—ãªã‹ã£ãŸäººï¼ˆã‚¯ãƒ©ã‚¹0ï¼‰ã‚’ã‚ˆã‚Šç¢ºå®Ÿã«è­˜åˆ¥ã§ãã¦ã„ã‚‹ä¸€æ–¹ã§ã€ç”Ÿå­˜è€…ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã‚‚ååˆ†ã«æ¤œå‡ºã§ãã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
        "\n",
        "ãƒã‚¯ãƒ­å¹³å‡ã®F1ã‚¹ã‚³ã‚¢0.83ã¯ã‚¯ãƒ©ã‚¹é–“ã®ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ãŠã‚Šã€å¤šæ§˜ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ã‚’åŠ¹æœçš„ã«çµ„ã¿åˆã‚ã›ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°æ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒç¢ºèªã§ãã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEw9oeXp-Sd_"
      },
      "source": [
        "### ğŸ”§ 5.4.10 Tuned Logistic Regression Meta-Model for Stacking Ensemble / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰\n",
        "\n",
        "This section implements a stacking ensemble model using multiple base learnersâ€”including a calibrated linear SVM (LinearSVC)â€”and tunes a logistic regression model as the meta-learner using GridSearchCV. The base models are pre-trained and already tuned individually, and their predictions are stacked and passed to the logistic regression meta-model.\n",
        "The pipeline includes preprocessing for numerical features and handles categorical features directly. Cross-validation is used both for stacking and evaluation.\n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ã®ç·šå½¢SVMï¼ˆLinearSVCï¼‰ã‚’å«ã‚€è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹å­¦ç¿’å™¨ã‚’ç”¨ã„ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’æ§‹ç¯‰ã—ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ãƒ¡ã‚¿å­¦ç¿’å™¨ã¨ã—ã¦ GridSearchCV ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
        "å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯å€‹åˆ¥ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã§ã€ãã‚Œã‚‰ã®äºˆæ¸¬çµæœã‚’çµ±åˆã—ã¦ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«æ¸¡ã—ã¾ã™ã€‚æ•°å€¤ç‰¹å¾´é‡ã«ã¯å‰å‡¦ç†ï¼ˆæ¨™æº–åŒ–ï¼‰ã‚’æ–½ã—ã€ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾å‡¦ç†ã—ã¾ã™ã€‚è©•ä¾¡ã«ã¯äº¤å·®æ¤œè¨¼ã‚’ç”¨ã„ã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable\n",
        "#    ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features\n",
        "#    æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / æ•°å€¤ç‰¹å¾´é‡ï¼ˆæ¨™æº–åŒ–å¯¾è±¡ï¼‰\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / æ®‹ã‚Šã¯ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ã™ã‚‹\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and calibrated SVM pipeline\n",
        "#    å‰å‡¦ç†ã¨Calibrated SVMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),      # Standardize numerical features / æ•°å€¤ç‰¹å¾´é‡ã®æ¨™æº–åŒ–\n",
        "    ('cat', 'passthrough', categorical_features)        # Pass categorical features / ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾é€šã™\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),                 # Preprocessing step / å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),      # Linear SVC model / ç·šå½¢SVMãƒ¢ãƒ‡ãƒ«\n",
        "        method='sigmoid',                               # Use sigmoid calibration / ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "        cv=5                                             # Use 5-fold CV for calibration / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨CV\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models for stacking\n",
        "#    ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ç”¨ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),             # Random Forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ\n",
        "    ('xgb', best_model_xgb),               # XGBoost / XGBoost\n",
        "    ('lgb', best_model_lgb),               # LightGBM / LightGBM\n",
        "    ('svm', svm_pipeline_calibrated)       # Calibrated LinearSVC / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿LinearSVC\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model\n",
        "#    ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰\n",
        "# =====================================================\n",
        "final_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier\n",
        "#    ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã‚’æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_LR_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model,\n",
        "    passthrough=False,             # Do not use original features / ç”Ÿã®ç‰¹å¾´é‡ã¯ä½¿ã‚ãªã„\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Tune hyperparameters using GridSearchCV\n",
        "#    GridSearchCVã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "# =====================================================\n",
        "param_grid = {\n",
        "    'final_estimator__C': [0.01, 0.1, 1, 10],\n",
        "    'final_estimator__penalty': ['l2'],\n",
        "    'final_estimator__solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "grid_stacking_stacking_LR = GridSearchCV(stacking_LR_clf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "grid_stacking_stacking_LR.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 8. Use best estimator from GridSearchCV\n",
        "#    GridSearchCVã§å¾—ãŸæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
        "# =====================================================\n",
        "best_model_stacking_LR_tuned = grid_stacking_stacking_LR.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 9. Define evaluation function\n",
        "#    è©•ä¾¡ç”¨é–¢æ•°ã‚’å®šç¾©ï¼ˆäº¤å·®æ¤œè¨¼ã§æ€§èƒ½ç¢ºèªï¼‰\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()     # Mean accuracy / å¹³å‡ç²¾åº¦\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()       # Mean ROC AUC / å¹³å‡ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 10. Evaluate the best stacking model\n",
        "#     æœ€è‰¯ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_stacking_LR_tuned = evaluate_model_cv(best_model_stacking_LR_tuned, X_selected, y_selected, cv=cv)\n",
        "\n",
        "y_pred_stacking_LR_tuned = cross_val_predict(best_model_stacking_LR_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_LR_tuned = classification_report(y_selected, y_pred_stacking_LR_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results\n",
        "#     çµæœã‚’å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ“Œ Tuned StackingClassifier with Calibrated LinearSVC (Logistic Regression meta-model)\")\n",
        "print(\"âœ… Best Parameters:\", grid_stacking_stacking_LR.best_params_)\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_LR_tuned['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_LR_tuned['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_LR_tuned)"
      ],
      "metadata": {
        "id": "iANRHPLlmLj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywNvOpgXV9-b"
      },
      "source": [
        "### ğŸ” Model Comparison / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ\n",
        "\n",
        "| Evaluation Metric / è©•ä¾¡æŒ‡æ¨™             | Before Tuning / èª¿æ•´å‰ | After Tuning / èª¿æ•´å¾Œ |\n",
        "| ------------------------------------ | ------------------- | ------------------ |\n",
        "| **Mean CV Accuracy / å¹³å‡ç²¾åº¦**          | 0.8406              | 0.8406             |\n",
        "| **Mean ROC AUC / å¹³å‡ROC AUC**         | 0.8821              | 0.8818             |\n",
        "| **Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰**  | 0.82                | 0.82               |\n",
        "| **Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰**     | 0.75                | 0.74               |\n",
        "| **F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰** | 0.78                | 0.78               |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MijKhYxYHEkv"
      },
      "source": [
        "### ğŸ§  ğŸ” Analysis: Tuned Stacking with Logistic Regression (passthrough=False) / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã€passthrough=Falseï¼‰\n",
        "\n",
        "### Discussion\n",
        "\n",
        "- **Minimal difference in performance:**  \n",
        "  The accuracy and ROC AUC values are almost identical before and after tuning, indicating stable performance.\n",
        "\n",
        "- **Limited impact of hyperparameter tuning:**  \n",
        "  Tuning the logistic regression meta-model's parameters brought only marginal improvements, suggesting the default settings were already effective.\n",
        "\n",
        "- **Base learners are already well-optimized:**  \n",
        "  Since the base models perform strongly, fine-tuning the meta-model yields little gain. Exploring different meta-models or feature engineering may help improve performance further.\n",
        "\n",
        "- **Practical stability:**  \n",
        "  Both models demonstrate robust and reliable results suitable for practical use.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ãƒ¢ãƒ‡ãƒ«  \n",
        "- CVç²¾åº¦ã¨ROC AUCã¯ãã‚Œãã‚Œç´„0.84ã€0.88ã§å®‰å®šã—ã¦ã„ã‚‹ã€‚  \n",
        "- ã‚¯ãƒ©ã‚¹0ã®ãƒªã‚³ãƒ¼ãƒ«ã¯é«˜ãã€ã‚¯ãƒ©ã‚¹1ã¯ã‚„ã‚„ä½ã‚ã§ã‚ã‚‹ã€‚  \n",
        "\n",
        "### 2. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œãƒ¢ãƒ‡ãƒ«  \n",
        "- ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯C=0.1ã®L2æ­£å‰‡åŒ–ã€solverã¯lbfgsã¨æ±ºå®šã€‚  \n",
        "- ç²¾åº¦ãƒ»ROC AUCã¨ã‚‚ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã¨ã»ã¼å¤‰ã‚ã‚‰ãªã„ã€‚  \n",
        "- ã‚¯ãƒ©ã‚¹1ã®ãƒªã‚³ãƒ¼ãƒ«ãŒã‚ãšã‹ã«ä¸‹ãŒã£ãŸãŒã€ã»ã¼åŒç­‰ã®çµæœã€‚\n",
        "\n",
        "### ç·åˆè€ƒå¯Ÿ  \n",
        "- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æ€§èƒ½ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆãšã€ãƒ¢ãƒ‡ãƒ«ã¯å®‰å®šã—ã¦ã„ã‚‹ã€‚  \n",
        "- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒã™ã§ã«è‰¯ããƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®èª¿æ•´åŠ¹æœã¯é™å®šçš„ã€‚  \n",
        "- æ€§èƒ½å‘ä¸Šã‚’ç‹™ã†ã«ã¯ã€åˆ¥ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®å°å…¥ã‚„ç‰¹å¾´é‡æ”¹å–„ã®æ¤œè¨ãŒæœ‰åŠ¹ã¨æ€ã‚ã‚Œã‚‹ã€‚  \n",
        "- å®Ÿå‹™ä¸Šã¯ã©ã¡ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚‚ååˆ†å®Ÿç”¨çš„ã§ä¿¡é ¼ã§ãã‚‹ã€‚\n",
        "\n",
        "ä»¥ä¸Šã‚ˆã‚Šã€ä»Šå›ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯å¤§å¹…ãªæ€§èƒ½æ”¹å–„ã«ã¯ç¹‹ãŒã‚‰ãªã‹ã£ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã‚’å†ç¢ºèªã§ããŸã¨è¨€ãˆã‚‹ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C82LMVbNH87m"
      },
      "source": [
        "###ğŸŒ² 5.4.11 Stacking with Random Forest as Meta-Model / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ç”¨ã„ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°\n",
        "To explore the effect of non-linear meta-models in stacking, we replaced the logistic regression with a Random Forest classifier as the final estimator.\n",
        "This approach allows the stacking model to learn complex interactions among base model predictions, potentially improving classification performance.\n",
        "\n",
        "ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ä»£ã‚ã‚Šã«ã€éç·šå½¢ã®å­¦ç¿’ãŒå¯èƒ½ãªãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚\n",
        "ã“ã®å¤‰æ›´ã«ã‚ˆã‚Šã€å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å‡ºåŠ›ã«å«ã¾ã‚Œã‚‹è¤‡é›‘ãªç›¸äº’ä½œç”¨ã‚’å­¦ç¿’ã—ã€ã‚ˆã‚Šé«˜ã„äºˆæ¸¬æ€§èƒ½ã‚’å¼•ãå‡ºã™ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã—ãŸã€‚  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m76l7resBkJi"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / æ•°å€¤ç‰¹å¾´é‡ï¼ˆæ¨™æº–åŒ–å¯¾è±¡ï¼‰\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / æ®‹ã‚Šã¯ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ã™ã‚‹\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and Calibrated SVM pipeline / SVMç”¨ã®å‰å‡¦ç†ï¼‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (Random Forest) / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰\n",
        "# =====================================================\n",
        "final_model_rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,           # éå­¦ç¿’æŠ‘åˆ¶ã®ãŸã‚é©åº¦ã«åˆ¶é™\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã‚’æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_RF_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_rf,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define evaluation function / è©•ä¾¡é–¢æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / è©•ä¾¡ã®å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "scores_stacking_RF = evaluate_model_cv(stacking_RF_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "# =====================================================\n",
        "y_pred_stacking_RF = cross_val_predict(stacking_RF_clf, X_selected, y_selected, cv=cv)\n",
        "report_stacking_RF = classification_report(y_selected, y_pred_stacking_RF)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸŒ² StackingClassifier with Calibrated LinearSVC (Random Forest meta-model)\")\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_RF['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_RF['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_RF)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Interpretation / è€ƒå¯Ÿ\n",
        "\n",
        "- **Solid Overall Performance**  \n",
        "  The model achieves strong and balanced performance, with high accuracy and a good ROC AUC, indicating reliable discrimination between classes.\n",
        "\n",
        "- **Class 1 (Survived) Detection**  \n",
        "  While slightly behind in recall compared to class 0, the precision (0.80) and F1-score (0.78) for class 1 show that the model handles minority class prediction reasonably well.\n",
        "\n",
        "- **Effectiveness of Random Forest as Meta-Model**  \n",
        "  Using a shallow Random Forest as the meta-learner captures non-linear relationships among base model outputs, without significantly increasing overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "- **å …å®Ÿãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½**  \n",
        "  ç²¾åº¦ãƒ»ROC AUC ã¨ã‚‚ã«é«˜ãã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ã†ã¾ãæ´»ã‹ã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- **ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜è€…ï¼‰ã®è­˜åˆ¥**  \n",
        "  Recallï¼ˆå†ç¾ç‡ï¼‰ã¯ã‚„ã‚„ä½ã‚ï¼ˆ0.77ï¼‰ã§ã™ãŒã€Precisionï¼ˆé©åˆç‡ï¼‰0.80ã€F1ã‚¹ã‚³ã‚¢0.78ã¨ã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸçµæœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- **ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’æ¡ç”¨ã—ãŸåŠ¹æœ**  \n",
        "  Logistic Regression ã‚ˆã‚Šã‚‚æŸ”è»Ÿã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®éç·šå½¢ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã€æµ…ã„æ·±ã•ã§éå­¦ç¿’ã‚‚æŠ‘ãˆã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fx9PDBN0Puvr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVNXsSfZQztX"
      },
      "source": [
        "###ğŸŒ² 5.4.12 Stacking with Tuned Random Forest as Meta-Model / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ç”¨ã„ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°  \n",
        "To enhance the flexibility of the stacking ensemble, we replaced the logistic regression meta-model with a Random Forest and applied hyperparameter tuning. This approach allows the meta-model to learn non-linear relationships among base model outputs, potentially improving performance on complex decision boundaries.\n",
        "\n",
        "ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æŸ”è»Ÿæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’æ¡ç”¨ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›é–“ã«ã‚ã‚‹éç·šå½¢ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã€ã‚ˆã‚Šè¤‡é›‘ãªåˆ¤åˆ¥å¢ƒç•Œã§ã®æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / é¸æŠã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨\n",
        "y_selected = df_fe4['Survived']         # Target variable / ç›®çš„å¤‰æ•°ï¼šç”Ÿå­˜ã‹å¦ã‹\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / æ•°å€¤ç‰¹å¾´é‡ï¼ˆæ¨™æº–åŒ–å¯¾è±¡ï¼‰\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / æ®‹ã‚Šã¯ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ã™ã‚‹\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVMç”¨ã®å‰å‡¦ç†ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),   # Standardize numerical features / æ•°å€¤ç‰¹å¾´é‡ã«æ¨™æº–åŒ–ã‚’é©ç”¨\n",
        "    ('cat', 'passthrough', categorical_features)     # Pass categorical features without transformation / ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¯ãã®ã¾ã¾é€šã™\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),  # Preprocessing step / å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—\n",
        "    ('svc', CalibratedClassifierCV(      # Calibration wrapper for probability output / ç¢ºç‡å‡ºåŠ›ç”¨ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ãƒƒãƒ‘ãƒ¼\n",
        "        LinearSVC(random_state=42, max_iter=5000),  # Linear SVM model / ç·šå½¢SVMãƒ¢ãƒ‡ãƒ«\n",
        "        method='sigmoid',                           # Sigmoid calibration / ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "        cv=5                                        # Inner CV for calibration / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å†…éƒ¨CV\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ãƒ™ãƒ¼ã‚¹å­¦ç¿’å™¨ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),             # Random Forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ\n",
        "    ('xgb', best_model_xgb),               # XGBoost / XGBoost\n",
        "    ('lgb', best_model_lgb),               # LightGBM / LightGBM\n",
        "    ('svm', svm_pipeline_calibrated)       # Calibrated SVM pipeline / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿SVM\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (Random Forest) / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰\n",
        "# =====================================================\n",
        "final_model_rf_tuned = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã®æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified 5-Fold CV / å±¤åŒ–5åˆ†å‰²äº¤å·®æ¤œè¨¼\n",
        "\n",
        "stacking_RF_clf_tuned = StackingClassifier(\n",
        "    estimators=estimators,          # Base models / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤\n",
        "    final_estimator=final_model_rf_tuned,  # Meta-model: Random Forest / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼šãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ\n",
        "    passthrough=False,              # Do not pass raw features / ç”Ÿã®ç‰¹å¾´é‡ã¯æ¸¡ã•ãªã„\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Hyperparameter tuning using GridSearchCV / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "# =====================================================\n",
        "param_grid_rf = {\n",
        "    'final_estimator__n_estimators': [50],   # Number of trees / æœ¨ã®æ•°\n",
        "    'final_estimator__max_depth': [5, None],     # Max depth of trees / æœ¨ã®æ·±ã•åˆ¶é™\n",
        "    'final_estimator__min_samples_split': [5, 10],  # Minimum samples to split node / åˆ†å‰²ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "    'final_estimator__min_samples_leaf': [1, 2]     # Minimum samples at leaf node / è‘‰ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
        "}\n",
        "\n",
        "grid_stacking_RF = GridSearchCV(\n",
        "    stacking_RF_clf_tuned,\n",
        "    param_grid_rf,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_stacking_RF.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 8. Use best estimator from GridSearchCV / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
        "# =====================================================\n",
        "best_model_stacking_RF_tuned = grid_stacking_RF.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 9. Define evaluation function / è©•ä¾¡ç”¨é–¢æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    ãƒ¢ãƒ‡ãƒ«ã‚’äº¤å·®æ¤œè¨¼ã§è©•ä¾¡ã™ã‚‹é–¢æ•°\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()   # å¹³å‡æ­£è§£ç‡\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()     # å¹³å‡ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 10. Evaluate the best tuned stacking model / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_stacking_RF_tuned = evaluate_model_cv(best_model_stacking_RF_tuned, X_selected, y_selected, cv)\n",
        "\n",
        "y_pred_stacking_RF_tuned = cross_val_predict(best_model_stacking_RF_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_RF_tuned = classification_report(y_selected, y_pred_stacking_RF_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results / çµæœã‚’å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸŒ² Tuned StackingClassifier with Calibrated LinearSVC (Random Forest meta-model)\")\n",
        "print(\"âœ… Best Parameters:\", grid_stacking_RF.best_params_)\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_RF_tuned['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_RF_tuned['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_RF_tuned)"
      ],
      "metadata": {
        "id": "qqcImYXgQqr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgN_V0VMVZX3"
      },
      "source": [
        "### ğŸ” Model Comparison / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ\n",
        "\n",
        "| ğŸ§ª Evaluation Metric / è©•ä¾¡æŒ‡æ¨™                   | ğŸ”¹ Before Tuning / èª¿æ•´å‰ | ğŸ”¸ After Tuning / èª¿æ•´å¾Œ |\n",
        "|--------------------------------------------------|---------------------------|--------------------------|\n",
        "| âœ… Mean CV Accuracy / å¹³å‡ç²¾åº¦                   | 0.8384                    | 0.8327                   |\n",
        "| ğŸ“ˆ Mean ROC AUC / å¹³å‡ROC AUC                    | 0.8763                    | 0.8698                   |\n",
        "| ğŸ¯ Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰       | 0.80                      | 0.79                     |\n",
        "| ğŸ” Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰          | 0.77                      | 0.77                     |\n",
        "| ğŸ§® F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰      | 0.78                      | 0.78                     |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š Discussion\n",
        "\n",
        "The results show that tuning the Random Forest meta-model slightly reduced the overall accuracy and ROC AUC, though class-level metrics remained stable. In particular:\n",
        "\n",
        "- **Performance Trade-off**:  \n",
        "  After tuning, accuracy dropped marginally by ~0.0056 and ROC AUC by ~0.0065. This small decrease may result from the constraints introduced (e.g., limiting `max_depth=5`) to improve generalization.\n",
        "\n",
        "- **Class-wise Stability**:  \n",
        "  Precision, recall, and F1-score remained almost identical for both classes. This indicates that tuning didnâ€™t significantly impact how well the model distinguished between survivors and non-survivors.\n",
        "\n",
        "- **Interpretability vs. Performance**:  \n",
        "  The tuned model might generalize better to unseen data, despite slightly lower CV metrics. Simplifying the meta-model (via controlled depth and split criteria) enhances interpretability and reduces overfitting risk.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Summary\n",
        "\n",
        "While tuning led to a minor decrease in performance metrics, the modelâ€™s stability across classes and potential for improved generalization make the trade-off acceptableâ€”especially when interpretability and robustness are priorities.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š è€ƒå¯Ÿ\n",
        "\n",
        "çµæœã‹ã‚‰è¦‹ã‚‹ã¨ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã“ã¨ã§ã€å…¨ä½“ã®æ­£è§£ç‡ãŠã‚ˆã³ROC AUCãŒã‚„ã‚„ä½ä¸‹ã—ã¾ã—ãŸãŒã€ã‚¯ãƒ©ã‚¹ã”ã¨ã®æŒ‡æ¨™ã«ã¯å¤§ããªå¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n",
        "\n",
        "- **æ€§èƒ½ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**ï¼š  \n",
        "  ç²¾åº¦ã¯ç´„0.0056ã€ROC AUCã¯ç´„0.0065ä½ä¸‹ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€`max_depth=5`ãªã©ã®åˆ¶é™ã«ã‚ˆã£ã¦éå­¦ç¿’ã‚’é˜²ãã€æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸã“ã¨ã«ã‚ˆã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n",
        "- **ã‚¯ãƒ©ã‚¹å˜ä½ã§ã®å®‰å®šæ€§**ï¼š  \n",
        "  ã‚¯ãƒ©ã‚¹0ãƒ»1ãã‚Œãã‚Œã®F1ã‚¹ã‚³ã‚¢ã¯å¤‰ã‚ã‚‰ãšï¼ˆ0.87 / 0.78ï¼‰ã€åˆ†é¡æ€§èƒ½è‡ªä½“ã«å¤§ããªå¤‰å‹•ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n",
        "\n",
        "- **è§£é‡ˆæ€§ã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹**ï¼š  \n",
        "  æœ¨ã®æ·±ã•ãªã©ã‚’åˆ¶é™ã—ãŸã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ãŒå‘ä¸Šã—ã€æœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹é ‘å¥æ€§ã‚‚æœŸå¾…ã§ãã¾ã™ã€‚ã‚ãšã‹ãªç²¾åº¦ã®ä½ä¸‹ã‚’å—ã‘å…¥ã‚Œã¦ã§ã‚‚ã€éå­¦ç¿’ã‚’æŠ‘ãˆã‚‹ãƒ¡ãƒªãƒƒãƒˆã¯å¤§ãã„ã§ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… ç·æ‹¬\n",
        "\n",
        "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯å¾®æ¸›ã—ãŸã‚‚ã®ã®ã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã¨æ±åŒ–æ€§ã®å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã€å®Ÿå‹™çš„ã«ã¯æœ‰åŠ¹ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã„ãˆã¾ã™ã€‚ç‰¹ã«ã€è§£é‡ˆæ€§ã‚„éå­¦ç¿’å¯¾ç­–ã‚’é‡è¦–ã™ã‚‹å ´åˆã«ã¯ã€æœ‰åŠ›ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ãªã‚‹ã§ã—ã‚‡ã†ã€‚"
      ],
      "metadata": {
        "id": "oYoMnqv_Xnw1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96WuQSw5wZq"
      },
      "source": [
        "\n",
        "### ğŸŒŸ 5.4.13 Stacking with XGBoost Meta-Model (Default Settings) / XGBoost ã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ç”¨ã„ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰  \n",
        "\n",
        "To explore the effect of a powerful non-linear meta-model, we replaced the logistic regression with XGBoost in the stacking ensemble. This setup aims to capture complex interactions among the base model outputs without any hyperparameter tuning.\n",
        "\n",
        "å¼·åŠ›ãªéç·šå½¢ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœã‚’ç¢ºèªã™ã‚‹ãŸã‚ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ä»£ã‚ã‚Šã« XGBoost ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ã“ã®æ§‹æˆã§ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã‚’è¡Œã‚ãšã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›é–“ã«å­˜åœ¨ã™ã‚‹è¤‡é›‘ãªç›¸äº’ä½œç”¨ã®å­¦ç¿’ã‚’ç‹™ã„ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak6AHMdA5c_Z"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and Calibrated SVM pipeline / SVMç”¨ã®å‰å‡¦ç†ï¼‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (XGBoost) / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ï¼ˆXGBoostï¼‰\n",
        "# =====================================================\n",
        "final_model_xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã‚’æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_XGB_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_xgb,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define evaluation function / è©•ä¾¡é–¢æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / è©•ä¾¡ã®å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "scores_stacking_XGB = evaluate_model_cv(stacking_XGB_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "# =====================================================\n",
        "y_pred_stacking_XGB = cross_val_predict(stacking_XGB_clf, X_selected, y_selected, cv=cv)\n",
        "report_stacking_XGB = classification_report(y_selected, y_pred_stacking_XGB)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"âš¡ StackingClassifier with Calibrated LinearSVC (XGBoost meta-model)\")\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_XGB['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_XGB['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_XGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insight (Pre-Tuning Stage) / è€ƒå¯Ÿï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ï¼‰\n",
        "\n",
        "The StackingClassifier using XGBoost as a meta-model showed comparable results to those using logistic regression or random forest in terms of precision (0.79) and F1-score (0.77).\n",
        "However, its recall (0.76) was slightly lower.\n",
        "\n",
        "This means the model is more conservative when predicting positive cases (i.e., predicting someone survived). It avoids making incorrect positive predictions (i.e., calling someone \"survived\" when they didn't), which helps reduce false positives.\n",
        "But as a result, it ends up missing some actual survivors, increasing the number of false negatives.\n",
        "\n",
        "In practical terms, the model is careful when saying \"this person survived\", but might miss some who actually did.\n",
        "\n",
        "---\n",
        "\n",
        "XGBoost ã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã¯ã€**é©åˆç‡ï¼ˆ0.79ï¼‰ã‚„F1ã‚¹ã‚³ã‚¢ï¼ˆ0.77ï¼‰**ã§ã¯ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚„ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚\n",
        "ã—ã‹ã—ã€**å†ç¾ç‡ï¼ˆ0.76ï¼‰**ãŒã‚ãšã‹ã«ä½ä¸‹ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã€Œã“ã®äººã¯ç”Ÿå­˜ã—ãŸã€ã¨åˆ¤æ–­ã™ã‚‹éš›ã«ã€ã‚ˆã‚Šæ…é‡ãªå§¿å‹¢ã‚’å–ã£ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€èª¤ã£ã¦ã€Œç”Ÿå­˜ã€ã¨äºˆæ¸¬ã™ã‚‹ï¼ˆå½é™½æ€§ï¼‰ã“ã¨ã‚’é¿ã‘ã‚‹ä¸€æ–¹ã§ã€å®Ÿéš›ã«ã¯ç”Ÿå­˜ã—ã¦ã„ãŸäººã‚’ã€Œæ­»äº¡ã€ã¨äºˆæ¸¬ã—ã¦ã—ã¾ã†ï¼ˆå½é™°æ€§ï¼‰ã‚±ãƒ¼ã‚¹ãŒå¢—ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "å®Ÿéš›ã«ã¯ç”Ÿãå»¶ã³ãŸäººã‚’ä¸€éƒ¨è¦‹é€ƒã—ã¦ã—ã¾ã†ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã‚‚ã®ã®ã€ã€Œç”Ÿå­˜ã€ã¨äºˆæ¸¬ã—ãŸäººã®ç²¾åº¦ã¯é«˜ã„ã€ã¨ã„ã†ä¿å®ˆçš„ãªäºˆæ¸¬ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒã£ã¦ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "urUFOtp0sQdm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX0OI4WY6bnH"
      },
      "source": [
        "### ğŸ§ª 5.4.14 Stacking with Tuned XGBoost Meta-Model / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆXGBoostãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãï¼‰  \n",
        "This section demonstrates how to build a stacking ensemble classifier using multiple base learners and an XGBoost model as the meta-model. The XGBoost meta-model undergoes hyperparameter tuning to optimize its performance. The tuning process leverages cross-validation to find the best combination of parameters, improving the overall predictive accuracy and robustness of the stacking ensemble.  \n",
        "  \n",
        "æœ¬ç¯€ã§ã¯ã€è¤‡æ•°ã®ãƒ™ãƒ¼ã‚¹å­¦ç¿’å™¨ã‚’çµ„ã¿åˆã‚ã›ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«XGBoostã‚’ç”¨ã„ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã®æ§‹ç¯‰æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚XGBoostã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ã¦æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’æ¢ç´¢ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°å…¨ä½“ã®äºˆæ¸¬ç²¾åº¦ã¨ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w32KjlUj6hZf"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVMç”¨ã®å‰å‡¦ç†ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (XGBoost) / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«XGBoostã‚’è¨­å®š\n",
        "# =====================================================\n",
        "final_model_xgb_tuned = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',  # è­¦å‘Šå›é¿ã®ãŸã‚ã«è¨­å®š\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã‚’æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_XGB_clf_tuned = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_xgb_tuned,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define hyperparameter grid for XGBoost meta-model / ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰\n",
        "# =====================================================\n",
        "param_grid_xgb = {\n",
        "    'final_estimator__n_estimators': [50, 100],\n",
        "    'final_estimator__max_depth': [3, 5],\n",
        "    'final_estimator__learning_rate': [0.01, 0.1],\n",
        "    'final_estimator__subsample': [0.7, 1.0],\n",
        "    'final_estimator__colsample_bytree': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run GridSearchCV for tuning / GridSearchCVã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "grid_stacking_XGB = GridSearchCV(\n",
        "    stacking_XGB_clf_tuned,\n",
        "    param_grid_xgb,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_stacking_XGB.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Get best estimator from tuning / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
        "# =====================================================\n",
        "best_model_stacking_XGB_tuned = grid_stacking_XGB.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 10. Define evaluation function / è©•ä¾¡é–¢æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 11. Evaluate tuned model / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_stacking_XGB_tuned = evaluate_model_cv(best_model_stacking_XGB_tuned, X_selected, y_selected, cv)\n",
        "\n",
        "y_pred_stacking_XGB_tuned = cross_val_predict(best_model_stacking_XGB_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_XGB_tuned = classification_report(y_selected, y_pred_stacking_XGB_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 12. Output results / çµæœã‚’å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸŒ² Tuned StackingClassifier with Calibrated LinearSVC (XGBoost meta-model)\")\n",
        "print(\"âœ… Best Parameters:\", grid_stacking_XGB.best_params_)\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_XGB_tuned['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_XGB_tuned['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_XGB_tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ” Model Comparison / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ\n",
        "\n",
        "| ğŸ§ª Evaluation Metric / è©•ä¾¡æŒ‡æ¨™         | ğŸ”¹ Before Tuning / èª¿æ•´å‰ | ğŸ”¸ After Tuning / èª¿æ•´å¾Œ |\n",
        "| ----------------------------------- | ---------------------- | --------------------- |\n",
        "| âœ… Mean CV Accuracy / å¹³å‡ç²¾åº¦           | 0.829                  | 0.836                 |\n",
        "| ğŸ“ˆ Mean ROC AUC / å¹³å‡ROC AUC         | 0.872                  | 0.874                 |\n",
        "| ğŸ¯ Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰  | 0.79                   | 0.82                  |\n",
        "| ğŸ” Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰     | 0.76                   | 0.73                  |\n",
        "| ğŸ§® F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ | 0.77                   | 0.77                  |\n",
        "            |\n"
      ],
      "metadata": {
        "id": "7oKAKFx8xTpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance After Tuning / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½\n",
        "\n",
        "After tuning, the XGBoost meta-model achieved slightly higher accuracy and ROC AUC, along with improved precision for the positive class (0.82).\n",
        "However, recall dropped from 0.76 to 0.73, meaning the model became more conservative in predicting survivorsâ€”it makes fewer false positives, but misses more actual survivors.\n",
        "\n",
        "This reflects a trade-off:\n",
        "\n",
        "ğŸ”¼ Better precision (more correct \"survived\" predictions)\n",
        "\n",
        "ğŸ”½ Lower recall (more missed survivors)\n",
        "\n",
        "Thus, the model is more cautious in predicting survival, which may be preferable if the cost of a false positive (incorrectly predicting survival) is higher.\n",
        "\n",
        "---\n",
        "ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®XGBoostãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¯ã€ç²¾åº¦ã¨ROC AUCãŒã‚ãšã‹ã«å‘ä¸Šã—ã€ã‚¯ãƒ©ã‚¹1ï¼ˆç”Ÿå­˜è€…ï¼‰ã®é©åˆç‡ã‚‚ 0.79 â†’ 0.82 ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸã€‚\n",
        "ä¸€æ–¹ã§ã€å†ç¾ç‡ã¯ 0.76 â†’ 0.73 ã«ä½ä¸‹ã—ã€ç”Ÿå­˜è€…ã‚’ã€Œæ­»äº¡ã€ã¨èª¤åˆ†é¡ã™ã‚‹å‰²åˆãŒã‚„ã‚„å¢—åŠ ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã€Œç”Ÿå­˜ã€ã¨åˆ¤æ–­ã™ã‚‹éš›ã«ã‚ˆã‚Šæ…é‡ã«ãªã‚Šã€å½é™½æ€§ï¼ˆèª¤ã£ã¦ç”Ÿå­˜ã¨åˆ¤æ–­ï¼‰ã‚’æ¸›ã‚‰ã™ä¸€æ–¹ã§ã€å½é™°æ€§ï¼ˆå®Ÿéš›ã«ã¯ç”Ÿå­˜ã—ã¦ã„ãŸäººã‚’è¦‹é€ƒã™ï¼‰ãŒå¢—ãˆã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
        "\n",
        "ã¤ã¾ã‚Šã€æ¬¡ã®ã‚ˆã†ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒè¦‹ã‚‰ã‚Œã¾ã™ï¼š\n",
        "\n",
        "ğŸ”¼ é©åˆç‡ã®å‘ä¸Šï¼ˆã€Œç”Ÿå­˜ã€ã¨äºˆæ¸¬ã—ãŸäººã®æ­£ç¢ºæ€§ãŒé«˜ã„ï¼‰\n",
        "\n",
        "ğŸ”½ å†ç¾ç‡ã®ä½ä¸‹ï¼ˆå®Ÿéš›ã«ç”Ÿå­˜ã—ãŸäººã®è¦‹é€ƒã—ãŒå¢—ãˆã‚‹ï¼‰\n",
        "\n",
        "ã“ã‚Œã¯ã€èª¤ã£ã¦ã€Œç”Ÿå­˜ã€ã¨äºˆæ¸¬ã™ã‚‹ãƒªã‚¹ã‚¯ã‚’æ¸›ã‚‰ã—ãŸã„ã‚±ãƒ¼ã‚¹ã§ã¯æœ‰åŠ¹ã§ã™ãŒã€ã™ã¹ã¦ã®ç”Ÿå­˜è€…ã‚’ç¢ºå®Ÿã«è¦‹ã¤ã‘ãŸã„ã‚±ãƒ¼ã‚¹ã§ã¯å†ç¾ç‡ã®ä½ä¸‹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚\n"
      ],
      "metadata": {
        "id": "2VrgkUs6yY7a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAohKwVf9MQV"
      },
      "source": [
        "### ğŸ”†5.4.15 Stacking with LightGBM as Meta-Model / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã« LightGBM ã‚’ç”¨ã„ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°  \n",
        "To assess the performance of a more expressive meta-model, we constructed a stacking classifier using LightGBM as the final estimator. LightGBM is a gradient boosting framework that can capture complex non-linear relationships in the base model predictions. This approach is aimed at leveraging the strengths of both ensemble methods and boosting for improved generalization.\n",
        "\n",
        "ã‚ˆã‚Šè¡¨ç¾åŠ›ã®é«˜ã„ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã®ãƒ¡ã‚¿æ¨å®šå™¨ã¨ã—ã¦ LightGBM ã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚LightGBM ã¯å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã«åŸºã¥ããƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã«å«ã¾ã‚Œã‚‹è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¨ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®ä¸¡æ–¹ã®å¼·ã¿ã‚’æ´»ã‹ã—ã€ã‚ˆã‚Šé«˜ã„æ±åŒ–æ€§èƒ½ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eygtYnNp87Qk"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and Calibrated SVM pipeline / SVMç”¨ã®å‰å‡¦ç†ï¼‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "# â€» best_model_rf_sel, best_model_xgb, best_model_lgb ã¯æ—¢ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã¨ã—ã¾ã™\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (LightGBM) / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ï¼ˆLightGBMï¼‰\n",
        "# =====================================================\n",
        "final_model_lgb = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã‚’æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_LGB_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_lgb,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define evaluation function / è©•ä¾¡é–¢æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / è©•ä¾¡ã®å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "scores_stacking_LGB = evaluate_model_cv(stacking_LGB_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ\n",
        "# =====================================================\n",
        "y_pred_stacking_LGB = cross_val_predict(stacking_LGB_clf, X_selected, y_selected, cv=cv)\n",
        "report_stacking_LGB = classification_report(y_selected, y_pred_stacking_LGB)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / çµæœã®å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸ’¡ StackingClassifier with Calibrated LinearSVC (LightGBM meta-model)\")\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_LGB['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_LGB['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_LGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo5iKl1q-Yn_"
      },
      "source": [
        "### Default LightGBM Stacking Model Results / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆLightGBMã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®çµæœ\n",
        "\n",
        "The LightGBM meta-model achieved decent performance (CV Accuracy: 0.8249, ROC AUC: 0.8668). However, compared to other meta-models like Random Forest and XGBoost, it showed slightly lower recall for the positive class (0.75). This means that the model was slightly more conservative in predicting survivors, possibly leading to more false negatives.\n",
        "\n",
        "Despite having the same precision (0.79) and F1-score (0.77) as XGBoost before tuning, its lower recall indicates it may miss some true survivors. Therefore, while LightGBM is computationally efficient, it might not be the best choice if the goal is to minimize missed survivors.\n",
        "\n",
        "---\n",
        "\n",
        "LightGBM ã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ä½¿ç”¨ã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã¯ã€ã¾ãšã¾ãšã®æ€§èƒ½ï¼ˆå¹³å‡ç²¾åº¦ 0.8249ã€ROC AUC 0.8668ï¼‰ã‚’ç¤ºã—ã¾ã—ãŸã€‚ãŸã ã—ã€ä»–ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆç‰¹ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚„XGBoostï¼‰ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€é™½æ€§ï¼ˆç”Ÿå­˜ï¼‰ã‚¯ãƒ©ã‚¹ã®å†ç¾ç‡ï¼ˆ0.75ï¼‰ãŒã‚„ã‚„ä½ããªã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã¯ã€LightGBMãŒç”Ÿå­˜è€…ã‚’äºˆæ¸¬ã™ã‚‹éš›ã«ã‚„ã‚„æ…é‡ãªå‚¾å‘ã‚’æŒã¡ã€å½é™½æ€§ã‚’æŠ‘ãˆã¤ã¤ã‚‚ã€çœŸã®ç”Ÿå­˜è€…ã‚’è¦‹é€ƒã™å¯èƒ½æ€§ï¼ˆå½é™°æ€§ã®å¢—åŠ ï¼‰ãŒã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ç²¾åº¦ã‚„F1ã‚¹ã‚³ã‚¢ã¯XGBoostï¼ˆèª¿æ•´å‰ï¼‰ã¨åŒç­‰ã§ã™ãŒã€å†ç¾ç‡ãŒä½ã„ãŸã‚ã€ã€Œç”Ÿå­˜è€…ã®è¦‹é€ƒã—ã‚’æ¸›ã‚‰ã™ã€ã“ã¨ãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã¯ã€æœ€é©ãªé¸æŠã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCgsLnwB_D8-"
      },
      "source": [
        "### 5.4.16 Tuned LightGBM Meta-Model for Stacking Ensemble / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®LightGBMãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰\n",
        "\n",
        "This section focuses on the implementation and evaluation of the LightGBM meta-model used as the final estimator in the stacking ensemble after hyperparameter tuning. By optimizing the LightGBM parameters, we aim to enhance the overall predictive performance of the ensemble, particularly improving metrics like accuracy and ROC AUC.\n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã‚’çµŒãŸLightGBMãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®æœ€çµ‚æ¨å®šå™¨ã¨ã—ã¦å®Ÿè£…ãƒ»è©•ä¾¡ã—ã¾ã™ã€‚LightGBMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã‚„ROC AUCãªã©ã®æŒ‡æ¨™å‘ä¸Šã‚’ç›®æŒ‡ã—ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å…¨ä½“ã®äºˆæ¸¬æ€§èƒ½ã‚’é«˜ã‚ã¾ã™ã€‚  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT0zNRWG-Hpd"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é¡\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVMç”¨ã®å‰å‡¦ç†ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (LightGBM) / ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«LightGBMã‚’è¨­å®š\n",
        "# =====================================================\n",
        "final_model_lgb = LGBMClassifier(\n",
        "    objective='binary',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°åˆ†é¡å™¨ã‚’æ§‹ç¯‰\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_LGB_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_lgb,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define hyperparameter grid for LightGBM meta-model / LightGBMã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰\n",
        "# =====================================================\n",
        "param_grid_lgb = {\n",
        "    'final_estimator__n_estimators': [100],\n",
        "    'final_estimator__learning_rate': [0.1],\n",
        "    'final_estimator__max_depth': [3, 5],\n",
        "    'final_estimator__num_leaves': [15, 31],\n",
        "    'final_estimator__subsample': [0.7],\n",
        "    'final_estimator__colsample_bytree': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run GridSearchCV for tuning / GridSearchCVã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "grid_stacking_LGB = GridSearchCV(\n",
        "    stacking_LGB_clf,\n",
        "    param_grid_lgb,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_stacking_LGB.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Get best estimator from tuning / æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
        "# =====================================================\n",
        "best_model_stacking_LGB_tuned = grid_stacking_LGB.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 10. Define evaluation function / è©•ä¾¡é–¢æ•°ã‚’å®šç¾©\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 11. Evaluate tuned model / ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡\n",
        "# =====================================================\n",
        "scores_stacking_LGB_tuned = evaluate_model_cv(best_model_stacking_LGB_tuned, X_selected, y_selected, cv)\n",
        "\n",
        "y_pred_stacking_LGB_tuned = cross_val_predict(best_model_stacking_LGB_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_LGB_tuned = classification_report(y_selected, y_pred_stacking_LGB_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 12. Output results / çµæœã‚’å‡ºåŠ›\n",
        "# =====================================================\n",
        "print(\"ğŸŒ¿ Tuned StackingClassifier with Calibrated LinearSVC (LightGBM meta-model)\")\n",
        "print(\"âœ… Best Parameters:\", grid_stacking_LGB.best_params_)\n",
        "print(\"ğŸ“ˆ CV Accuracy (mean):\", scores_stacking_LGB_tuned['accuracy'])\n",
        "print(\"ğŸ“ˆ CV ROC AUC (mean):\", scores_stacking_LGB_tuned['roc_auc'])\n",
        "print(\"ğŸ“ Classification Report:\\n\", report_stacking_LGB_tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ” Model Comparison / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ\n",
        "\n",
        "| ğŸ§ª Evaluation Metric / è©•ä¾¡æŒ‡æ¨™         | ğŸ’¡ Before Tuning / èª¿æ•´å‰ | ğŸŒ¿ After Tuning / èª¿æ•´å¾Œ |\n",
        "| ----------------------------------- | ---------------------- | --------------------- |\n",
        "| âœ… Mean CV Accuracy / å¹³å‡ç²¾åº¦           | 0.8249                 | 0.8249                |\n",
        "| ğŸ“ˆ Mean ROC AUC / å¹³å‡ROC AUC         | 0.8668                 | 0.8668                |\n",
        "| ğŸ¯ Precision (Class 1) / é©åˆç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰  | 0.79                   | 0.79                  |\n",
        "| ğŸ” Recall (Class 1) / å†ç¾ç‡ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰     | 0.75                   | 0.75                  |\n",
        "| ğŸ§® F1-score (Class 1) / F1ã‚¹ã‚³ã‚¢ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ | 0.77                   | 0.77                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "7O84F_QX8f-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“ Interpretation / è€ƒå¯Ÿ\n",
        "\n",
        "There was no change in performance after tuning the LightGBM meta-model. All key metrics â€” accuracy, ROC AUC, precision, recall, and F1-score â€” remained the same. This suggests that either:\n",
        "\n",
        "The initial hyperparameters were already near-optimal, or\n",
        "\n",
        "The tuning grid did not explore a significantly better combination.\n",
        "\n",
        "---\n",
        "\n",
        "LightGBM ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã‚‚ã€ã™ã¹ã¦ã®è©•ä¾¡æŒ‡æ¨™ã«å¤‰åŒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ç²¾åº¦ãƒ»ROC AUCãƒ»é©åˆç‡ãƒ»å†ç¾ç‡ãƒ»F1ã‚¹ã‚³ã‚¢ã¯ã„ãšã‚Œã‚‚ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã¨åŒã˜ã§ã™ã€‚\n",
        "ã“ã®ã“ã¨ã¯ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ï¼š\n",
        "\n",
        "åˆæœŸã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã™ã§ã«ã»ã¼æœ€é©ã ã£ãŸ\n",
        "\n",
        "ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®éš›ã«ä½¿ç”¨ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ãŒä¸ååˆ†ã ã£ãŸ\n"
      ],
      "metadata": {
        "id": "IFZ4aQf49qRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 ğŸ“Š Evaluation & Comparison / ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨æ¯”è¼ƒ\n",
        "In this section, I evaluate multiple models using cross-validation.\n",
        "Key metrics such as accuracy, F1 score, ROC AUC, and training/prediction time are measured.\n",
        "Additionally, I compare the cross-validation results with Kaggle leaderboard scores to assess generalization.\n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€äº¤å·®æ¤œè¨¼ã‚’ç”¨ã„ã¦è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
        "è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ æ­£è§£ç‡ï¼ˆAccuracyï¼‰ã€F1ã‚¹ã‚³ã‚¢ã€ROC AUC ã«åŠ ãˆã€å­¦ç¿’ãƒ»æ¨è«–æ™‚é–“ã‚‚æ¸¬å®šã—ã¾ã™ã€‚\n",
        "ã¾ãŸã€äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ã¨Kaggleæå‡ºæ™‚ã®ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã«ã¤ã„ã¦ã‚‚æ¤œè¨ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "7vp4jp2Xlc2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1  âœ… Model Evaluation Summary / ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚µãƒãƒªãƒ¼\n",
        "\n",
        "ä»¥ä¸‹ã®è¡¨ã¯ã€è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã€ç²¾åº¦ï¼ˆAccuracyï¼‰ã€F1ã‚¹ã‚³ã‚¢ã€ROC AUCã‚¹ã‚³ã‚¢ã€å­¦ç¿’æ™‚é–“ãŠã‚ˆã³æ¨è«–æ™‚é–“ã‚’æ¯”è¼ƒã—ãŸã‚‚ã®ã§ã™ã€‚  \n",
        "å„ãƒ¢ãƒ‡ãƒ«ã¯5åˆ†å‰²ã®å±¤åŒ–äº¤å·®æ¤œè¨¼(Stratified K-Fold CV)ã‚’ç”¨ã„ã¦è©•ä¾¡ã•ã‚Œã¦ãŠã‚Šã€  \n",
        "åˆ†é¡æ€§èƒ½ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã®ä¸¡é¢ã‹ã‚‰ç·åˆçš„ãªæ¯”è¼ƒã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "This table summarizes the evaluation of multiple machine learning models in terms of accuracy, F1 score, ROC AUC, training time, and prediction time.  \n",
        "All models were assessed using 5-fold Stratified Cross-Validation, allowing a comprehensive comparison of classification performance and computational cost.\n"
      ],
      "metadata": {
        "id": "x2IGAj3mDZtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Function to evaluate model and measure time\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æ™‚é–“æ¸¬å®šã®ãŸã‚ã®é–¢æ•°\n",
        "# ===============================================\n",
        "def evaluate_model_with_timing(model, X, y, cv, model_name=\"model\"):\n",
        "    result = {}  # Dictionary to store results / çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸\n",
        "    result['Model'] = model_name  # Store model name / ãƒ¢ãƒ‡ãƒ«åã‚’æ ¼ç´\n",
        "\n",
        "    # Measure training time (explicitly fit the model) / å­¦ç¿’æ™‚é–“ï¼ˆæ˜ç¤ºçš„ã«fitï¼‰\n",
        "    start_train = time.time()\n",
        "    model.fit(X, y)\n",
        "    end_train = time.time()\n",
        "    result['Train Time (s)'] = end_train - start_train  # Calculate training time / å­¦ç¿’æ™‚é–“ã‚’è¨ˆç®—\n",
        "\n",
        "    # Measure prediction time (using cross-validation) / æ¨è«–æ™‚é–“ï¼ˆcross_val_predictï¼‰\n",
        "    start_pred = time.time()\n",
        "    y_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)  # Prediction (with cross-validation) / æ¨è«–ï¼ˆäºˆæ¸¬ï¼‰\n",
        "    end_pred = time.time()\n",
        "    result['Predict Time (s)'] = end_pred - start_pred  # Calculate prediction time / æ¨è«–æ™‚é–“ã‚’è¨ˆç®—\n",
        "\n",
        "    # Evaluation metrics / è©•ä¾¡æŒ‡æ¨™\n",
        "    result['Accuracy'] = accuracy_score(y, y_pred)  # Accuracy / æ­£è§£ç‡\n",
        "    result['F1 Score'] = f1_score(y, y_pred)  # F1 Score / F1ã‚¹ã‚³ã‚¢\n",
        "    result['ROC AUC'] = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1).mean()  # ROC AUC\n",
        "    result['Classification Report'] = classification_report(y, y_pred, output_dict=False)  # Classification report / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
        "\n",
        "    return result  # Return the result / çµæœã‚’è¿”ã™\n",
        "\n",
        "# ===============================================\n",
        "# Define models to compare\n",
        "# æ¯”è¼ƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\n",
        "# ===============================================\n",
        "models = {\n",
        "    'Random Forest': best_model_rf_sel,  # Random Forest / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ\n",
        "    'XGBoost': best_model_xgb,  # XGBoost / XGBoost\n",
        "    'LightGBM': best_model_lgb,  # LightGBM / LightGBM\n",
        "    'Calibrated LinearSVC': svm_pipeline_calibrated,  # Calibrated LinearSVC / ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸLinearSVC\n",
        "    'Voting': voting_clf,  # Voting Classifier (ensemble) / ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆæŠ•ç¥¨ï¼‰\n",
        "    'Stacking (LR meta)': best_model_stacking_LR_tuned,  # Stacking (LR meta) / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆLRãƒ¡ã‚¿ï¼‰\n",
        "    'Stacking (RF meta)': best_model_stacking_RF_tuned,  # Stacking (RF meta) / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆRFãƒ¡ã‚¿ï¼‰\n",
        "    'Stacking (XGB meta)': best_model_stacking_XGB_tuned,  # Stacking (XGB meta) / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆXGBãƒ¡ã‚¿ï¼‰\n",
        "    'Stacking (LGB meta)': best_model_stacking_LGB_tuned  # Stacking (LGB meta) / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆLGBãƒ¡ã‚¿ï¼‰\n",
        "}\n",
        "\n",
        "# ===============================================\n",
        "# Stratified KFold cross-validation setup and result storage\n",
        "# Stratified KFoldäº¤å·®æ¤œè¨¼ã®è¨­å®šã¨çµæœã®æ ¼ç´\n",
        "# ===============================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # StratifiedKFold (å±¤åŒ–äº¤å·®æ¤œè¨¼) / StratifiedKFold cross-validation\n",
        "results = []  # List to store results / çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
        "\n",
        "# ===============================================\n",
        "# Evaluate all models\n",
        "# å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n",
        "# ===============================================\n",
        "for name, model in models.items():\n",
        "    print(f\"ğŸ” Evaluating: {name}\")  # Print the current model being evaluated / è©•ä¾¡ä¸­ã®ãƒ¢ãƒ‡ãƒ«åã‚’è¡¨ç¤º\n",
        "    result = evaluate_model_with_timing(model, X_selected, y_selected, cv, model_name=name)  # Call the evaluation function / è©•ä¾¡é–¢æ•°ã‚’å‘¼ã³å‡ºã—\n",
        "    results.append(result)  # Append result to the list / çµæœã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
        "\n",
        "# ===============================================\n",
        "# Convert results to DataFrame and sort by accuracy\n",
        "# çµæœã‚’DataFrameã«å¤‰æ›ã—ã€æ­£è§£ç‡ã§ã‚½ãƒ¼ãƒˆ\n",
        "# ===============================================\n",
        "df_results = pd.DataFrame(results)  # Convert to DataFrame / DataFrameã«å¤‰æ›\n",
        "df_results_sorted = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)  # Sort by accuracy in descending order / æ­£è§£ç‡é †ã«ä¸¦ã³æ›¿ãˆ\n",
        "\n",
        "# ===============================================\n",
        "# Display results\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "# ===============================================\n",
        "print(\"\\nâœ… ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨ï¼ˆç²¾åº¦é †ï¼‰:\")  # Print the results / çµæœã‚’è¡¨ç¤º\n",
        "print(df_results_sorted[['Model', 'Accuracy', 'F1 Score', 'ROC AUC', 'Train Time (s)', 'Predict Time (s)']])  # Display evaluation metrics and time / è©•ä¾¡æŒ‡æ¨™ã¨æ™‚é–“ã‚’è¡¨ç¤º"
      ],
      "metadata": {
        "id": "NyaGay494kC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison Results / ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ\n",
        "\n",
        "The table below compares various models based on their **Accuracy**, **F1 Score**, **ROC AUC**, **Training Time**, and **Prediction Time**. When selecting models, it is important to consider these metrics holistically.\n",
        "\n",
        "ä»¥ä¸‹ã®è¡¨ã¯ã€å„ãƒ¢ãƒ‡ãƒ«ã®**ç²¾åº¦ (Accuracy)**ã€**F1ã‚¹ã‚³ã‚¢ (F1 Score)**ã€**ROC AUC**ã€**å­¦ç¿’æ™‚é–“ (Training Time)**ã€**äºˆæ¸¬æ™‚é–“ (Prediction Time)**ã‚’æ¯”è¼ƒã—ãŸã‚‚ã®ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«é¸å®šã®éš›ã«ã¯ã€ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã‚’ç·åˆçš„ã«è€ƒæ…®ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n",
        "\n",
        "| Model Name / ãƒ¢ãƒ‡ãƒ«å             | Accuracy / ç²¾åº¦   | F1 Score / F1ã‚¹ã‚³ã‚¢   | ROC AUC / ROC AUC   | Train Time (s) / å­¦ç¿’æ™‚é–“ (ç§’) | Predict Time (s) / äºˆæ¸¬æ™‚é–“ (ç§’) |\n",
        "|-----------------------------------|-------------------|-----------------------|---------------------|------------------------------|----------------------------------|\n",
        "| **Random Forest**                 | 0.847363          | 0.790769              | 0.876013            | 0.570739                     | 5.786143                        |\n",
        "| **Voting**                        | 0.841751          | 0.786039              | 0.881637            | 0.513798                     | 2.070606                        |\n",
        "| **Stacking (LR meta)**            | 0.840629          | 0.781538              | 0.881800            | 1.824270                     | 9.396360                        |\n",
        "| **XGBoost**                       | 0.839506          | 0.784962              | 0.886833            | 0.472127                     | 0.426812                        |\n",
        "| **LightGBM**                      | 0.838384          | 0.782477              | 0.880299            | 0.048291                     | 4.601200                        |\n",
        "| **Stacking (XGB meta)**           | 0.836139          | 0.773994              | 0.874314            | 1.841040                     | 9.379701                        |\n",
        "| **Stacking (RF meta)**            | 0.832772          | 0.779911              | 0.869783            | 1.870320                     | 10.905935                       |\n",
        "| **Stacking (LGB meta)**           | 0.824916          | 0.766467              | 0.866834            | 1.868337                     | 11.698572                       |\n",
        "| **Calibrated LinearSVC**          | 0.819304          | 0.756430              | 0.866038            | 0.667395                     | 1.026984                        |\n",
        "\n",
        "### Explanation / è§£èª¬\n",
        "\n",
        "- **Accuracy**: The proportion of correct predictions made by the model.  \n",
        "`Random Forest` showed the highest accuracy among all models.\n",
        "\n",
        "- **F1 Score**: The harmonic mean of Precision and Recall.  \n",
        "`XGBoost` had a relatively high F1 score, indicating it balances precision and recall well.\n",
        "\n",
        "- **ROC AUC**: The area under the ROC curve, which measures the performance of a classification model.  \n",
        "`XGBoost` achieved the highest ROC AUC, indicating its strong classification ability.\n",
        "\n",
        "- **Training Time**: The time it took to train the model.  \n",
        "`LightGBM` was exceptionally fast, taking only 0.048 seconds for training.\n",
        "\n",
        "- **Prediction Time**: The time it took for the model to make predictions.  \n",
        "`XGBoost` and `Voting` were very fast in terms of prediction time.\n",
        "\n",
        "### è§£èª¬\n",
        "\n",
        "- **Accuracy (ç²¾åº¦)**: ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãäºˆæ¸¬ã—ãŸå‰²åˆã§ã™ã€‚  \n",
        "`Random Forest` ã¯å…¨ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§æœ€ã‚‚é«˜ã„ç²¾åº¦ã‚’ç¤ºã—ã¾ã—ãŸã€‚\n",
        "\n",
        "- **F1 Score (F1ã‚¹ã‚³ã‚¢)**: ç²¾åº¦ã¨å†ç¾ç‡ã®èª¿å’Œå¹³å‡ã§ã™ã€‚  \n",
        "`XGBoost` ã¯æ¯”è¼ƒçš„é«˜ã„F1ã‚¹ã‚³ã‚¢ã‚’æŒã¡ã€ç²¾åº¦ã¨å†ç¾ç‡ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- **ROC AUC (ROC AUC)**: å—ä¿¡è€…å‹•ä½œç‰¹æ€§æ›²ç·šï¼ˆROCæ›²ç·šï¼‰ã®ä¸‹ã®é¢ç©ã§ã€åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã§ã™ã€‚  \n",
        "`XGBoost` ã¯æœ€ã‚‚é«˜ã„ROC AUCã‚’é”æˆã—ã€å¼·åŠ›ãªåˆ†é¡èƒ½åŠ›ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "- **Training Time (å­¦ç¿’æ™‚é–“)**: ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ã‹ã‹ã£ãŸæ™‚é–“ã§ã™ã€‚  \n",
        "`LightGBM` ã¯éå¸¸ã«é«˜é€Ÿã§ã€å­¦ç¿’ã«ã‚ãšã‹0.048ç§’ã—ã‹ã‹ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n",
        "\n",
        "- **Prediction Time (äºˆæ¸¬æ™‚é–“)**: ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã‚’è¡Œã†ã®ã«ã‹ã‹ã£ãŸæ™‚é–“ã§ã™ã€‚  \n",
        "`XGBoost` ã¨ `Voting` ã¯äºˆæ¸¬æ™‚é–“ãŒéå¸¸ã«é€Ÿã„ã§ã™ã€‚\n",
        "\n",
        "### Model Selection Considerations / ãƒ¢ãƒ‡ãƒ«é¸å®šæ™‚ã®è€ƒæ…®äº‹é …\n",
        "\n",
        "- If accuracy is the priority, Random Forest or Voting are the top contenders.\n",
        "- If training and prediction time are more important (for real-time systems, for example), then XGBoost and LightGBM are better choices due to their fast processing times.\n",
        "- Stacking Models generally perform well in terms of accuracy but take longer to train and predict.\n",
        "- Model Selection should consider the trade-off between performance and computational efficiency.\n",
        "- Additionally, Stacking Models can offer better overall performance but are more computationally expensive.  \n",
        "\n",
        "- ç²¾åº¦ãŒæœ€å„ªå…ˆã§ã‚ã‚Œã°ã€Random Forest ã‚„ Voting ãŒæœ€é©ãªå€™è£œã§ã™ã€‚\n",
        "- å­¦ç¿’æ™‚é–“ã‚„äºˆæ¸¬æ™‚é–“ãŒã‚ˆã‚Šé‡è¦ãªå ´åˆï¼ˆä¾‹ãˆã°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚·ã‚¹ãƒ†ãƒ ã®å ´åˆï¼‰ã¯ã€XGBoost ã‚„ LightGBM ãŒé«˜é€Ÿã§å‡¦ç†ã§ãã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ãŒæœ€é©ã§ã™ã€‚\n",
        "- Stacking ç³»ã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸€èˆ¬çš„ã«ç²¾åº¦ãŒé«˜ã„ã§ã™ãŒã€å­¦ç¿’ã¨äºˆæ¸¬ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n",
        "- ãƒ¢ãƒ‡ãƒ«é¸å®šã¯ã€æ€§èƒ½ã¨è¨ˆç®—åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è€ƒæ…®ã™ã‚‹ã¹ãã§ã™ã€‚\n",
        "- ã¾ãŸã€Stacking ç³»ã®ãƒ¢ãƒ‡ãƒ«ã¯ç·ã˜ã¦é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã‚‚ã®ã®ã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚ŠãŒã¡ã§ã™ã€‚\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TWQUcLFP7gkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2ğŸš¢ Kaggle Scores Comparison Table / ã‚«ã‚°ãƒ«ã‚¹ã‚³ã‚¢æ¯”è¼ƒè¡¨\n",
        "\n",
        "| âœ… Model Type / ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥       | Model / ãƒ¢ãƒ‡ãƒ«å                  | ğŸ… Kaggle Score | ğŸ“ Notes / å‚™è€ƒ                                                                                      |\n",
        "| -------------------------- | ----------------------------- | --------------- | -------------------------------------------------------------------------------------------------- |\n",
        "| âœ… Single Model / å˜ç‹¬ãƒ¢ãƒ‡ãƒ«     | Random Forest (tuned)         | **0.77990**     | ğŸŒ² **Best single model. Strong balance of accuracy and robustness.** / å˜ç‹¬ãƒ¢ãƒ‡ãƒ«ã§æœ€é«˜æ€§èƒ½ã€‚ç²¾åº¦ã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹ãŒå„ªç§€ã€‚  |\n",
        "|                            | LightGBM (tuned)              | 0.74880         | âš¡ Very fast training but weaker generalization. / å­¦ç¿’ã¯éå¸¸ã«é«˜é€Ÿã ãŒã€æ±åŒ–æ€§èƒ½ã¯ã‚„ã‚„åŠ£ã‚‹ã€‚                            |\n",
        "|                            | XGBoost (tuned)               | 0.77033         | ğŸ”¥ Reliable and stable with solid generalization. / å®‰å®šã—ãŸæ€§èƒ½ã¨æ±åŒ–èƒ½åŠ›ã€‚                                   |\n",
        "|                            | SVM (Calibrated, tuned)       | 0.75598         | ğŸ§­ Reasonable performance, but slightly lower than tree-based models. / ç²¾åº¦ã¯ã¾ãšã¾ãšã ãŒã€æ±ºå®šæœ¨ç³»ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚„ã‚„åŠ£ã‚‹ã€‚   |\n",
        "| âœ… Voting Ensemble / ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« | Voting (RF + XGB + LGB + SVM) | 0.77033         | ğŸ—³ï¸ Robust and stable by combining strengths of multiple models. / è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ã¦å®‰å®šæ€§ã‚’ç¢ºä¿ã€‚          |\n",
        "| âœ… Stacking / ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°        | Meta: Logistic Regression     | 0.77272         | ğŸ“Š Interpretable and effective. Performs better than some single models. / è§£é‡ˆã—ã‚„ã™ãã€å˜ç‹¬ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå„ªã‚Œã‚‹å ´é¢ã‚‚ã‚ã‚Šã€‚ |\n",
        "|                            | Meta: XGBoost                 | **0.77990**     | ğŸš€ Matches best single model. Strong meta-learner. / å˜ä½“XGBoostã¨åŒç­‰ã®ã‚¹ã‚³ã‚¢ã€‚å¼·åŠ›ãªãƒ¡ã‚¿å­¦ç¿’å™¨ã€‚                    |\n",
        "|                            | Meta: LightGBM                | 0.76076         | âš™ï¸ Improved from base LGB, but still below top models. / å˜ä½“LGBã‚ˆã‚Šæ”¹å–„ã—ãŸãŒã€ä¸Šä½ãƒ¢ãƒ‡ãƒ«ã«ã¯åŠã°ãªã„ã€‚                 |\n",
        "|                            | Meta: Random Forest           | 0.77272         | ğŸŒ³ Stable with good overall metrics, slightly lower than the top. / å®‰å®šã—ã¦ã„ã‚‹ãŒã€ã‚¹ã‚³ã‚¢ã¯ã‚„ã‚„åŠ£ã‚‹ã€‚              |\n"
      ],
      "metadata": {
        "id": "QOKe4KTEu06P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary / ã¾ã¨ã‚\n",
        "- The comparison of various machine learning models for the Titanic survival prediction task reveals several important insights:\n",
        "\n",
        "- Random Forest (tuned) achieved the highest Kaggle score (0.77990) among all models. It balances accuracy and robustness well, making it a strong standalone choice.\n",
        "\n",
        "- XGBoost, both as a single model and as a meta-model in stacking, also performed consistently well, matching the best score. This confirms its strong generalization ability.\n",
        "\n",
        "- Voting ensemble and Logistic Regression stacking achieved slightly lower scores, but showed stable performance, indicating they effectively aggregate diverse model strengths.\n",
        "\n",
        "- LightGBM, while extremely fast, had relatively lower scores. Even when used as a meta-model, the performance did not surpass Random Forest or XGBoost, suggesting it may be more sensitive to tuning or data structure.\n",
        "\n",
        "- Calibrated LinearSVC performed reasonably but did not outperform tree-based models, possibly due to linear limitations in a complex, non-linear problem like this one.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ç”Ÿå­˜äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æ§˜ã€…ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã‹ã‚‰ã€ã„ãã¤ã‹ã®é‡è¦ãªçŸ¥è¦‹ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "- ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãŒæœ€ã‚‚é«˜ã„Kaggleã‚¹ã‚³ã‚¢ï¼ˆ0.77990ï¼‰ã‚’è¨˜éŒ²ã—ã€ç²¾åº¦ã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ãŸã‚ã€å˜ç‹¬ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å„ªç§€ã§ã™ã€‚\n",
        "\n",
        "- XGBoostã¯å˜ç‹¬ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚‚å®‰å®šã—ãŸé«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã€å¼·ã„æ±åŒ–èƒ½åŠ›ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "- Votingã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚„ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã¯ã‚„ã‚„ã‚¹ã‚³ã‚¢ãŒåŠ£ã‚Šã¾ã™ãŒã€å®‰å®šçš„ãªæ€§èƒ½ã‚’ç¤ºã—ã€å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ã‚’ã†ã¾ãçµ±åˆã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- LightGBMã¯éå¸¸ã«é«˜é€Ÿã§ã™ãŒã‚¹ã‚³ã‚¢ã¯æ¯”è¼ƒçš„ä½ãã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚„XGBoostã‚’ä¸Šå›ã‚‹ã“ã¨ã¯ãªãã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å½±éŸ¿ã•ã‚Œã‚„ã™ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "- Calibrated LinearSVCã¯ã¾ãšã¾ãšã®æ€§èƒ½ã§ã™ãŒã€éç·šå½¢å•é¡Œã«ãŠã„ã¦ã¯æœ¨æ§‹é€ ãƒ¢ãƒ‡ãƒ«ã«åŠ£ã‚‹çµæœã¨ãªã‚Šã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®é™ç•ŒãŒå½±éŸ¿ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "_8zWUBn7h1z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Comparison of Kaggle Scores and Cross-Validation Accuracy with Difference Labels / Kaggleã‚¹ã‚³ã‚¢ã¨äº¤å·®æ¤œè¨¼ç²¾åº¦ã®æ¯”è¼ƒï¼ˆå·®åˆ†ãƒ©ãƒ™ãƒ«ä»˜ãï¼‰\n",
        "\n",
        "This bar chart compares the Kaggle leaderboard scores and cross-validation (CV) accuracies for each model.\n",
        "The numbers displayed next to the CV accuracy bars indicate the difference between CV accuracy and Kaggle score, which can help identify potential overfitting or model stability issues.\n",
        "\n",
        "ã“ã®æ£’ã‚°ãƒ©ãƒ•ã¯ã€å„ãƒ¢ãƒ‡ãƒ«ã®Kaggleãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ã¨äº¤å·®æ¤œè¨¼ï¼ˆCVï¼‰ç²¾åº¦ã‚’æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚\n",
        "CVç²¾åº¦ã®ãƒãƒ¼ã®æ¨ªã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹æ•°å€¤ã¯ã€CVç²¾åº¦ã¨Kaggleã‚¹ã‚³ã‚¢ã®å·®ã‚’è¡¨ã—ã¦ãŠã‚Šã€éå­¦ç¿’ã‚„ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã®å•é¡Œã‚’è¦‹æ¥µã‚ã‚‹æ‰‹ãŒã‹ã‚Šã«ãªã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "bOmKvXhyZ82k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
        "data = {\n",
        "    \"Model\": [\n",
        "        \"Random Forest\",\n",
        "        \"LightGBM\",\n",
        "        \"XGBoost\",\n",
        "        \"SVM\",\n",
        "        \"Voting\",\n",
        "        \"Stacking (LR)\",\n",
        "        \"Stacking (XGB)\",\n",
        "        \"Stacking (LGB)\",\n",
        "        \"Stacking (RF)\"\n",
        "    ],\n",
        "    \"Kaggle Score\": [\n",
        "        0.77990,\n",
        "        0.74880,\n",
        "        0.77033,\n",
        "        0.75598,\n",
        "        0.77033,\n",
        "        0.77272,\n",
        "        0.77990,\n",
        "        0.76076,\n",
        "        0.77272\n",
        "    ],\n",
        "    \"CV Accuracy\": [\n",
        "        0.847363,\n",
        "        0.838384,\n",
        "        0.839506,\n",
        "        0.819304,\n",
        "        0.841751,\n",
        "        0.840629,\n",
        "        0.836139,\n",
        "        0.824916,\n",
        "        0.832772\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# å·®åˆ†è¨ˆç®—\n",
        "df[\"Difference\"] = df[\"CV Accuracy\"] - df[\"Kaggle Score\"]\n",
        "\n",
        "# meltå‡¦ç†ã§Seabornç”¨ãƒ‡ãƒ¼ã‚¿æ•´å½¢\n",
        "df_plot = df.melt(id_vars=\"Model\", value_vars=[\"Kaggle Score\", \"CV Accuracy\"],\n",
        "                  var_name=\"Metric\", value_name=\"Score\")\n",
        "\n",
        "# å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.set(style=\"whitegrid\")\n",
        "barplot = sns.barplot(data=df_plot, x=\"Score\", y=\"Model\", hue=\"Metric\")\n",
        "\n",
        "# å·®åˆ†ã®ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ \n",
        "for index, row in df.iterrows():\n",
        "    plt.text(\n",
        "        row[\"CV Accuracy\"] + 0.002,\n",
        "        index,\n",
        "        f'{row[\"Difference\"]:.3f}',\n",
        "        color='black',\n",
        "        va='center'\n",
        "    )\n",
        "\n",
        "plt.title(\"Comparison of Kaggle Score and CV Accuracy by Model with Difference Labels\", fontsize=14)\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.legend(title=\"Metric\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vs7RemOyXYhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Š Model Performance Comparison / ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆCV vs Kaggleï¼‰\n",
        "\n",
        "\n",
        "| âœ… **Model / ãƒ¢ãƒ‡ãƒ«å** | ğŸ“ˆ **CV Accuracy** | ğŸ… **Kaggle Score** | ğŸ“‰ **Gap / ä¹–é›¢** | ğŸ“ **Notes / ã‚³ãƒ¡ãƒ³ãƒˆ**                                            |\n",
        "| ------------------ | ------------------ | ------------------- | --------------- | -------------------------------------------------------------- |\n",
        "| Random Forest      | 0.847              | 0.7799              | 0.0675          | âš ï¸ High accuracy but gap indicates overfitting / é«˜ç²¾åº¦ã ãŒéå­¦ç¿’ã®æ‡¸å¿µã‚ã‚Š |\n",
        "| LightGBM           | 0.838              | 0.7488              | 0.0896          | âš ï¸ Fast but shows significant drop / é«˜é€Ÿã ãŒCVã¨ã®ä¹–é›¢ãŒå¤§ãã„            |\n",
        "| XGBoost            | 0.840              | 0.7703              | 0.0692          | âœ… Balanced performance / ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„æ€§èƒ½                             |\n",
        "| SVM                | 0.819              | 0.7559              | 0.0633          | ğŸ§­ Moderate generalization / ã¾ãšã¾ãšã®æ±åŒ–æ€§èƒ½                         |\n",
        "| Voting             | 0.842              | 0.7703              | 0.0715          | â— Stable but gap exists / å®‰å®šæ€§ã¯ã‚ã‚‹ãŒä¹–é›¢ã‚ã‚Š                          |\n",
        "| Stacking (LR)      | 0.841              | 0.7727              | 0.0680          | ğŸ“Š Interpretability with decent generalization / è§£é‡ˆæ€§ã¨ãã“ãã“ã®æ±åŒ–æ€§èƒ½ |\n",
        "| Stacking (XGB)     | 0.836              | 0.7799              | 0.0562          | âœ… Best consistency across metrics / æœ€ã‚‚å®‰å®šã—ãŸãƒ¢ãƒ‡ãƒ«                  |\n",
        "| Stacking (LGB)     | 0.825              | 0.7608              | 0.0642          | âš™ï¸ Improvement from base LGB / å˜ä½“LGBã‚ˆã‚Šæ”¹å–„                       |\n",
        "| Stacking (RF)      | 0.833              | 0.7727              | 0.0603          | ğŸŒ³ Stable stacking performance / å®‰å®šã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°æ€§èƒ½                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "gphBMzSVVOrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion / çµè«–ï¼š\n",
        "Although the tuned Random Forest achieved the highest cross-validation accuracy (0.8474), its relatively large gap with the Kaggle score (0.7799) suggests potential overfitting. This makes it a risky choice as the final model, especially for deployment or unseen data.\n",
        "On the other hand, the Stacking model with XGBoost as meta-learner showed a smaller performance gap and consistent scores across validation and Kaggle.\n",
        "Therefore, it is safer and more reliable to select the Stacking (XGBoost meta) model as the final choice.  \n",
        "\n",
        "ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¯äº¤å·®æ¤œè¨¼ã§æœ€é«˜ã®ç²¾åº¦ï¼ˆ0.8474ï¼‰ã‚’ç¤ºã—ã¾ã—ãŸãŒã€Kaggleã‚¹ã‚³ã‚¢ï¼ˆ0.7799ï¼‰ã¨ã®ä¹–é›¢ãŒå¤§ããã€éå­¦ç¿’ã®å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã¾ã™ã€‚ãã®ãŸã‚ã€æœªçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚„æœ¬ç•ªé‹ç”¨ã‚’æƒ³å®šã—ãŸå ´åˆã€æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é¸ã¶ã«ã¯ã‚„ã‚„ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "ä¸€æ–¹ã§ã€XGBoostã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ä½¿ç”¨ã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯ã€CVã‚¹ã‚³ã‚¢ã¨Kaggleã‚¹ã‚³ã‚¢ã®å·®ãŒå°ã•ãã€ã‚ˆã‚Šä¸€è²«ã—ãŸå®‰å®šã—ãŸæ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚\n",
        "ã—ãŸãŒã£ã¦ã€æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ Stacking (XGBoost meta) ã‚’é¸æŠã™ã‚‹ã®ãŒå®‰å…¨ã§ä¿¡é ¼æ€§ãŒé«˜ã„ã¨è¨€ãˆã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "cXJJIzkcbVDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 ğŸ“¤ Final Submission Code / æœ€çµ‚æå‡ºç”¨ã‚³ãƒ¼ãƒ‰\n",
        "\n",
        "This section generates the final prediction file (`submission_final.csv`) using the best-performing model from our comparisons.  \n",
        "ä»¥ä¸‹ã¯ã€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã®çµæœæœ€ã‚‚æ€§èƒ½ãŒè‰¯ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€Kaggle ã«æå‡ºã™ã‚‹äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`submission_final.csv`ï¼‰ã‚’ä½œæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚\n"
      ],
      "metadata": {
        "id": "fIz2IudVEfKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTabtBuW8Mho"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Prepare test data / ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚’æº–å‚™\n",
        "# â€» df_test ã«å¿…è¦ãªå‰å‡¦ç†ï¼ˆæ¬ æå‡¦ç†ãƒ»ç‰¹å¾´é‡å¤‰æ›ï¼‰ãŒæ¸ˆã‚“ã§ã„ã‚‹å‰æã§ã™\n",
        "# =====================================================\n",
        "X_test_selected = df_fe4_test[selected_features]  # test.csvã¨åŒæ§˜ã®ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\n",
        "passenger_ids = df_test['PassengerId']  # Kaggle æå‡ºç”¨ã® ID ã‚«ãƒ©ãƒ \n",
        "\n",
        "# =====================================================\n",
        "# 2. Make predictions / äºˆæ¸¬ã‚’å®Ÿè¡Œ\n",
        "# =====================================================\n",
        "y_test_pred = best_model_stacking_XGB_tuned.predict(X_test_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Create submission DataFrame / æå‡ºç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ\n",
        "# =====================================================\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Survived': y_test_pred\n",
        "})\n",
        "\n",
        "# =====================================================\n",
        "# 4. Save to CSV / CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆKaggleã«æå‡ºå¯èƒ½ï¼‰\n",
        "# =====================================================\n",
        "submission.to_csv('submission_final.csv', index=False)\n",
        "print(\"âœ… submission.csv has been saved!\")\n",
        "\n",
        "\n",
        "# Colabã®å ´åˆã€è‡ªå‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "from google.colab import files\n",
        "files.download('submission_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 âœ… Final Model Recommendation Report / æœ€çµ‚ãƒ¢ãƒ‡ãƒ«æ¨è–¦ãƒ¬ãƒãƒ¼ãƒˆ\n",
        "\n",
        "### ğŸ† Selected Model / é¸å®šãƒ¢ãƒ‡ãƒ«  \n",
        "**Stacking (XGB meta)**\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Reason for Selection / é¸å®šç†ç”±\n",
        "\n",
        "- Consistent performance across both CV and Kaggle leaderboard  \n",
        "- High ROC AUC and F1 score, indicating strong classification balance  \n",
        "- Although it requires longer training (1.84 sec) and prediction time (9.37 sec) compared to others, its superior generalization and stability outweigh the computational cost  \n",
        "\n",
        "- CVã¨Kaggleã‚¹ã‚³ã‚¢ã®ä¸¡æ–¹ã§å®‰å®šã—ãŸæ€§èƒ½  \n",
        "- ROC AUCã‚„F1ã‚¹ã‚³ã‚¢ã‚‚é«˜ãã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸåˆ†é¡æ€§èƒ½  \n",
        "- å­¦ç¿’æ™‚é–“ã¯1.84ç§’ã€æ¨è«–æ™‚é–“ã¯9.37ç§’ã¨ä»–ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šé•·ã„ã§ã™ãŒã€å®‰å®šã—ãŸæ€§èƒ½ãŒãã‚Œã‚’è£œã£ã¦ã„ã¾ã™\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“‰ Alternative Models Considered / ä»–ã«æ¤œè¨ã—ãŸãƒ¢ãƒ‡ãƒ«\n",
        "\n",
        "- **Random Forest**: Highest CV accuracy but larger gap with Kaggle score, suggesting overfitting. Training time 0.57 sec, prediction time 5.78 sec.  \n",
        "- **XGBoost**: Strong and fast, with training time 0.23 sec and prediction time 0.73 sec, but slightly outperformed by Stacking (XGB) in generalization  \n",
        "- **Voting**: Good accuracy but larger variation and weaker test set performance. Training and prediction times are relatively low (0.51 sec and 2.07 sec), making it suitable for faster inference scenarios despite slightly lower stability.  \n",
        "\n",
        "- **Random Forest**ï¼šCVç²¾åº¦ãŒæœ€ã‚‚é«˜ã„ãŒã€Kaggleã¨ã®ä¹–é›¢ãŒå¤§ããéå­¦ç¿’ã®æ‡¸å¿µã‚ã‚Šï¼ˆå­¦ç¿’0.57ç§’ã€æ¨è«–5.78ç§’ï¼‰  \n",
        "- **XGBoost**ï¼šå®‰å®šæ€§ã¯é«˜ãäºˆæ¸¬ã‚‚é«˜é€Ÿï¼ˆå­¦ç¿’0.23ç§’ã€æ¨è«–0.73ç§’ï¼‰ã ãŒã€Stackingã®æ–¹ãŒç·åˆçš„ã«å„ªã‚Œã¦ã„ãŸ  \n",
        "- **Voting**ï¼šé«˜ç²¾åº¦ã ãŒã€ã‚„ã‚„æ±åŒ–æ€§èƒ½ãŒåŠ£ã‚‹ã€‚å­¦ç¿’0.51ç§’ã€æ¨è«–2.07ç§’ã¨é«˜é€Ÿã§ã€æ¨è«–é€Ÿåº¦ã‚’é‡è¦–ã™ã‚‹å ´åˆã«é©ã—ã¦ã„ã‚‹  \n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š Summary Table / ã‚µãƒãƒªãƒ¼è¡¨ï¼ˆæŠœç²‹ï¼‰\n",
        "\n",
        "| Model               | CV Accuracy | Kaggle Score | Gap    | Train Time | Predict Time |\n",
        "|---------------------|-------------|---------------|--------|-------------|---------------|\n",
        "| Stacking (XGB meta) | 0.836       | 0.7799        | 0.0562 | 1.84 sec    | 9.37 sec      |\n",
        "| Random Forest       | 0.847       | 0.7799        | 0.0675 | 0.57 sec    | 5.78 sec      |\n",
        "| Voting              | 0.842       | 0.7703        | 0.0715 | 0.51 sec    | 2.07 sec      |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŸ¢ Final Recommendation / æœ€çµ‚æ¨è–¦\n",
        "\n",
        "> **Deploy Stacking (XGB meta)** as the final model for its excellent generalization and consistent performance across all metrics.  \n",
        "> **æ¡ç”¨ãƒ¢ãƒ‡ãƒ«ï¼šStacking (XGB meta)**  \n",
        "> ç†ç”±ï¼šç²¾åº¦ã€æ±åŒ–æ€§èƒ½ã€å®‰å®šæ€§ã€Kaggleã¨ã®ä¸€è‡´æ€§ã«ãŠã„ã¦æœ€ã‚‚ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã‹ã£ãŸãŸã‚ã€‚\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8YfamNmwCn3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8ğŸ¯ SHAP-Based Model Contribution (Meta Model: XGBoost) / SHAPã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è²¢çŒ®åº¦åˆ†æï¼ˆãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼šXGBoostï¼‰  \n",
        "\n",
        "We analyzed the importance of each base modelâ€™s predictions using SHAP values from the final meta-model (XGBoost) in the stacking ensemble.\n",
        "\n",
        "ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã®ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoostï¼‰ã«å¯¾ã—ã€å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ãŒã©ã‚Œã»ã©é‡è¦ã ã£ãŸã‹ã‚’SHAPå€¤ã§è©•ä¾¡ã—ã¾ã—ãŸã€‚"
      ],
      "metadata": {
        "id": "neQJC7e-JNCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Step 1: Prepare base model predictions\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šå„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬å€¤ï¼ˆç”Ÿå­˜ç¢ºç‡ï¼‰ã‚’è¨ˆç®—\n",
        "# These will be used as input features for the meta-model\n",
        "# â†’ ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoostï¼‰ã®å…¥åŠ›ç‰¹å¾´é‡ã¨ãªã‚Šã¾ã™\n",
        "# ===========================================\n",
        "base_preds = []\n",
        "\n",
        "# Get base model names\n",
        "model_names = list(best_model_stacking_XGB_tuned.named_estimators_.keys())\n",
        "\n",
        "# For each base model, get the predicted probability for class 1 (Survived)\n",
        "# å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€ç”Ÿå­˜ã‚¯ãƒ©ã‚¹ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—\n",
        "for name, model in best_model_stacking_XGB_tuned.named_estimators_.items():\n",
        "    # Use predict_proba (SVM pipeline is already calibrated)\n",
        "    preds = model.predict_proba(X_selected)[:, 1]\n",
        "    base_preds.append(preds)\n",
        "\n",
        "# ===========================================\n",
        "# Step 2: Stack predictions as meta-features\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šäºˆæ¸¬å€¤ã‚’åˆ—æ–¹å‘ã«çµåˆã—ã¦ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ç‰¹å¾´é‡ã‚’ä½œæˆ\n",
        "# Shape will be (n_samples, n_base_models)\n",
        "# â†’ (ã‚µãƒ³ãƒ—ãƒ«æ•°, ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ•°)ã®2æ¬¡å…ƒé…åˆ—ã«ãªã‚Šã¾ã™\n",
        "# ===========================================\n",
        "meta_features = np.column_stack(base_preds)\n",
        "\n",
        "# ===========================================\n",
        "# Step 3: Create SHAP explainer for the meta-model\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—3ï¼šãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoostï¼‰ç”¨ã®SHAP Explainerã‚’ä½œæˆ\n",
        "# ===========================================\n",
        "explainer_meta = shap.Explainer(best_model_stacking_XGB_tuned.final_estimator_)\n",
        "\n",
        "# ===========================================\n",
        "# Step 4: Calculate SHAP values for meta-model inputs\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—4ï¼šãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹SHAPå€¤ã‚’è¨ˆç®—\n",
        "# â†’ å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒã€ã‚¹ã‚¿ãƒƒã‚¯äºˆæ¸¬ã«ã©ã‚Œã ã‘è²¢çŒ®ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†æ\n",
        "# ===========================================\n",
        "shap_values_meta = explainer_meta(meta_features)\n",
        "\n",
        "# ===========================================\n",
        "# Step 5: Plot SHAP summary plot\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—5ï¼šSHAPã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã§è¦–è¦šåŒ–\n",
        "# â†’ å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã€Œå¹³å‡çš„ãªé‡è¦åº¦ã€ã‚’å¯è¦–åŒ–\n",
        "# ===========================================\n",
        "shap.summary_plot(shap_values_meta, features=meta_features, feature_names=model_names)\n",
        "\n",
        "# ===========================================\n",
        "# Step 6: Print mean absolute SHAP values\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—6ï¼šå„ç‰¹å¾´é‡ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰ã®å¹³å‡SHAPå€¤ã‚’æ•°å€¤ã§è¡¨ç¤º\n",
        "# â†’ ã‚°ãƒ©ãƒ•ã ã‘ã§ãªãã€æ•°å€¤ã§ã‚‚æ¯”è¼ƒå¯èƒ½ã«ã—ã¾ã™\n",
        "# ===========================================\n",
        "print(\"ğŸ§­ Mean SHAP Value (Importance) per Base Model / å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡SHAPå€¤ï¼ˆé‡è¦åº¦ï¼‰:\")\n",
        "mean_abs_shap = np.abs(shap_values_meta.values).mean(axis=0)\n",
        "\n",
        "for name, val in zip(model_names, mean_abs_shap):\n",
        "    print(f\"{name}: {val:.4f}\")"
      ],
      "metadata": {
        "id": "r6oiB3MEUMUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### ğŸ§­ Mean SHAP Values (Model Importance) / å¹³å‡SHAPå€¤ï¼ˆãƒ¢ãƒ‡ãƒ«ã®é‡è¦åº¦ï¼‰\n",
        "\n",
        "| Base Model | SHAP Importance | Interpretation / è§£é‡ˆ |\n",
        "|------------|------------------|------------------------|\n",
        "| XGBoost (`xgb`) | 0.3570 | â­ Most important contributor to final predictions / æœ€ã‚‚äºˆæ¸¬ã«è²¢çŒ® |\n",
        "| Random Forest (`rf`) | 0.3067 | ğŸ”¸ Strong contribution / å¼·ã„å½±éŸ¿ã‚ã‚Š |\n",
        "| LightGBM (`lgb`) | 0.2203 | âš ï¸ Moderate impact / ä¸­ç¨‹åº¦ã®è²¢çŒ® |\n",
        "| SVM (`svm`) | 0.0418 | ğŸ”» Minimal influence / ã»ã¨ã‚“ã©å½±éŸ¿ãªã— |\n",
        "\n",
        "This analysis confirms that the meta-model heavily relied on XGBoost and Random Forest to make its final decisions.  \n",
        "\n",
        "ã“ã®çµæœã‹ã‚‰ã€ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¯XGBoostã¨Random Forestã®å‡ºåŠ›ã‚’ä¸»ã«ä¿¡é ¼ã—ã¦ã„ãŸã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n"
      ],
      "metadata": {
        "id": "3fB5MLTxUwQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Contribution Analysis Based on SHAP Values / SHAPå€¤ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«è²¢çŒ®åº¦ã®åˆ†æ\n",
        "\n",
        "The SHAP summary plot shows the contribution of each base model to the final stacked prediction. Among the base models, XGBoost has the highest mean SHAP value (0.357), followed by Random Forest (0.307), LightGBM (0.220), and SVM (0.042).\n",
        "\n",
        "This suggests that the meta-model relies most heavily on the tree-based models (XGBoost, Random Forest, LightGBM) for its predictions. The relatively low contribution of SVM may be due to its linear nature, which limits its ability to capture complex nonlinear relationships in the data. Moreover, since the SVM is calibrated, its predicted probabilities are expected to be reliable but may lack the expressive power of tree-based models.\n",
        "\n",
        "Therefore, combining diverse model types in stacking leverages their complementary strengths: tree models capture nonlinearities well, while SVM provides a linear perspective. The meta-model effectively weights these contributions to improve overall prediction accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "SHAPã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã¯ã€æœ€çµ‚çš„ãªã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°äºˆæ¸¬ã«å¯¾ã™ã‚‹å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è²¢çŒ®åº¦ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã¯ã€XGBoostãŒæœ€ã‚‚é«˜ã„å¹³å‡SHAPå€¤ï¼ˆ0.357ï¼‰ã‚’æŒã¡ã€æ¬¡ã„ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆ0.307ï¼‰ã€LightGBMï¼ˆ0.220ï¼‰ã€SVMï¼ˆ0.042ï¼‰ã¨ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã¯ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒä¸»ã«æœ¨æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoostã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€LightGBMï¼‰ã«ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ä¸€æ–¹ã§ã€SVMã®è²¢çŒ®åº¦ãŒä½ã„ç†ç”±ã¯ã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚è¤‡é›‘ãªéç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆã«ãã„ã“ã¨ã«èµ·å› ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚SVMã¯ç¢ºç‡ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ–½ã—ã¦ã„ã‚‹ãŸã‚ã€äºˆæ¸¬ç¢ºç‡ã®ä¿¡é ¼æ€§ã¯é«˜ã„ã‚‚ã®ã®ã€è¡¨ç¾åŠ›ã§ã¯æœ¨æ§‹é€ ãƒ¢ãƒ‡ãƒ«ã«åŠ£ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã—ãŸãŒã£ã¦ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã«ãŠã„ã¦å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãã‚Œãã‚Œã®å¼·ã¿ã‚’æ´»ã‹ã—ã¦ã„ã¾ã™ã€‚æœ¨æ§‹é€ ãƒ¢ãƒ‡ãƒ«ã¯éç·šå½¢æ€§ã‚’ã‚ˆãæ‰ãˆã€SVMã¯ç·šå½¢æ€§ã‚’è£œå®Œã™ã‚‹å½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã¯ã“ã‚Œã‚‰ã®å¯„ä¸åº¦ã‚’ã†ã¾ãåŠ é‡ã—ã€å…¨ä½“ã®äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "AJqbAZ2MYw0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 ğŸ” Final Model Analysis / æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨è€ƒå¯Ÿ\n",
        "\n",
        "In this section, I summarize the performance of the final stacking model using both quantitative metrics and model interpretation techniques.\n",
        "Cross-validation scores are analyzed alongside SHAP values to evaluate which base models contributed most to the final prediction.\n",
        "This analysis helps assess the modelâ€™s strengths and limitations, especially in handling class imbalance.  \n",
        "\n",
        "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€æœ€çµ‚çš„ã«æ¡ç”¨ã—ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ã€å®šé‡çš„ãªè©•ä¾¡æŒ‡æ¨™ã¨ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã®è¦³ç‚¹ã‹ã‚‰ç·æ‹¬ã—ã¾ã™ã€‚\n",
        "äº¤å·®æ¤œè¨¼ã®ã‚¹ã‚³ã‚¢ã«åŠ ãˆã¦SHAPå€¤ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã©ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒæœ€çµ‚äºˆæ¸¬ã«æœ€ã‚‚è²¢çŒ®ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ã¨èª²é¡Œï¼ˆç‰¹ã«ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¸ã®å¯¾å¿œï¼‰ã«ã¤ã„ã¦è€ƒå¯Ÿã‚’è¡Œã„ã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… 1. Model Performance Summary / ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡\n",
        "\n",
        "| Metric / æŒ‡æ¨™            | Value / å€¤  |\n",
        "|--------------------------|-------------|\n",
        "| ğŸ¯ Mean CV Accuracy      | **0.836**   |\n",
        "| ğŸ“ˆ Mean CV ROC AUC       | **0.874**   |\n",
        "\n",
        "ğŸ“‹ **Classification Report (CV predictions) / åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆäº¤å·®æ¤œè¨¼ï¼‰:**\n",
        "\n",
        "| Class / ã‚¯ãƒ©ã‚¹ | Precision | Recall | F1-score |\n",
        "|----------------|-----------|--------|----------|\n",
        "| 0 (Not Survived / éç”Ÿå­˜) | 0.84 | 0.90 | 0.87 |\n",
        "| 1 (Survived / ç”Ÿå­˜)       | 0.82 | 0.73 | 0.77 |\n",
        "\n",
        "ğŸ—£ï¸ **Interpretation / è§£é‡ˆ:**\n",
        "- The model predicts non-survivors well, with high recall (0.90).  \n",
        "- Survivor recall (0.73) is lower, indicating some survivors are missed.\n",
        "- ãƒ¢ãƒ‡ãƒ«ã¯éç”Ÿå­˜è€…ã®äºˆæ¸¬ç²¾åº¦ãŒé«˜ãã€å†ç¾ç‡ï¼ˆRecallï¼‰ãŒ0.90ã¨å„ªã‚Œã¦ã„ã‚‹ã€‚\n",
        "- ä¸€æ–¹ã€ç”Ÿå­˜è€…ã®RecallãŒ0.73ã¨ä½ã‚ã§ã€ä¸€éƒ¨ã®ç”Ÿå­˜è€…ã‚’è¦‹é€ƒã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š 2. SHAP-Based Contribution of Base Models / SHAPã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é‡è¦åº¦åˆ†æ\n",
        "\n",
        "| Base Model | Mean SHAP Value / å¹³å‡SHAPå€¤ |\n",
        "|------------|-------------------------------|\n",
        "| XGBoost    | **0.3570**                    |\n",
        "| RandomForest | **0.3067**                  |\n",
        "| LightGBM   | **0.2203**                    |\n",
        "| SVM        | **0.0418**                    |\n",
        "\n",
        "ğŸ§  **Discussion / è€ƒå¯Ÿ:**\n",
        "- XGBoost and RandomForest had the highest SHAP importance â†’ strong contribution to final prediction.  \n",
        "- These models handle non-linear relationships well, which fits the Titanic dataset.  \n",
        "- SVM had the lowest contribution, likely because it is linear and cannot capture complex patterns.  \n",
        "- Therefore, tree-based models are better suited for this task.\n",
        "\n",
        "-  XGBoostã¨RandomForestã®SHAPå€¤ãŒæœ€ã‚‚é«˜ãã€æœ€çµ‚äºˆæ¸¬ã¸ã®è²¢çŒ®ãŒå¤§ãã„ã¨ã‚ã‹ã‚‹ã€‚\n",
        "- ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯éç·šå½¢æ§‹é€ ã«å¼·ãã€Titanicã®ã‚ˆã†ãªè¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿ã«é©ã—ã¦ã„ã‚‹ã€‚\n",
        "-  SVMã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚ã€è¤‡é›‘ãªæ§‹é€ ã‚’æ‰ãˆã«ããã€è²¢çŒ®åº¦ãŒä½ã‹ã£ãŸã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚\n",
        "-  ã‚ˆã£ã¦ã€ãƒ„ãƒªãƒ¼ç³»ãƒ¢ãƒ‡ãƒ«ãŒã“ã®å•é¡Œã«ã¯ã‚ˆã‚Šé©ã—ã¦ã„ã‚‹ã¨çµè«–ã¥ã‘ã‚‰ã‚Œã‚‹ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ 3. Conclusion / çµè«–\n",
        "\n",
        "ğŸ“Œ The stacking model with XGBoost meta-learner performs well overall, but recall for survivors can be improved.  \n",
        "ğŸ“Œ XGBoostã‚’ãƒ¡ã‚¿å­¦ç¿’å™¨ã«ä½¿ã£ãŸã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯å…¨ä½“ã¨ã—ã¦é«˜æ€§èƒ½ã ãŒã€ç”Ÿå­˜è€…ã®Recallã¯æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ã€‚\n",
        "\n",
        "ğŸ“ˆ **Possible improvements / æ”¹å–„ã®æ–¹å‘æ€§:**\n",
        "- Class balancing (e.g. SMOTE) or class weight tuning may improve recall for minority class (survivors).  \n",
        "- Base model selection can be further optimized.  \n",
        "-   ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¸ã®å¯¾å¿œï¼ˆä¾‹ï¼šSMOTEï¼‰ã‚„ã‚¯ãƒ©ã‚¹é‡ã¿èª¿æ•´ã§Recallã‚’æ”¹å–„ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\n",
        "- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ§‹æˆã‚„é¸å®šã‚’è¦‹ç›´ã™ã“ã¨ã§ã€ã•ã‚‰ãªã‚‹æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã§ãã‚‹ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Summary / ã¾ã¨ã‚\n",
        "\n",
        "- Final stacking model (meta: XGBoost) achieves 83.6% accuracy and 87.4% ROC AUC.\n",
        "- SHAP analysis shows strongest contributions from XGBoost and RandomForest.\n",
        "- SVM has low impact, likely due to its linear nature.\n",
        "- Overall strong performance, with room to improve recall for class 1.\n",
        "\n",
        "- æœ€çµ‚çš„ãªã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼šXGBoostï¼‰ã¯ã€**æ­£è§£ç‡ 83.6%**ã€**ROC AUC 87.4%** ã‚’é”æˆã—ã¾ã—ãŸã€‚  \n",
        "- SHAPè§£æã«ã‚ˆã‚Œã°ã€XGBoostã¨RandomForestãŒæœ€ã‚‚å¤§ããªè²¢çŒ®ã‚’ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸã€‚  \n",
        "- SVMã®è²¢çŒ®åº¦ã¯ä½ãã€ã“ã‚Œã¯ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã«ãã„ã“ã¨ãŒåŸå› ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚  \n",
        "- å…¨ä½“ã¨ã—ã¦æ€§èƒ½ã¯é«˜ã„ã§ã™ãŒã€ç”Ÿå­˜è€…ï¼ˆã‚¯ãƒ©ã‚¹1ï¼‰ã®Recallã«ã¯æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚\n"
      ],
      "metadata": {
        "id": "mrHJHGem7dWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 ğŸ“Œ Conclusion / çµè«–  \n",
        "\n",
        "In this final section, I summarize the overall approach, results, key insights, and areas for future improvement based on the Titanic survival prediction project.\n",
        "This reflection highlights what was learned through model development, evaluation, and interpretation.\n",
        "\n",
        "ã“ã®æœ€çµ‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Titanicç”Ÿå­˜äºˆæ¸¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãŠã‘ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€çµæœã€å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ã€ä»Šå¾Œã®æ”¹å–„ç‚¹ã«ã¤ã„ã¦ç·æ‹¬ã—ã¾ã™ã€‚\n",
        "ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‹ã‚‰è©•ä¾¡ãƒ»è§£é‡ˆã«è‡³ã‚‹ã¾ã§ã®ä¸€é€£ã®éç¨‹ã‚’æŒ¯ã‚Šè¿”ã‚Šã€å¾—ã‚‰ã‚ŒãŸå­¦ã³ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚\n",
        "\n",
        "ğŸ¯ **Objective**  \n",
        "The goal of this project was to predict passenger survival on the Titanic using machine learning.  \n",
        "æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„ã¯ã€Titanicä¹—å®¢ã®ç”Ÿå­˜ã‚’æ©Ÿæ¢°å­¦ç¿’ã§äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã—ãŸã€‚\n",
        "\n",
        "ğŸ”§ **Approach**  \n",
        "- Data preprocessing (missing value imputation, feature engineering, encoding)  \n",
        "- Built and tuned base models: RandomForest, XGBoost, LightGBM, and a calibrated SVM  \n",
        "- Combined all models using stacking, with XGBoost as the meta-learner  \n",
        "ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆæ¬ æå€¤è£œå®Œã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ›ï¼‰ã‚’è¡Œã„ã€  \n",
        "RandomForestã€XGBoostã€LightGBMã€SVMãªã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€  \n",
        "ãã‚Œã‚‰ã‚’XGBoostãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "ğŸ“ˆ **Results**  \n",
        "- Final stacking model achieved **83.6% accuracy** and **87.4% ROC AUC**  \n",
        "- SHAP analysis showed that XGBoost and RandomForest had the highest impact  \n",
        "- SVM contributed the least, likely due to its linear nature  \n",
        "æœ€çµ‚çš„ãªã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯**æ­£è§£ç‡83.6%**ã€**ROC AUCã¯87.4%**ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚  \n",
        "SHAPåˆ†æã®çµæœã€XGBoostã¨RandomForestãŒæœ€ã‚‚å¤§ããªå½±éŸ¿ã‚’æŒã¡ã€  \n",
        "ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹SVMã¯ã€è²¢çŒ®åº¦ãŒæ¯”è¼ƒçš„ä½ã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã—ãŸã€‚\n",
        "\n",
        "ğŸ§  **Insights**  \n",
        "- Tree-based models handled complex, non-linear patterns better in the Titanic dataset  \n",
        "- Linear models like SVM may be less effective in such settings  \n",
        "ãƒ„ãƒªãƒ¼ç³»ãƒ¢ãƒ‡ãƒ«ã¯Titanicã®ã‚ˆã†ãªè¤‡é›‘ã§éç·šå½¢ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¼·ãã€  \n",
        "SVMã®ã‚ˆã†ãªç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¯ãã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸å‘ãã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚\n",
        "\n",
        "ğŸš€ **Future Work**  \n",
        "- Improve recall for the minority class (survivors) using techniques like SMOTE or class weighting  \n",
        "- Consider adding more features or optimizing base model selection  \n",
        "å°‘æ•°ã‚¯ãƒ©ã‚¹ï¼ˆç”Ÿå­˜è€…ï¼‰ã®Recallã‚’æ”¹å–„ã™ã‚‹ã«ã¯ã€SMOTEãªã©ã®æ‰‹æ³•ã‚„ã‚¯ãƒ©ã‚¹é‡ã¿ã®èª¿æ•´ãŒæœ‰åŠ¹ã§ã™ã€‚  \n",
        "ã¾ãŸã€ç‰¹å¾´é‡ã®è¿½åŠ ã‚„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é¸å®šã‚„æ§‹æˆã®æœ€é©åŒ–ã‚‚ä»Šå¾Œã®æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚\n",
        "\n",
        "ğŸ§¾ **Summary**  \n",
        "This project demonstrated that stacking ensemble modelsâ€”combined with SHAP for interpretabilityâ€”can achieve strong predictive performance on classification tasks like Titanic survival.  \n",
        "æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã¨SHAPã«ã‚ˆã‚‹èª¬æ˜å¯èƒ½æ€§ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€Titanicç”Ÿå­˜äºˆæ¸¬ã®ã‚ˆã†ãªåˆ†é¡ã‚¿ã‚¹ã‚¯ã§é«˜ç²¾åº¦ãªäºˆæ¸¬ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚\n"
      ],
      "metadata": {
        "id": "ALvwY2Sz-Qjm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}