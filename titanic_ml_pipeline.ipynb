{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNhjfvf9ryWT"
      },
      "source": [
        "# Titanic ML Pipeline: From Preprocessing to Model Evaluation  \n",
        "## タイタニック機械学習パイプライン：前処理からモデル評価まで  \n",
        "\n",
        "### 🧭 Introduction / はじめに  \n",
        "The Titanic dataset is a classic introductory dataset for binary classification tasks in machine learning.  \n",
        "In this notebook, I build a complete machine learning pipeline using multiple models (e.g., RandomForest, XGBoost, SVM), perform hyperparameter tuning, and analyze model interpretability with SHAP.\n",
        "\n",
        "The final stacking model (with XGBoost as meta learner) achieved 83.6% accuracy and 87.4% ROC AUC in cross-validation.  \n",
        "All steps are explained in both English and Japanese to make the content more accessible and educational.\n",
        "\n",
        "---\n",
        "\n",
        "タイタニック号のデータセットは、機械学習のバイナリ分類問題として有名な入門用課題です。  \n",
        "このノートブックでは、ランダムフォレスト・XGBoost・SVM など複数のモデルを使って機械学習パイプラインを構築し、チューニングとSHAPを用いた解釈も行います。\n",
        "\n",
        "最終的なスタッキングモデル（XGBoostをメタ学習器として使用）は、交差検証にて正解率83.6%、ROC AUC 87.4%を達成しました。  \n",
        "すべてのステップには英語と日本語の説明を加えており、学習用・ポートフォリオ用の両方に役立つ内容となっています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0O4IP3Zczdc"
      },
      "source": [
        "## 🎯 Objective  \n",
        "- Predict passenger survival on the Titanic dataset  \n",
        "タイタニック号の乗客の生存予測モデルを構築すること\n",
        "\n",
        "- Build a reproducible and interpretable ML pipeline  \n",
        "再現性・説明可能性の高い機械学習パイプラインを実装すること\n",
        "---\n",
        "\n",
        "## 📝 Contents  \n",
        "1. 📥 Data Loading & Initial Exploration / データ読み込みと初期確認\n",
        "2. 🔀 Data Preprocessing / データの前処理  \n",
        "3. ♒ Baseline Modeling / ベースラインモデル構築\n",
        "4. 🛠 Feature Engineering / 特徴量エンジニアリング\n",
        "5. 🤖 Model Training & Tuning / モデル訓練とチューニング\n",
        "6. 📊 Evaluation & Comparison / モデル評価と比較\n",
        "7. 📤 Final Submission / 提出ファイルの作成   \n",
        "8. 🎯 SHAP-Based Model Contribution (Meta Model: XGBoost) / SHAPによるベースモデルの貢献度分析  \n",
        "9. 🔍 Final Model Analysis / 最終モデルの評価と考察  \n",
        "10. 📌 Conclusion / 結論\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Tools & Libraries\n",
        "\n",
        "| Library             | Usage                                   |\n",
        "| ------------------- | --------------------------------------- |\n",
        "| pandas, numpy       | Data preprocessing / データ前処理             |\n",
        "| matplotlib, seaborn | Visualization / 可視化                     |\n",
        "| scikit-learn        | Modeling & Evaluation / モデルと評価          |\n",
        "| xgboost, lightgbm   | High-performance ML algorithms / 高性能モデル |\n",
        "| SHAP                | Model interpretability / モデルの解釈性        |\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Notes  \n",
        "- Feature selection based on top 80% cumulative importance  \n",
        "重要度の累積80%を基準に特徴量選定を行いました\n",
        "\n",
        "- GridSearchCV & RandomizedSearchCV used for hyperparameter tuning  \n",
        "パラメータ最適化にグリッド・ランダムサーチを使用\n",
        "\n",
        "- Developed in Google Colab, dataset loaded from Google Drive  \n",
        "Colab上で作成し、データはGoogle Driveから読み込み\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 Feedback Welcome! / フィードバック歓迎\n",
        "This notebook is a work in progress as I continue improving my skills.\n",
        "ご意見・ご提案があれば、ぜひお寄せください！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ZWgAsqL99T"
      },
      "source": [
        "## 📁 Data Access (Google Colab or Kaggle) / データの読み込み\n",
        "\n",
        "🔍 This notebook automatically detects the environment (Google Colab or Kaggle)  \n",
        "and sets the file path accordingly.  \n",
        "Google Colab users should save the dataset in Google Drive.  \n",
        "Kaggle users should upload it using the \"Add Data\" button.\n",
        "\n",
        "🔍 このノートブックは Google Colab または Kaggle の実行環境を自動判定し、パスを設定します。  \n",
        "Colab を使う場合は Google Drive にデータを保存してください。  \n",
        "Kaggle の場合は「Add Data」からデータセットを追加してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEOkuVTLLWi9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ✅ Detect environment / 実行環境の判定\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# 📁 Set file path depending on environment / 実行環境に応じてパスを設定\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    path = '/content/drive/MyDrive/Colab Notebooks/Kaggle/Titanic/'\n",
        "else:\n",
        "\n",
        "    path = '/kaggle/input/titanic-data/'\n",
        "\n",
        "# 📄 Load datasets / データセットを読み込む\n",
        "df_train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "df_test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "\n",
        "# 🔎 Preview\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf3VVXpk8kmK"
      },
      "outputs": [],
      "source": [
        "# 🔧 Basic Libraries / 基本ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "# 📊 Visualization / 可視化ライブラリのインポート\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ⚙️ Preprocessing & Model Selection / 前処理およびモデル選定関連のライブラリ\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, StratifiedKFold, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.base import clone\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# 🤖 Classification Algorithms / 分類アルゴリズム\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb  # for Booster API if needed\n",
        "\n",
        "# 📈 Evaluation Metrics & Tools / 評価指標およびツール\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.tree import plot_tree\n",
        "import time\n",
        "\n",
        "# 🚫 Suppress Warnings / 警告の非表示設定\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ check data / データの中身を確認します。"
      ],
      "metadata": {
        "id": "j5xk4QLa1mZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9dq35Oq-Ukw"
      },
      "outputs": [],
      "source": [
        "# Check data types and missing values / データと欠損値の確認\n",
        "df.info()\n",
        "\n",
        "# View summary statistics for numerical features / 数値特徴量の要約統計量を表示する\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15tFE13VRLQb"
      },
      "source": [
        "# 1. Data Exploration / データ理解・探索\n",
        "### 1.1 Check Missing Values / 欠損値の確認\n",
        "\n",
        "We check the number of missing values for each feature in both training and test datasets to identify columns that need imputation or special handling.\n",
        "\n",
        "訓練データとテストデータの各特徴量について、欠損値の数を確認します。これにより、補完や特別な処理が必要な列を特定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hyTqjDN_m7k"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UVeugipCS4H"
      },
      "outputs": [],
      "source": [
        "df_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFOKNeEyN4gU"
      },
      "source": [
        "### 🔍 Missing Values / 欠損値の確認\n",
        "\n",
        "We found missing values in the following columns:  \n",
        "以下のカラムに欠損値が見つかりました：\n",
        "\n",
        "- **Age**: Needs to be imputed. → Will fill with the median value.  \n",
        "  **年齢**：補完が必要 → 中央値で補完します。\n",
        "\n",
        "- **Cabin**: Many missing values. → Will drop or ignore this feature. Extract Deck (first letter) → Replace missing with 'U' (Unknown) → One-hot encode Deck → Create binary flag for Cabin presence → Drop original Cabin and Deck columns    \n",
        "  **客室番号**：欠損が非常に多い → 先頭文字を抽出してDeckとして扱う → 欠損は'U'（不明）に置換 → Deckをダミー変数化 → Cabinの有無を示す2値特徴量を作成 → 元のCabinとDeck列を削除\n",
        "\n",
        "- **Embarked**: 2 missing values. → Will fill with the most frequent value (mode).  \n",
        "  **乗船地**：2件の欠損 → 最頻値で補完します。\n",
        "\n",
        "- **Fare (in test set)**: 1 missing value. → Will fill with the median value.  \n",
        "  **運賃（テストデータ）**：1件の欠損 → 中央値で補完します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHw-T_rpNNE_"
      },
      "source": [
        "### 1.2 Visualizing Survival Rate / 生存率の可視化\n",
        "Let's visualize the distribution of survivors and non-survivors in the dataset using both a pie chart and a count plot.\n",
        "\n",
        "円グラフと棒グラフ（カウントプロット）を使って、生存者と非生存者の分布を可視化します。\n",
        "\n",
        "This helps us understand the class balance in the target variable.\n",
        "\n",
        "これにより、目的変数（Survived）のクラスのバランスを把握できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ40HPQxDwjw"
      },
      "outputs": [],
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
        "df['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
        "ax[0].set_title('Survived')\n",
        "ax[0].set_ylabel('')\n",
        "sns.countplot(x = 'Survived',data=df,ax=ax[1])\n",
        "ax[1].set_title('Survived')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymvUT8aDprDo"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと\n",
        "\n",
        "- About **62%** of passengers did not survive, while **38%** survived.  \n",
        "  約 **62%** の乗客が生存できず、**38%** が生存していました。\n",
        "- The dataset is somewhat imbalanced, so using accuracy alone might be misleading.  \n",
        "  データセットはやや不均衡であり、単純な精度だけでは評価に偏りが生じる可能性があります。\n",
        "- Therefore, other metrics like ROC AUC and F1 score should also be considered.  \n",
        "  このため、ROC AUC や F1スコアなどの指標の併用が重要になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_LQpTDR7Bk"
      },
      "source": [
        "## 1.3 Passenger Class Distribution and Survival by Class / 乗客クラスの分布とクラスごとの生存率\n",
        "\n",
        "In this section, we explore how passengers are distributed among the three classes (`Pclass`).  \n",
        "このセクションでは、乗客が3つのクラス（Pclass）にどのように分布しているかを確認します。\n",
        "\n",
        "The bar chart on the left shows the number of passengers in each class.  \n",
        "左の棒グラフは、各クラスに属する乗客の人数を示しています。\n",
        "\n",
        "The count plot on the right breaks down survival status by class, giving us insight into survival trends per passenger class.  \n",
        "右の棒蔵グラフでは、生存状況をクラスごとに分解して表示し、クラスによる生存傾向がわかります。\n",
        "\n",
        "As we can see, passengers in 1st class had a higher chance of survival, while those in 3rd class faced lower survival rates.  \n",
        "ご覧のとおり、1等船室の乗客は生存率が高く、3等船室の乗客は生存率が低い傾向にあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhbzVuMpAcDW"
      },
      "outputs": [],
      "source": [
        "f,ax=plt.subplots(1,2,figsize=(18,8), facecolor='gray')\n",
        "df['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\n",
        "ax[0].set_title('Number of Passengers By Pclass')\n",
        "ax[0].set_ylabel('Count')\n",
        "sns.countplot(x='Pclass',hue='Survived',data=df,ax=ax[1])\n",
        "ax[1].set_title('Pclass:Perished vs Survived')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX-HtIG3zHdJ"
      },
      "source": [
        "## 1.4 Survival Rate by Fare Group within Pclass 3 / 3等船室における運賃グループ別生存率\n",
        "\n",
        "We select passengers in Pclass 3 and group them by fare quartiles to analyze survival rates.  \n",
        "3等船室の乗客を抽出し、運賃の四分位数ごとにグループ分けして生存率を分析します。\n",
        "\n",
        "The survival rates are calculated for each fare group to see if fare impacts survival within Pclass 3.  \n",
        "各運賃グループごとに生存率を計算し、3等船室内で運賃が生存に影響を与えているかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlMY4rgzImZh"
      },
      "outputs": [],
      "source": [
        "# Select Pclass 3 passengers\n",
        "pclass3 = df[df['Pclass'] == 3].copy()\n",
        "\n",
        "# Create fare quartile groups\n",
        "pclass3['FareGroup'] = pd.qcut(pclass3['Fare'], q=4)\n",
        "\n",
        "# Calculate survival rate by FareGroup\n",
        "survival_by_fare = pclass3.groupby('FareGroup')['Survived'].mean()\n",
        "\n",
        "print(survival_by_fare)\n",
        "\n",
        "# Optional: visualize survival rates by fare group\n",
        "survival_by_fare.plot(kind='bar')\n",
        "plt.title('Survival Rate by Fare Group (Pclass 3)')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5wcEZCKv8Q"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと  \n",
        "As shown in the plot, survival rates are similar across fare quartiles,  \n",
        "indicating that fare does not significantly affect survival within Pclass 3.  \n",
        "グラフの通り、運賃の四分位数ごとに生存率はほぼ同じであり、  \n",
        "3等船室内では運賃が生存に大きな影響を与えていないことがわかります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPpCl-IPRzuF"
      },
      "source": [
        "## 1.5 🚻 Survival Rate by Sex within Pclass 3 / 3等船室における性別ごとの生存率  \n",
        "Next, we calculate survival rates by sex among passengers in Pclass 3.  \n",
        "次に、3等船室の乗客について性別ごとの生存率を計算します。\n",
        "\n",
        "This helps us understand how gender influenced survival chances within the lowest passenger class.  \n",
        "これにより、最も下位の乗客クラス内で性別が生存率にどのように影響したかがわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKhbW36hSU71"
      },
      "outputs": [],
      "source": [
        "# Calculate survival rate by sex within Pclass 3\n",
        "survival_by_sex_pclass3 = pclass3.groupby('Sex')['Survived'].mean()\n",
        "print(survival_by_sex_pclass3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTV4X-2mSOGr"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと  \n",
        "The results show that females in Pclass 3 had a significantly higher survival rate compared to males.  \n",
        "結果を見ると、3等船室の女性は男性に比べてかなり高い生存率を持っていることがわかります。\n",
        "\n",
        "This reflects the \"women and children first\" evacuation policy practiced on the Titanic.  \n",
        "これは、タイタニック号で実施された「女性と子供を先に救助する」方針を反映しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hJxDDrXJjEO"
      },
      "source": [
        "## 1.6 🎻 Visualizing Age Distribution by Pclass and Sex (Violin Plot) /        Pclass・性別ごとの年齢分布と生存状況の可視化  \n",
        "We use violin plots to visualize the age distribution of passengers,  \n",
        "split by survival status across different `Pclass` and `Sex` categories.  \n",
        "バイオリンプロットを使って、乗客の年齢分布を生存状況ごとに表示します。  \n",
        "ここでは `Pclass`（乗客クラス）と `Sex`（性別）ごとに分けています。\n",
        "\n",
        "These plots help identify patterns in how age and other factors affect survival.  \n",
        "このプロットによって、年齢や他の要素が生存率にどのように影響したかの傾向を視覚的に把握できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBsPtplpISHh"
      },
      "outputs": [],
      "source": [
        "# Create side-by-side violin plots\n",
        "f, ax = plt.subplots(1, 2, figsize=(18, 8), facecolor='gray')\n",
        "\n",
        "# Pclass × Age × Survived\n",
        "sns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=df, split=True, ax=ax[0])\n",
        "ax[0].set_title('Pclass and Age vs Survived')\n",
        "ax[0].set_yticks(range(0, 110, 10))\n",
        "\n",
        "# Sex × Age × Survived\n",
        "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=df, split=True, ax=ax[1])\n",
        "ax[1].set_title('Sex and Age vs Survived')\n",
        "ax[1].set_yticks(range(0, 110, 10))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne90luXsUVsJ"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと  \n",
        "In both plots, we observe that younger passengers (especially children) had higher survival rates,  \n",
        "particularly in 1st and 2nd class, and among females.  \n",
        "どちらのプロットでも、特に1等・2等船室や女性の中で、子ども（低年齢層）の生存率が高い傾向が見られます。\n",
        "\n",
        "On the other hand, older male passengers in 3rd class had lower survival rates.  \n",
        "一方で、3等船室の年配男性の生存率は低いことがわかります。\n",
        "\n",
        "This visualization supports previous findings and reinforces the influence of class, gender, and age.  \n",
        "この可視化は、これまでの分析結果を裏付け、クラス・性別・年齢の影響を再確認するものです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Lz5zBMVAYJ"
      },
      "source": [
        "We plot the average survival rate for each gender using a bar chart.  \n",
        "性別ごとの平均生存率を棒グラフで可視化します。\n",
        "\n",
        "This clearly shows that female passengers had a much higher survival rate than males.  \n",
        "このグラフから、女性の方が生存率がはるかに高いことが明確にわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7FWqLbEN-B_"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='Sex', y='Survived', data=df)\n",
        "plt.title(\"Survival Rate by Sex\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2t7uHqaUZs"
      },
      "source": [
        "## 1.7 👶 Survival Rate of Children by Sex /                                   子ども（10歳未満）の生存率を性別ごとに確認  \n",
        "We define a new column `Child` that marks passengers under 10 years old as 1, and others as 0.  \n",
        "10歳未満の乗客を「子ども（Child = 1）」とし、それ以外を0として新しい列を作成します。\n",
        "\n",
        "Then, we create a normalized crosstab to compare survival rates by `Child` and `Sex`.  \n",
        "次に、`Child`と`Sex`の組み合わせで、生存率をクロス集計（正規化）します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MKfsy_uQCAs"
      },
      "outputs": [],
      "source": [
        "# 子どもフラグを追加し、Child × Sex ごとの生存率をクロス集計\n",
        "df.assign(Child = df['Age'].apply(lambda x: 1 if x < 10 else 0)) \\\n",
        "  .pipe(lambda d: pd.crosstab([d['Child'], d['Sex']], d['Survived'], normalize='index'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7FM9s82WUbm"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと  \n",
        "- **Adult women (Child = 0, female)** had the **highest survival rate** (75.4%), consistent with the \"ladies first\" policy.  \n",
        "  （大人の女性は最も高い生存率を持っており、これは「女性優先」の原則と一致しています。）\n",
        "\n",
        "- **Adult men (Child = 0, male)** had the **lowest survival rate** (16.5%), as expected.  \n",
        "  （大人の男性の生存率は最も低く、予想通りの結果です。）\n",
        "\n",
        "- **Child males** had a significantly **higher survival rate (59.4%)** than adult males.  \n",
        "  （子どもの男性は大人の男性よりも大幅に高い生存率を示しています。）\n",
        "\n",
        "- Interestingly, **child females** had a lower survival rate (63.3%) than **adult females** (75.4%).  \n",
        "  （興味深いことに、女の子の生存率は大人の女性よりもやや低くなっています。）\n",
        "\n",
        "This may be due to factors such as the location of cabins, family groups, or evacuation priority.  \n",
        "（これは、キャビンの場所、家族単位の移動、避難の優先順位などが影響している可能性があります。）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My2wYFm8ZH0V"
      },
      "source": [
        "## 1.8 🔥 Correlation Heatmap of Numerical Features / 数値特徴量の相関関係ヒートマップ  \n",
        "We plot a correlation heatmap of all numerical features in the dataset.  \n",
        "データセット内の数値型特徴量について、相関関係のヒートマップを描画します。\n",
        "\n",
        "This helps identify which features are strongly related to each other or to the target variable (`Survived`).  \n",
        "これにより、特徴量間、または目的変数（`Survived`）との強い関係があるものを見つけることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPw58i9QSBWg"
      },
      "outputs": [],
      "source": [
        "# Select numeric columns only\n",
        "df_numeric = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Plot heatmap of correlations\n",
        "sns.heatmap(df_numeric.corr(), annot=True, cmap='bwr', linewidths=0.2)\n",
        "\n",
        "# Set figure size\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dFKrWAdaNzf"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと  \n",
        "- `Fare` shows a moderate positive correlation with `Survived`.  \n",
        "  `Fare`（運賃）は `Survived` と中程度の正の相関を持っています。\n",
        "\n",
        "- `Pclass` has a negative correlation with `Survived`, indicating that higher classes had better survival chances.  \n",
        "  `Pclass` は `Survived` と負の相関があり、高いクラス（1等）が生存しやすかったことを示しています。\n",
        "\n",
        "- Some features like `SibSp` and `Parch` also have weak correlations, which might still be useful in combination.  \n",
        "  `SibSp` や `Parch` のような特徴量も弱い相関を示しますが、他の特徴と組み合わせると有効な場合があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsLwvPplgn4C"
      },
      "source": [
        "# 2 🧼 Data Preprocessing / データの前処理\n",
        "\n",
        "Before training models, we need to clean and preprocess the dataset.  \n",
        "This includes handling missing values, encoding categorical variables, and standardizing formats.  \n",
        "モデルを訓練する前に、データセットの欠損値処理、カテゴリ変数のエンコーディング、フォーマットの統一などを行います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_m1oSXHaqvJ"
      },
      "source": [
        "📦 Backup the Preprocessed Data / 前処理済みデータのバックアップ  \n",
        "We create backup copies of the preprocessed train and test datasets.  \n",
        "前処理済みの訓練データとテストデータをバックアップとして保存しておきます。\n",
        "\n",
        "This is useful when experimenting with feature engineering or trying different preprocessing strategies,  \n",
        "as we can always revert to the original cleaned state.  \n",
        "特徴量エンジニアリングや前処理方法を試す際に、元の状態に戻せるので便利です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPH8H2Ixaftg"
      },
      "outputs": [],
      "source": [
        "df_base = df.copy() # baseline\n",
        "df_base_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrPd67x-auUV"
      },
      "source": [
        "We create backup copies of the preprocessed train and test datasets.  \n",
        "前処理済みの訓練データとテストデータをバックアップとして保存しておきます。\n",
        "\n",
        "This is useful when experimenting with feature engineering or trying different preprocessing strategies,  \n",
        "as we can always revert to the original cleaned state.  \n",
        "特徴量エンジニアリングや前処理方法を試す際に、元の状態に戻せるので便利です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5p0uC1-6RlE"
      },
      "source": [
        "## 2.1 🧓 Age Imputation / Ageの補完\n",
        "\n",
        "`Age` has a relatively large number of missing values.  \n",
        "To ensure consistency between training and test data, we will fill the missing values using the **median age of the combined train and test datasets**.\n",
        "\n",
        "`Age` には比較的多くの欠損値があります。  \n",
        "訓練データとテストデータで一貫性を保つために、**訓練＋テスト全体の年齢の中央値**を使って補完します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF3uDUhgTFij"
      },
      "outputs": [],
      "source": [
        "# Combine train and test Age columns to calculate a consistent median\n",
        "age = pd.concat([df_base['Age'],df_base_test['Age']])\n",
        "\n",
        "# Fill missing Age values in both datasets with the same median\n",
        "df_base['Age'] = df_base['Age'].fillna(age.median())\n",
        "df_base_test['Age'] = df_base_test['Age'].fillna(age.median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg_7zBe_6tVZ"
      },
      "source": [
        "## 2.2 💰 Fare Imputation (Test Data) / Fareの補完（テストデータ）\n",
        "\n",
        "The test dataset contains one missing value in the `Fare` column.  \n",
        "Instead of using the overall median, we calculate the median `Fare` by passenger class (`Pclass`) and fill the missing value with the median of the corresponding class.\n",
        "\n",
        "テストデータには `Fare` の欠損値が1件含まれています。  \n",
        "単純に全体の中央値で補完するのではなく、乗客の等級（`Pclass`）ごとに `Fare` の中央値を計算し、  \n",
        "該当する `Pclass` の中央値で補完することで、より妥当な推定を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFmt5VqjmAuP"
      },
      "outputs": [],
      "source": [
        "# Calculate the median Fare for each Pclass from the training data\n",
        "# 訓練データから Pclass ごとの Fare の中央値を計算\n",
        "fare_per_class = df_base.groupby('Pclass')['Fare'].median()\n",
        "\n",
        "# Apply class-specific median to fill missing Fare in the test data\n",
        "# テストデータの欠損値に対して、対応する Pclass の中央値で補完\n",
        "df_base_test['Fare'] = df_base_test.apply(\n",
        "    lambda row: fare_per_class[row['Pclass']] if pd.isnull(row['Fare']) else row['Fare'],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-cB5Z0J7BTV"
      },
      "source": [
        "## 2.3 🛳️ Cabin Feature Engineering / Cabin情報の特徴量変換\n",
        "\n",
        "The `Cabin` column contains many missing values and detailed cabin numbers, which are difficult to use directly.  \n",
        "We extract the first letter of each cabin as a new feature called `Deck`, since it may relate to passenger class or location on the ship.  \n",
        "We also create a binary feature `Has_Cabin` that indicates whether cabin information was available.\n",
        "\n",
        "`Cabin` 列には非常に多くの欠損があり、詳細なキャビン番号のままでは使用が難しいです。  \n",
        "そのため、キャビンの最初の文字を `Deck` という新しい特徴量として抽出しました。これは、乗客の等級や船内での位置に関係している可能性があります。  \n",
        "また、`Cabin` 情報の有無を示す2値の特徴量 `Has_Cabin` も作成しました（1: あり、0: なし）。\n",
        "\n",
        "元の `Cabin` 列は使用しないため削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3eEIS_-YmFp"
      },
      "outputs": [],
      "source": [
        "# Extract the first letter from Cabin to represent Deck\n",
        "# キャビンの先頭文字を取り出してDeckとして扱う。欠損は'U'（Unknown）に。\n",
        "df_base['Deck'] = df_base['Cabin'].str[0].fillna('U')\n",
        "df_base_test['Deck'] = df_base_test['Cabin'].str[0].fillna('U')\n",
        "\n",
        "# Replace rare 'T' deck with 'U' (Unknown)\n",
        "# 稀に存在する 'T' は情報が少ないため 'U' に統合\n",
        "df_base['Deck'] = df_base['Deck'].replace('T', 'U')\n",
        "df_base_test['Deck'] = df_base_test['Deck'].replace('T', 'U')\n",
        "\n",
        "# Visualize survival rate by Deck\n",
        "# Deckごとの生存率を表示して可視化\n",
        "deck_survival = df_base.groupby('Deck')['Survived'].mean().sort_values().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=deck_survival, x='Deck', y='Survived', palette='viridis')\n",
        "plt.title('Survival Rate by Deck')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.xlabel('Deck')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWy3hf6CtPq_"
      },
      "outputs": [],
      "source": [
        "# Combine train and test Deck columns to create dummy variables\n",
        "# 訓練データとテストデータのDeck列を連結し、ダミー変数化\n",
        "all_decks = pd.concat([df_base['Deck'], df_base_test['Deck']], axis=0)\n",
        "decks_ohe = pd.get_dummies(all_decks, prefix='Deck').astype(int)\n",
        "\n",
        "# Split back to train and test datasets and concatenate\n",
        "# 元の行数で分割して、それぞれのデータフレームに結合\n",
        "df_base = pd.concat([\n",
        "    df_base.reset_index(drop=True),\n",
        "    decks_ohe.iloc[:len(df_base)].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "df_base_test = pd.concat([\n",
        "    df_base_test.reset_index(drop=True),\n",
        "    decks_ohe.iloc[len(df_base):].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "# Create a binary feature indicating whether Cabin info is available\n",
        "# Cabin情報があったかどうかの2値特徴量（1: あり, 0: なし）\n",
        "df_base['Has_Cabin'] = df_base['Cabin'].notnull().astype(int)\n",
        "df_base_test['Has_Cabin'] = df_base_test['Cabin'].notnull().astype(int)\n",
        "\n",
        "# Drop the original Cabin and Deck columns as they are no longer needed\n",
        "# 元の Cabin と Deck の列は不要なので削除\n",
        "df_base.drop(['Cabin', 'Deck'], axis=1, inplace=True)\n",
        "df_base_test.drop(['Cabin', 'Deck'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR7lxUv97PI7"
      },
      "source": [
        "## 2.5 ⚓ Embarked Imputation / Embarkedの欠損補完\n",
        "\n",
        "The `Embarked` column has a few missing values.  \n",
        "Since most passengers embarked from 'S' (Southampton),  \n",
        "we fill missing values with 'S', the most frequent port.\n",
        "\n",
        "`Embarked` 列には欠損値がいくつかあります。  \n",
        "乗客の大多数が 'S'（サウサンプトン）から乗船しているため、  \n",
        "欠損値は最頻値である 'S' で補完します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y31AymFQTeYT"
      },
      "source": [
        "### 2.5.1 ⚓ Visualization of Embarked Port Counts / Embarkedの乗船人数の可視化\n",
        "\n",
        "This plot shows the distribution of passengers by their port of embarkation.  \n",
        "It helps us understand the class imbalance and supports the decision to impute missing values with the most frequent port.\n",
        "\n",
        "乗船地ごとの乗客数を示したグラフです。  \n",
        "クラスの偏りを把握でき、欠損値を最頻値で補完する根拠の一つになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6ZeW4qNrPo0"
      },
      "outputs": [],
      "source": [
        "# Visualize the count of passengers by Embarked port\n",
        "# Embarkedごとの乗船人数をカウントプロットで表示\n",
        "sns.countplot(x='Embarked', data=df_base)\n",
        "plt.title('Number of Passengers by Embarked Port')\n",
        "plt.xlabel('Embarked Port')\n",
        "plt.ylabel('Number of Passengers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYj_tWYJV79y"
      },
      "source": [
        "### 2.5.2 Survival Rate by Embarked / Embarkedごとの生存率\n",
        "\n",
        "The following bar plot shows the survival rates grouped by the port of embarkation (`Embarked`).  \n",
        "Among the three main embarkation points — Southampton (S), Cherbourg (C), and Queenstown (Q) —  \n",
        "passengers who boarded at Cherbourg (C) had the highest survival rate.\n",
        "\n",
        "下の棒グラフは、乗船地 (`Embarked`) ごとの生存率を示しています。  \n",
        "3つの主な乗船地、サウサンプトン (S)、シェルブール (C)、クイーンスタウン (Q) の中で、  \n",
        "シェルブール (C) から乗船した乗客が最も高い生存率を示しました。\n",
        "\n",
        "This indicates that embarkation point might be related to survival outcomes,  \n",
        "possibly reflecting differences in passenger demographics or ticket classes.\n",
        "\n",
        "このことは、乗船地が生存結果に関係している可能性があり、  \n",
        "乗客の属性やチケットクラスの違いを反映しているかもしれません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL5U1EOxruHL"
      },
      "outputs": [],
      "source": [
        "# Visualize survival rate by Embarked port\n",
        "# Embarkedごとの生存率を棒グラフで可視化\n",
        "sns.barplot(x='Embarked', y='Survived', data=df_base)\n",
        "plt.title('Survival Rate by Embarked')\n",
        "plt.ylabel('Survival Rate')\n",
        "plt.xlabel('Embarked Port')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic57czfmtnsl"
      },
      "outputs": [],
      "source": [
        "# Fill missing Embarked values with the most frequent value 'S'\n",
        "# Embarkedの欠損値を最頻値'S'で補完\n",
        "df_base['Embarked'].fillna('S', inplace=True)\n",
        "df_base.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfil7Tgw1m4S"
      },
      "outputs": [],
      "source": [
        "df_base_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kadE02gT4ES3"
      },
      "source": [
        "## 2.6 Categorical Data Preparation / カテゴリカルデータの準備\n",
        "\n",
        "We have completed handling all missing values.  \n",
        "Next, we will prepare categorical data so that models can handle them effectively.  \n",
        "\n",
        "これで全ての欠損値処理が完了しました。  \n",
        "次は、モデルが扱いやすいようにカテゴリカルデータの処理を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjX4M-OMbJo2"
      },
      "source": [
        "### 2.6.1 Dropping Complex Text Features: Name and Ticket /  Name と Ticket の複雑な文字列特徴量を削除  \n",
        "The categorical columns include `Name`, `Sex`, `Ticket`, `Embarked`, and `Pclass`.  \n",
        "As a baseline, we will start by simply dropping the `Name` and `Ticket` columns since they are complex and not immediately useful.\n",
        "\n",
        "カテゴリカル変数は `Name`, `Sex`, `Ticket`, `Embarked`, `Pclass` があります。  \n",
        "今回はベースラインとして、複雑な `Name` と `Ticket` は簡単に削除します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzdOizhN7z98"
      },
      "outputs": [],
      "source": [
        "# Drop 'Name' column (too detailed for baseline model)\n",
        "# 'Name'列は複雑なのでベースラインでは削除\n",
        "df_base.drop(['Name'], axis=1, inplace=True)\n",
        "df_base_test.drop(['Name'], axis=1, inplace=True)\n",
        "\n",
        "# Drop 'Ticket' column (too noisy for now)\n",
        "# 'Ticket'列もベースラインでは情報が不明瞭なため削除\n",
        "df_base.drop(['Ticket'], axis=1, inplace=True)\n",
        "df_base_test.drop(['Ticket'], axis=1, inplace=True)\n",
        "\n",
        "# Check remaining columns after drop\n",
        "# 削除後のカラムを確認\n",
        "df_base.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMlTumtjwAey"
      },
      "source": [
        "### 2.6.2 Encoding Sex Column / Sex列のエンコーディング\n",
        "\n",
        "The `Sex` column is a binary categorical variable.  \n",
        "We convert it into numeric format by mapping `'male'` to `0` and `'female'` to `1`.  \n",
        "\n",
        "This allows models to interpret the gender information as numerical input.\n",
        "\n",
        "`Sex` 列は2値のカテゴリ変数です。  \n",
        "`'male'` を `0`、`'female'` を `1` に変換することで、モデルが性別情報を数値として扱えるようにします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGs2mAj-syBJ"
      },
      "outputs": [],
      "source": [
        "df_base['Sex'] = df_base['Sex'].map( {'male':0, 'female':1})\n",
        "df_base_test['Sex'] = df_base_test['Sex'].map( {'male':0, 'female':1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNUzP6buW375"
      },
      "source": [
        "### 2.6.3 One-Hot Encoding for Embarked / Embarked列のOne-Hotエンコーディング  \n",
        "The `Embarked` column contains three categories: `'S'`, `'C'`, and `'Q'`.  \n",
        "To convert this categorical data into a format usable by machine learning models,  \n",
        "we apply One-Hot Encoding and create separate binary columns for each port.\n",
        "\n",
        "`Embarked` 列には `'S'`, `'C'`, `'Q'` の3つのカテゴリがあります。  \n",
        "このカテゴリ変数を機械学習モデルで扱いやすくするため、  \n",
        "One-Hot Encoding を適用し、それぞれの港に対応したバイナリ列を作成します。\n",
        "\n",
        "We concatenate the training and test data before encoding to ensure consistency in the column order and structure.  \n",
        "Then we split them back and merge with the original datasets.  \n",
        "Finally, we drop the original `Embarked` column.\n",
        "\n",
        "エンコーディングの前に訓練データとテストデータを連結することで、列の順序と構造の一貫性を保ちます。  \n",
        "その後、元のデータセットに分割して結合し、`Embarked` 列は削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_k7PEkUX3Ri"
      },
      "outputs": [],
      "source": [
        "# Combine Embarked column from train and test to ensure consistent encoding\n",
        "# 訓練・テストデータのEmbarked列を結合して、One-Hotの整合性を保つ\n",
        "embarked = pd.concat([df_base['Embarked'], df_base_test['Embarked']])\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "# One-Hotエンコーディングを適用\n",
        "embarked_ohe = pd.get_dummies(embarked).astype(int)\n",
        "\n",
        "# Split back into train and test\n",
        "# 訓練用とテスト用に分割\n",
        "embarked_ohe_train = embarked_ohe[:len(df_base)]\n",
        "embarked_ohe_test = embarked_ohe[len(df_base):]\n",
        "\n",
        "# Add the encoded columns to the datasets\n",
        "# エンコーディング結果を元データに結合\n",
        "df_base = pd.concat([df_base, embarked_ohe_train], axis=1)\n",
        "df_base_test = pd.concat([df_base_test, embarked_ohe_test], axis=1)\n",
        "\n",
        "# Drop the original Embarked column\n",
        "# 元のEmbarked列を削除\n",
        "df_base.drop(columns=['Embarked'], inplace=True)\n",
        "df_base_test.drop(columns=['Embarked'], inplace=True)\n",
        "\n",
        "# Preview the result\n",
        "# 処理後の先頭データを表示\n",
        "df_base.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgZhK8kfESNk"
      },
      "source": [
        "### 2.6.4 One-Hot Encoding for Pclass / Pclass列のOne-Hotエンコーディング  \n",
        "Although `Pclass` is already numeric (1, 2, 3),  \n",
        "it is actually a categorical feature representing socio-economic class, not a continuous value.  \n",
        "\n",
        "Therefore, we apply One-Hot Encoding to prevent the model from interpreting it as ordinal.\n",
        "\n",
        "`Pclass` はすでに数値（1, 2, 3）として表現されていますが、  \n",
        "これは連続値ではなく社会的階級を示すカテゴリ変数です。\n",
        "\n",
        "そのため、モデルが誤って順序的な意味を持つと判断しないように、One-Hot Encoding を適用します。\n",
        "\n",
        "As before, we concatenate train and test data first, encode, and then split them back.  \n",
        "The original `Pclass` column is removed after encoding.\n",
        "\n",
        "これまでと同様に、訓練データとテストデータを連結してエンコーディングを行い、  \n",
        "その後分割して元のデータに結合します。エンコード後、`Pclass` 列は削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V54VnlQQEU_-"
      },
      "outputs": [],
      "source": [
        "# Combine Pclass from train and test to ensure consistent encoding\n",
        "# 訓練・テストのPclass列を結合して、One-Hotの整合性を保つ\n",
        "pclass = pd.concat([df_base['Pclass'], df_base_test['Pclass']])\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "# One-Hotエンコーディングを適用（prefix付き）\n",
        "pclass_ohe = pd.get_dummies(pclass, prefix='Pclass').astype(int)\n",
        "\n",
        "# Split back into train and test\n",
        "# 訓練用とテスト用に分割\n",
        "pclass_ohe_train = pclass_ohe[:len(df_base)]\n",
        "pclass_ohe_test = pclass_ohe[len(df_base):]\n",
        "\n",
        "# Add encoded columns to original datasets\n",
        "# エンコード結果を元データに追加\n",
        "df_base = pd.concat([df_base, pclass_ohe_train], axis=1)\n",
        "df_base_test = pd.concat([df_base_test, pclass_ohe_test], axis=1)\n",
        "\n",
        "# Drop original Pclass column\n",
        "# 元のPclass列を削除\n",
        "df_base.drop(columns=['Pclass'], inplace=True)\n",
        "df_base_test.drop(columns=['Pclass'], inplace=True)\n",
        "\n",
        "# Check resulting columns\n",
        "# 結果のカラム一覧を確認\n",
        "df_base.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDzQwFpZ8XhC"
      },
      "source": [
        "# 3. Baseline Modeling / ベースラインモデル構築\n",
        "\n",
        "## 🤖 Preparing for Model Training / モデル構築に向けて\n",
        "\n",
        "Through the preprocessing steps so far, all features have been converted into numeric values,  \n",
        "and missing values have been appropriately handled.\n",
        "\n",
        "ここまでの前処理により、全ての特徴量は数値化され、欠損値も適切に補完されました。  \n",
        "この状態であれば、機械学習モデルにそのまま入力することが可能です。\n",
        "\n",
        "---\n",
        "\n",
        "In this section, we will start building a baseline model using Logistic Regression.  \n",
        "To do so, we will first split the training data into training and validation sets.\n",
        "\n",
        "このセクションでは、ロジスティック回帰を使ってベースラインモデルを構築していきます。  \n",
        "まずは、訓練データを学習用と検証用に分割します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cht2sh_5ECxw"
      },
      "source": [
        "## 📂 3.1 Preparing the Data / データの準備\n",
        "\n",
        "We first separate the training data into features (`X`) and the target variable (`y`).  \n",
        "The target column is `Survived`, which we want to predict.  \n",
        "Therefore, we remove `Survived` from the feature set and assign it to `y`.\n",
        "\n",
        "まず、訓練データを特徴量 (`X`) と目的変数 (`y`) に分けます。  \n",
        "`Survived` は予測対象の変数であるため、`X` からは除外し、`y` に格納します。\n",
        "\n",
        "## 🧪 3.2 Splitting the Data and Training a Baseline Model / 訓練データの分割とベースラインモデルの学習\n",
        "\n",
        "We remove unnecessary columns such as `PassengerId` from the feature set.  \n",
        "Then we split the data into training and validation sets (70% train / 30% validation)  \n",
        "to evaluate the model's performance.\n",
        "\n",
        "`PassengerId` など不要な列を除外した後、  \n",
        "訓練データを学習用（70%）と検証用（30%）に分割して、  \n",
        "モデルの性能を適切に評価できるようにします。\n",
        "\n",
        "We use Logistic Regression as our baseline model.  \n",
        "ロジスティック回帰をベースラインモデルとして使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFVvrdoV8aSC"
      },
      "outputs": [],
      "source": [
        "# Feature matrix: drop PassengerId and Survived\n",
        "# 特徴量行列（PassengerIdとSurvivedは除外）\n",
        "X = df_base.drop(columns=['PassengerId', 'Survived'])\n",
        "\n",
        "# Target variable\n",
        "# 目的変数\n",
        "y = df_base['Survived']\n",
        "\n",
        "# Test data (for final prediction)\n",
        "# 提出用のテストデータ（予測専用）\n",
        "X_test = df_base_test.drop(columns=['PassengerId'])\n",
        "\n",
        "# Split training data: 70% train, 30% validation\n",
        "# 訓練データと検証データに分割（7:3）\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "# ロジスティック回帰モデルを初期化\n",
        "lr = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "# モデルを訓練\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn9XiJ3piFkV"
      },
      "source": [
        "## 📊 3.3 Baseline Model Performance / ベースラインモデルの精度評価\n",
        "\n",
        "We evaluate the baseline Logistic Regression model using accuracy on both the training and validation sets.  \n",
        "This gives us a rough idea of how well the model is learning and whether it is overfitting.  \n",
        "Detailed metrics such as precision and recall will be used later when comparing multiple models.\n",
        "\n",
        "ロジスティック回帰モデルを訓練データと検証データの両方で精度（Accuracy）を使って評価します。  \n",
        "このスコアにより、モデルが過学習していないか、おおまかな性能を確認できます。\n",
        "\n",
        "詳細な評価指標（precisionやrecallなど）は、後のモデル比較フェーズで使用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3MuW0F-hBrX"
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance on training and validation sets\n",
        "# 訓練データと検証データでの精度（Accuracy）を表示\n",
        "print('Train Score: {}'.format(round(lr.score(X_train, y_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr.score(X_valid, y_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5zDbt3bQcFr"
      },
      "source": [
        "# 4 🧱 特徴量エンジニアリング / Feature Engineering\n",
        "\n",
        "本セクションでは、モデルの性能向上のために新しい特徴量を作成します。  \n",
        "特に Titanic データでは、「名前」や「家族構成」から意味のある特徴を抽出できます。\n",
        "\n",
        "In this section, we will create new features to improve the model's performance.  \n",
        "In the Titanic dataset, features like \"Name\" and \"Family composition\" often provide useful information.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ6staZ6EfNd"
      },
      "source": [
        "## 4.1 Family Size / 家族人数\n",
        "We create a new feature called Family by adding the values of SibSp (number of siblings/spouses aboard), Parch (number of parents/children aboard), and 1 (the passenger themselves).  \n",
        "SibSp（兄弟・配偶者の同乗人数）とParch（親・子供の同乗人数）、そして本人を足して、新しい特徴量Family（同乗家族人数）を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G67uwRUNE2vE"
      },
      "outputs": [],
      "source": [
        "df_fe = df_base.copy()       # Create a copy for feature engineering (training data) / ベースラインをコピーする\n",
        "df_fe_test = df_base_test.copy()  # Create a copy for feature engineering (test data)\n",
        "\n",
        "# Add new feature 'Family' by summing SibSp, Parch, and 1 (the passenger themselves) / 新しい特徴量　Familyを作成\n",
        "df_fe['Family'] = df_fe['SibSp'] + df_fe['Parch'] + 1\n",
        "df_fe_test['Family'] = df_fe_test['SibSp'] + df_fe_test['Parch'] + 1\n",
        "\n",
        "df_fe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fm9RLsRp32F"
      },
      "source": [
        "### 4.1.1 Baseline model with Family feature using Logistic Regression /  Family特徴量を含めたベースラインモデル（ロジスティック回帰）\n",
        "We will now train a baseline model using Logistic Regression, including the newly added `Family` feature.  \n",
        "We split the dataset into training and validation sets (70:30), then evaluate accuracy.\n",
        "\n",
        "新たに作成した `Family` 特徴量を含めて、ロジスティック回帰でベースラインモデルを構築します。  \n",
        "データセットを訓練データと検証データに 7:3 に分割し、正解率（Accuracy）を評価します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNwktqKvOsIF"
      },
      "outputs": [],
      "source": [
        "X_fe = df_fe.drop(columns=['PassengerId','Survived'])\n",
        "y_fe = df_fe['Survived']\n",
        "\n",
        "X_fe_test =  df_fe_test.drop(columns=['PassengerId'])\n",
        "\n",
        "X_fe_train, X_fe_valid, y_fe_train, y_fe_valid = train_test_split(X_fe, y_fe, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_fe = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe.fit(X_fe_train, y_fe_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(lr_fe.score(X_fe_train, y_fe_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe.score(X_fe_valid, y_fe_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnVN6OhKTY7A"
      },
      "source": [
        "### 4.1.2 Comparison of L1 and L2 Regularization / L1・L2正則化の比較  \n",
        "Next, we apply **L1 and L2 regularization** to logistic regression and compare their performance.  \n",
        "L1 regularization helps with feature selection by pushing some coefficients to zero.  \n",
        "L2 regularization helps prevent overfitting by shrinking all coefficients.\n",
        "\n",
        "次に、ロジスティック回帰に **L1（Lasso）とL2（Ridge）正則化** を適用し、それぞれの精度を比較します。  \n",
        "L1正則化は一部の係数をゼロにすることで、特徴量選択に効果があります。  \n",
        "L2正則化は全ての係数を小さくして、過学習を抑制します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXfq0o9kHNRN"
      },
      "outputs": [],
      "source": [
        "# L1 正則化：penalty='l1', solver='liblinear'\n",
        "score_l1 = cross_val_score(\n",
        "    LogisticRegression(penalty='l1', solver='liblinear', C=0.5, max_iter=200, random_state=42),\n",
        "    X_fe_train, y_fe_train, cv=5\n",
        ").mean()\n",
        "\n",
        "# L2 正則化：penalty='l2', solver='liblinear'\n",
        "score_l2 = cross_val_score(\n",
        "    LogisticRegression(penalty='l2', solver='liblinear', C=0.5, max_iter=200, random_state=42),\n",
        "    X_fe_train, y_fe_train, cv=5\n",
        ").mean()\n",
        "\n",
        "print(f\"L1 score: {score_l1:.3f}\")\n",
        "print(f\"L2 score: {score_l2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5tCPioNT9gv"
      },
      "source": [
        "### 4.1.3 🧮 Checking Feature Coefficients (Importance) in Logistic Regression / 特徴量の係数（重み）確認  \n",
        "Let's now examine the **coefficients of the logistic regression model** to understand which features contribute the most to survival prediction.\n",
        "\n",
        "ここでは、ロジスティック回帰モデルの **係数（重み）** を確認し、どの特徴量が生存予測に寄与しているかを見ていきます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbE7M2jDOifV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Save feature names / 特徴量名を保存\n",
        "feature_names = df_fe.drop(columns=['PassengerId','Survived']).columns\n",
        "\n",
        "# Get model coefficients / モデルの係数を取得\n",
        "coef = lr_fe.coef_[0]\n",
        "\n",
        "# Create DataFrame / 特徴量名と対応付けてDataFrameにする\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coef\n",
        "})\n",
        "\n",
        "# Sort by absolute value / 絶対値の大きい順にソート\n",
        "print(coef_df.sort_values(by='Coefficient', key=abs, ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeW5h7TUUl3u"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと\n",
        "From the sorted coefficients, we can see the most influential features.  \n",
        "In particular:\n",
        "\n",
        "- `Sex` has the largest coefficient → gender is the strongest predictor of survival.\n",
        "- Some deck-related features like `Deck_E`, `Deck_G`, and `Deck_C` also have large weights.\n",
        "- `Family`, the new feature we added, has a small coefficient (≈ -0.11), suggesting its impact is limited in the current model.\n",
        "\n",
        "ソートされた係数から、最も影響の大きい特徴量がわかります。\n",
        "\n",
        "- `Sex`（性別）が最も大きな係数 → 生存率予測に最も強く寄与\n",
        "- `Deck_E` や `Deck_G` など、客室階層の特徴も高い影響度\n",
        "- 新たに追加した `Family` は係数が小さく（約 -0.11）、このモデルでは影響が限定的である可能性\n",
        "---\n",
        "#### ❓ Why is the impact of `Family` small?\n",
        "\n",
        "The `Family` feature may show low importance for several reasons:\n",
        "\n",
        "- The effect of `Family` might be **nonlinear**: both solo travelers and large families may have lower survival rates.\n",
        "- Logistic regression, being a linear model, may not capture such nonlinear effects well.\n",
        "\n",
        "You may consider the following approaches:\n",
        "\n",
        "- Using `FamilySize` bins (e.g., Small, Medium, Large)\n",
        "- Adding interaction terms\n",
        "- Switching to tree-based models (e.g., RandomForest)\n",
        "\n",
        "---\n",
        "\n",
        "#### ❓ `Family` の影響が小さい理由は？\n",
        "\n",
        "`Family` の影響が小さく見えるのは、以下のような理由が考えられます：\n",
        "\n",
        "- `Family` と生存率の関係が **非線形** の可能性があります。例えば、1人旅や大家族では生存率が低い傾向があるかもしれません。\n",
        "- ロジスティック回帰のような線形モデルでは、こうした非線形の影響をうまく捉えられないことがあります。\n",
        "\n",
        "対応策として、以下のような工夫が考えられます：\n",
        "\n",
        "- `FamilySize` をカテゴリ化する（例：1人＝Solo、2–4人＝Medium、5人以上＝Large）\n",
        "- 他の特徴量との交互作用項を追加する\n",
        "- 非線形の関係を捉えやすい木系モデル（例：RandomForest）に切り替える\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulbty8rXY2LS"
      },
      "source": [
        "### 4.1.4  Model with Random Forest Classifier  \n",
        "We now train a **Random Forest classifier** using the same features including `Family`, and compare its performance with logistic regression.\n",
        "Random Forest is a tree-based ensemble model, which can capture **nonlinear relationships and feature interactions** better than logistic regression.\n",
        "This allows features like `Family`, which might have a nonlinear effect, to contribute more effectively.  \n",
        "\n",
        "---\n",
        "\n",
        "ランダムフォレスト分類器を使用して、`Family` を含む特徴量でモデルを学習し、ロジスティック回帰との性能比較を行います。    \n",
        "ランダムフォレストは木構造に基づくアンサンブル学習モデルであり、\n",
        "非線形な関係性や特徴量間の相互作用をロジスティック回帰よりも適切に捉えることができます。    \n",
        "そのため、`Family` のような非線形な影響を持つ特徴量が効果的に働く可能性があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oErV8hIJEzNV"
      },
      "outputs": [],
      "source": [
        "# Train Random Forest / ランダムフォレストの学習\n",
        "rfc_fe = RandomForestClassifier(\n",
        "    max_depth=6, min_samples_leaf=5, n_estimators=100,\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "rfc_fe.fit(X_fe_train, y_fe_train)\n",
        "\n",
        "# Print accuracy / 精度表示\n",
        "print('Train Score: {}'.format(round(rfc_fe.score(X_fe_train, y_fe_train), 3)))\n",
        "print(' Test Score: {}'.format(round(rfc_fe.score(X_fe_valid, y_fe_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xseyvKt9bnt0"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと\n",
        "From the above comparison, we can observe the following:\n",
        "\n",
        "- **L2-regularized logistic regression** showed the best validation performance (CV ≈ 0.812).\n",
        "- **Random Forest** slightly overfits: higher train score (0.859) but lower validation score (0.799).\n",
        "- **L1-regularized logistic regression** performed slightly worse, suggesting that some weak features might be useful.\n",
        "\n",
        "これらの比較から、以下のような示唆が得られます：\n",
        "\n",
        "- **L2正則化ロジスティック回帰** が最も高い検証スコア（CV ≈ 0.812）を記録しました。\n",
        "- **ランダムフォレスト** は学習スコアが高い一方で、検証スコアがやや低下しており、やや過学習気味です。\n",
        "- **L1正則化** の精度はわずかに劣り、一部の弱い特徴も実は有効だった可能性があります。\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVk81SsBwLsW"
      },
      "source": [
        "## 4.2 Extracting Titles from Names / 敬称（Title）の抽出\n",
        "We extract **titles (e.g., Mr, Mrs, Miss)** from the `Name` column using regular expressions.  \n",
        "These titles can represent social status, gender, or age group, and may be useful for prediction.\n",
        "\n",
        "`Name` 列から敬称（例：Mr, Mrs, Missなど）を正規表現を用いて抽出します。  \n",
        "敬称は社会的地位・性別・年齢層などを表す可能性があり、予測に役立つことがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATnbyUUr7mpE"
      },
      "outputs": [],
      "source": [
        "df_fe1 = df_fe.copy() ## + Title features\n",
        "df_fe1_test = df_fe_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgv1vedgVi8_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to extract title / 敬称（Title）を抽出する関数\n",
        "def extract_title(name):\n",
        "    title_search = re.search(r' ([A-Za-z]+)\\.', name)  # Match pattern like \" Mr.\" / 「空白＋英字＋ドット」にマッチさせる\n",
        "    if title_search:\n",
        "        return title_search.group(1)  # Return only the title part / 敬称の部分のみ返す\n",
        "    return \"\"  # Return empty string if no match / 敬称が見つからなかった場合は空文字を返す\n",
        "\n",
        "# Combine train and test data (need Name column) / 訓練・テストデータを結合（Name列が必要）\n",
        "all_name = pd.concat([df, df_test], axis=0)\n",
        "\n",
        "# Apply the function to extract Title from Name / NameからTitleを抽出して新しい列を作成\n",
        "all_name['Title'] = all_name['Name'].apply(extract_title)\n",
        "\n",
        "# Check unique titles extracted / 抽出された敬称のユニーク値を確認\n",
        "print(all_name['Title'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9BZiq5hdd5n"
      },
      "source": [
        "### 4.2.1 🔍 Count Title Frequency / 敬称の出現回数をカウントする\n",
        "We count how frequently each title appears.  \n",
        "This helps us identify **rare titles** that should be grouped into a single category (e.g., 'Rare').\n",
        "\n",
        "各敬称の出現回数を確認することで、  \n",
        "**出現数が少ないレアな敬称** を見つけ、1つのカテゴリ（'Rare'）にまとめるための準備ができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NziVJS3KBHsn"
      },
      "outputs": [],
      "source": [
        "# Count how many times each title appears / 各敬称の出現回数をカウント\n",
        "title_counts = all_name['Title'].value_counts()\n",
        "print(title_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUEHVkrld3GK"
      },
      "source": [
        "### 4.2.2 🎩 Title Feature Cleaning & One-Hot Encoding / 敬称（Title）の整形とエンコーディング  \n",
        "We clean and group the extracted `Title` values from the Name column.  \n",
        "Some titles are equivalent (e.g., 'Mlle' ≈ 'Miss'), and rare titles are grouped as 'Rare' to reduce dimensionality.  \n",
        "Afterward, we perform one-hot encoding and merge them back into the training and test datasets.\n",
        "\n",
        "`Name`列から抽出した`Title`（敬称）を整形し、  \n",
        "同義のものを統合（例：'Mlle' ≈ 'Miss'）し、出現頻度の少ない敬称は 'Rare' にまとめて次元数を減らします。  \n",
        "その後、ワンホットエンコーディングを行い、訓練／テストデータに結合します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0EAiVa42pki"
      },
      "outputs": [],
      "source": [
        "# 一部の敬称を統合\n",
        "all_name['Title'] = all_name['Title'].replace({\n",
        "    'Mlle': 'Miss',\n",
        "    'Ms': 'Miss',\n",
        "    'Mme': 'Mrs'\n",
        "})\n",
        "\n",
        "# 頻度を再計算\n",
        "title_counts = all_name['Title'].value_counts()\n",
        "\n",
        "# 出現頻度が少ない敬称を「Rare」として統合\n",
        "rare_titles = title_counts[title_counts < 10].index       #10未満のインデックスの値（敬称の名前）だけを取り出す\n",
        "\n",
        "all_name['Title'] = all_name['Title'].replace(rare_titles, 'Rare')\n",
        "\n",
        "# 敬称のダミー変数化をし、ダミー変数をTrue/Falseから0/1に変換\n",
        "title_ohe = pd.get_dummies(all_name['Title'], prefix='Title').astype(int)\n",
        "\n",
        "# df_fe1/df_fe1_test に行数を合わせて結合\n",
        "df_fe1 = pd.concat([\n",
        "    df_fe1.reset_index(drop=True),\n",
        "    title_ohe.iloc[:len(df)].reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "df_fe1_test = pd.concat([\n",
        "    df_fe1_test.reset_index(drop=True),\n",
        "    title_ohe.iloc[len(df):].reset_index(drop=True)\n",
        "], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT4BCu4Dfa45"
      },
      "source": [
        "### 4.2.3 🤖 Logistic Regression with Title Feature / 敬称を加えたロジスティック回帰モデル  \n",
        "We now retrain the logistic regression model using the dataset that includes the newly added `Title` features.  \n",
        "This allows us to check whether including titles improves model performance.\n",
        "\n",
        "新たに追加した `Title` 特徴量を含めたデータセットを用いて、ロジスティック回帰モデルを再学習します。  \n",
        "これにより、敬称の追加がモデル性能を改善するかどうかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eyDu0GPgd29"
      },
      "outputs": [],
      "source": [
        "# Separate features and target / 説明変数と目的変数を分割\n",
        "X_fe1 = df_fe1.drop(columns=['PassengerId', 'Survived'])\n",
        "y_fe1 = df_fe1['Survived']\n",
        "\n",
        "X_fe1_test = df_fe1_test.drop(columns=['PassengerId'])\n",
        "\n",
        "# Split into training and validation sets / 訓練・検証データに分割\n",
        "X_fe1_train, X_fe1_valid, y_fe1_train, y_fe1_valid = train_test_split(\n",
        "    X_fe1, y_fe1, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression / ロジスティック回帰モデルの学習\n",
        "lr_fe1 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe1.fit(X_fe1_train, y_fe1_train)\n",
        "\n",
        "# Accuracy on train and validation / 訓練・検証スコアを表示\n",
        "print('Train Score: {}'.format(round(lr_fe1.score(X_fe1_train, y_fe1_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe1.score(X_fe1_valid, y_fe1_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_TlIe-_fxkA"
      },
      "source": [
        "Adding the `Title` feature improved the model's validation score from 0.812 to 0.821.  \n",
        "This suggests that **titles carry meaningful information** related to passenger survival — such as gender, age group, and social status.\n",
        "\n",
        "`Title` 特徴量を追加することで、検証スコアが 0.812 → 0.821 に改善されました。  \n",
        "これは、敬称が性別・年齢層・社会的地位など、**生存に関係する有益な情報を含んでいる**ことを示唆しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPhQkgVBLl_g"
      },
      "source": [
        "### 4.2.4🌲 Random Forest with Title Feature / 敬称を加えたランダムフォレスト  \n",
        "We retrain the Random Forest model using the updated dataset that includes the `Title` features.  \n",
        "This allows us to compare its performance with logistic regression and see whether nonlinear models benefit more from the new features.\n",
        "\n",
        "`Title` を含めた更新済みのデータセットを使って、ランダムフォレストモデルを再学習します。  \n",
        "ロジスティック回帰との比較や、非線形モデルがこの特徴量の恩恵をより受けるかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MMkuCVBCAEW"
      },
      "outputs": [],
      "source": [
        "# Train Random Forest with Title features / Titleを加えたデータでRF学習\n",
        "rfc_fe1 = RandomForestClassifier(\n",
        "    max_depth=6, min_samples_leaf=5,\n",
        "    n_estimators=100, n_jobs=-1, random_state=42\n",
        ")\n",
        "rfc_fe1.fit(X_fe1_train, y_fe1_train)\n",
        "\n",
        "# Accuracy / 精度\n",
        "print('Train Score: {}'.format(round(rfc_fe1.score(X_fe1_train, y_fe1_train), 3)))\n",
        "print(' Test Score: {}'.format(round(rfc_fe1.score(X_fe1_valid, y_fe1_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHc4VvuogNt3"
      },
      "source": [
        "### 🔍 What We found / 見えてきたこと  \n",
        "Both models improved with the addition of the `Title` feature.  \n",
        "Logistic Regression slightly outperforms Random Forest on the validation score (0.821 vs 0.817),  \n",
        "but Random Forest has a higher training score, which may suggest mild overfitting.\n",
        "\n",
        "両モデルとも、`Title` 特徴量の追加によってスコアが改善しました。  \n",
        "検証スコアではロジスティック回帰がわずかに優れています（0.821 vs 0.817）が、  \n",
        "ランダムフォレストは訓練スコアが高く、やや過学習の兆候があるかもしれません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibqvzdqXXzOY"
      },
      "source": [
        "## 4.3 Ticket Feature / チケットの処理\n",
        "We examine the `Ticket` feature to extract useful information.  \n",
        "Ticket numbers often contain prefixes that indicate ticket type or cabin location, which might affect survival rates.\n",
        "\n",
        "`Ticket`（チケット番号）を調査し、そこから有益な情報を抽出します。  \n",
        "チケット番号にはプレフィックス（接頭辞）が含まれていることが多く、これがチケットの種類やキャビンの位置を示し、生存率に影響する可能性があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqxTpsvfSJ1a"
      },
      "outputs": [],
      "source": [
        "df_fe2 = df_fe1.copy() ## + Ticket features\n",
        "df_fe2_test = df_fe1_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-j57jLFkciv"
      },
      "source": [
        "### 4.3.1 Explore Frequent Ticket Values / チケット番号の出現頻度を調査する\n",
        "\n",
        "Some passengers appear to have shared the same ticket number, possibly indicating group bookings (e.g., families or friends).  \n",
        "By checking the most common ticket numbers, we can get a sense of shared bookings or duplicates in the data.\n",
        "\n",
        "複数の乗客が同じチケット番号を持っている場合があり、家族や友人同士のグループ乗船を示唆している可能性があります。  \n",
        "頻出するチケット番号を確認することで、こうしたグループの存在やデータの重複の可能性を把握します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i6nExiWeUF-"
      },
      "outputs": [],
      "source": [
        "# Show the most common Ticket values / チケット番号の頻出値を表示\n",
        "print(df['Ticket'].value_counts().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAXPZN00mLJY"
      },
      "source": [
        "### 4.3.2 🎟️ Create Group Size Feature Based on Ticket / チケット番号からグループサイズ特徴量を作成する\n",
        "\n",
        "Passengers who share the same ticket number may have traveled together as a group (e.g., family or friends).  \n",
        "We compute the number of passengers per ticket and use it to create two new features:\n",
        "- `TicketGroupSize`: the size of the group (capped at 8 to reduce the effect of outliers)\n",
        "- `IsGroup`: binary flag indicating whether the passenger was in a group (size > 1)\n",
        "\n",
        "同じチケット番号を持つ乗客は、家族や友人などのグループで乗船している可能性があります。  \n",
        "そこで、チケットごとの人数を集計し、以下の2つの特徴量を作成します：\n",
        "- `TicketGroupSize`: チケットごとの人数（外れ値対策として最大8に制限）\n",
        "- `IsGroup`: 乗客がグループ（2人以上）に属しているかを示すフラグ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vz0MkNnMvAl"
      },
      "outputs": [],
      "source": [
        "# Combine training and test data to compute ticket frequencies / チケットの出現回数を全体で計算\n",
        "all_data = pd.concat([df, df_test], axis=0)\n",
        "ticket_counts = all_data['Ticket'].value_counts()\n",
        "\n",
        "# Map the ticket count to each passenger / 各乗客にチケットの人数を割り当てる\n",
        "df_fe2['TicketGroupSize'] = df['Ticket'].map(ticket_counts).fillna(1)\n",
        "df_fe2_test['TicketGroupSize'] = df_test['Ticket'].map(ticket_counts).fillna(1)\n",
        "\n",
        "# Limit maximum group size to 8 to avoid outliers / 外れ値の影響を避けるため、最大を8に制限\n",
        "df_fe2['TicketGroupSize'] = df_fe2['TicketGroupSize'].clip(upper=8)\n",
        "df_fe2_test['TicketGroupSize'] = df_fe2_test['TicketGroupSize'].clip(upper=8)\n",
        "\n",
        "# Create binary feature: Is the passenger in a group? / グループに属しているか（2人以上）\n",
        "df_fe2['IsGroup'] = (df_fe2['TicketGroupSize'] > 1).astype(int)\n",
        "df_fe2_test['IsGroup'] = (df_fe2_test['TicketGroupSize'] > 1).astype(int)\n",
        "\n",
        "# Check survival rate by ticket group size / TicketGroupSizeごとの生存率を確認\n",
        "print(df_fe2[['TicketGroupSize', 'Survived']].groupby('TicketGroupSize').mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dou7GkJ2qkyE"
      },
      "source": [
        " ### 📊 4.3.3 Ticket Group Size Distribution / チケットグループサイズの分布  \n",
        " We plot the distribution of `TicketGroupSize` to understand how common different group sizes are among passengers.  \n",
        "This helps us verify whether most people traveled alone or with others.\n",
        "\n",
        "`TicketGroupSize`（チケットを共有する人数）の分布を可視化することで、  \n",
        "乗客が一人旅なのか複数人で行動していたのかの傾向を把握します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPGAdl4cstS5"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of ticket group sizes / チケットグループ人数の分布を可視化\n",
        "df_fe2['TicketGroupSize'].hist(bins=20)\n",
        "plt.title('Ticket Group Size Distribution')  # タイトル\n",
        "plt.xlabel('Group Size')                    # x軸：人数\n",
        "plt.ylabel('Frequency')                     # y軸：頻度\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9wQ2NXcq6R3"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと  \n",
        "The histogram shows that most passengers had a group size of 1 or 2,  \n",
        "suggesting many were traveling alone or with just one companion.  \n",
        "This supports the idea that group-related features might reveal social structures affecting survival.\n",
        "\n",
        "このヒストグラムから、多くの乗客がグループサイズ1〜2であることがわかります。  \n",
        "これは一人旅または少人数での行動が多かったことを示しており、  \n",
        "グループに関する特徴量が生存に関係する社会的構造を示唆している可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGiKJM9dtLmU"
      },
      "source": [
        "### 4.3.4 Logistic Regression with TicketGroupSize and IsGroup /\n",
        "### TicketGroupSize・IsGroup を加えたロジスティック回帰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yuLJUQPSZuH"
      },
      "outputs": [],
      "source": [
        "X_fe2 = df_fe2.drop(columns=['PassengerId','Survived'])\n",
        "y_fe2 = df_fe2['Survived']\n",
        "\n",
        "X_fe2_test =  df_fe2_test.drop(columns=['PassengerId'])\n",
        "\n",
        "X_fe2_train, X_fe2_valid, y_fe2_train, y_fe2_valid = train_test_split(X_fe2, y_fe2, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_fe2 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe2.fit(X_fe2_train, y_fe2_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(lr_fe2.score(X_fe2_train, y_fe2_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe2.score(X_fe2_valid, y_fe2_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdY5UL1ssjIJ"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと\n",
        "Adding `TicketGroupSize` and `IsGroup` did not significantly change the validation score,  \n",
        "but slightly improved the training accuracy.  \n",
        "This suggests these features may provide helpful context, especially when used with other variables like `Title`.\n",
        "\n",
        "`TicketGroupSize` と `IsGroup` を追加しても検証スコアには大きな変化はありませんでしたが、  \n",
        "訓練スコアがわずかに向上しています。  \n",
        "これらの特徴量は単独では影響が小さくても、`Title` など他の特徴量と組み合わせることで、背景情報として役立っている可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpluHVcRE1BS"
      },
      "source": [
        "## 🧩 4.4 Create Age Categories / 年齢をカテゴリ化する  \n",
        "🎯 Why Categorize Age? / なぜ年齢をカテゴリ化するのか？\n",
        "\n",
        "- In models like **logistic regression**, numerical features such as `Age` are interpreted linearly.  \n",
        "  This means the model assumes that survival probability changes steadily with age.\n",
        "\n",
        "- However, in reality, survival rates often vary **non-linearly across age groups** —  \n",
        "  for example, children, young adults, and seniors may have very different outcomes.\n",
        "\n",
        "- By converting age into **categories**, we allow the model to explicitly learn patterns specific to each age group.  \n",
        "  This can improve interpretability and performance, especially for tree-based and linear models.\n",
        "\n",
        "---\n",
        "\n",
        "- ロジスティック回帰のようなモデルでは、`Age` のような数値特徴量は線形的に解釈されます。  \n",
        "  つまり、年齢が上がるほど生存率が一様に変化する、という前提になります。\n",
        "\n",
        "- しかし実際には、生存率は「子ども」「若者」「高齢者」などの **年齢層ごとに非線形に変化** する傾向があります。\n",
        "\n",
        "- 年齢をカテゴリ化することで、モデルは **各年齢層に特有のパターン** を明示的に学習できるようになります。  \n",
        "  特に、決定木系や線形モデルで効果が期待できます。\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjKIZYjswMnP"
      },
      "source": [
        "### 4.4.1👶👨‍🦰🧓 Categorizing Age into Groups / `Age` を年齢層でカテゴリ化\n",
        "\n",
        "We convert the continuous `Age` feature into 3 categories:\n",
        "- `Child` for ages 0–10\n",
        "- `Adult` for ages 10–50\n",
        "- `Senior` for ages 50–100\n",
        "\n",
        "This helps the model understand non-linear survival patterns across different age groups.  \n",
        "We then apply one-hot encoding to make these categories usable in machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "`Age`（年齢）を連続値のまま使うのではなく、以下のようなカテゴリに分けて扱います：\n",
        "\n",
        "- `Child`：0〜10歳  \n",
        "- `Adult`：10〜50歳  \n",
        "- `Senior`：50〜100歳  \n",
        "\n",
        "これは、年齢層ごとに異なる生存率傾向（非線形パターン）をモデルが学習しやすくするためです。  \n",
        "その後、機械学習モデルで利用できるよう、ワンホットエンコーディングで数値化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McbWj9zBoB15"
      },
      "outputs": [],
      "source": [
        "# Copy the latest dataframe / 最新の特徴量付きデータをコピー\n",
        "df_fe3 = df_fe2.copy()  # + AgeGroup features\n",
        "df_fe3_test = df_fe2_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t48SO8DxoiyQ"
      },
      "outputs": [],
      "source": [
        "# Combine Age from train and test to ensure consistent binning\n",
        "# 訓練データとテストデータのAge列を結合し、同じ基準でビン分けできるようにする\n",
        "all_age = pd.concat([df_fe3['Age'], df_fe3_test['Age']], axis=0)\n",
        "\n",
        "# Create age group categories: Child (0–10), Adult (10–50), Senior (50–100)\n",
        "# 年齢をカテゴリに変換：Child（0–10）、Adult（10–50）、Senior（50–100）\n",
        "age_group = pd.cut(all_age, bins=[0, 10, 50, 100], labels=['Child','Adult', 'Senior'])\n",
        "\n",
        "# Assign AgeGroup back to train and test data\n",
        "# AgeGroup を訓練データ・テストデータに戻す\n",
        "df_fe3['AgeGroup'] = age_group[:len(df_fe3)].reset_index(drop=True)\n",
        "df_fe3_test['AgeGroup'] = age_group[len(df_fe3):].reset_index(drop=True)\n",
        "\n",
        "# Combine AgeGroup from both sets and one-hot encode\n",
        "# 両方の AgeGroup を結合してワンホットエンコーディング\n",
        "all_group = pd.concat([df_fe3['AgeGroup'], df_fe3_test['AgeGroup']], axis=0)\n",
        "age_dummies = pd.get_dummies(all_group, columns=['AgeGroup'], prefix='AgeGroup').astype(int)\n",
        "\n",
        "# Add one-hot encoded columns back to original train and test sets\n",
        "# ワンホット化した AgeGroup を元の訓練データ・テストデータに追加\n",
        "df_fe3 = pd.concat([df_fe3.reset_index(drop=True), age_dummies.iloc[:len(df_fe3)].reset_index(drop=True)], axis=1)\n",
        "df_fe3_test = pd.concat([df_fe3_test.reset_index(drop=True), age_dummies.iloc[len(df_fe3):].reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Drop original AgeGroup categorical column\n",
        "# 元のカテゴリ列（AgeGroup）は削除\n",
        "df_fe3.drop(columns=['AgeGroup'], inplace=True)\n",
        "df_fe3_test.drop(columns=['AgeGroup'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIC9Mmgz1U9-"
      },
      "source": [
        "### 4.4.2 Logistic Regression with AgeGroup / Age をカテゴリ化して追加したロジスティック回帰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL5hXUHxZf0w"
      },
      "outputs": [],
      "source": [
        "X_fe3 = df_fe3.drop(columns=['PassengerId','Survived']).values\n",
        "y_fe3 = df_fe3['Survived'].values\n",
        "\n",
        "X_fe3_test =  df_fe3_test.drop(columns=['PassengerId']).values\n",
        "\n",
        "X_fe3_train, X_fe3_valid, y_fe3_train, y_fe3_valid = train_test_split(X_fe3, y_fe3, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_fe3 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe3.fit(X_fe3_train, y_fe3_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(lr_fe3.score(X_fe3_train, y_fe3_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe3.score(X_fe3_valid, y_fe3_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA4Wma3Z1ufi"
      },
      "source": [
        "### 🔍 What We Found / 見えてきたこと\n",
        "We added age group features (Child, Adult, Senior) to capture potential non-linear effects of age on survival.  \n",
        "However, the validation score slightly decreased from 0.821 to 0.817, suggesting the new features did not significantly improve model performance.\n",
        "\n",
        "This may indicate that the model was already capturing the impact of age through other correlated features (e.g., Title or Family size),  \n",
        "or that the age groups chosen were not optimal.\n",
        "\n",
        "---\n",
        "\n",
        "生存率に対する年齢の非線形な影響を捉えるために、Age をカテゴリ化して追加しました（Child、Adult、Senior）。  \n",
        "しかし、検証スコアは 0.821 から 0.817 にわずかに低下し、新しい特徴量がモデル性能を明確に改善したとは言えません。\n",
        "\n",
        "これは、年齢の影響がすでに他の関連特徴（例：TitleやFamily size）で捉えられていた可能性や、  \n",
        "年齢層の区分けが最適でなかった可能性を示唆しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmVMprgwuKKq"
      },
      "source": [
        "## 4.5 💰 Transforming Fare for Stability and Discretization / Fareの安定化と区間化処理\n",
        "\n",
        "The Fare feature has two main challenges: zero values and a heavily skewed distribution.  \n",
        "These issues can negatively affect model training and prediction accuracy.  \n",
        "To solve these, we apply several data transformations.\n",
        "\n",
        "---\n",
        "\n",
        "Fare（運賃）には「0の値」と「右に大きく歪んだ分布」という2つの課題があります。  \n",
        "これらはモデルの学習や予測精度に悪影響を与える可能性があります。  \n",
        "そこで、いくつかのデータ変換を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2gsEhbS6rOz"
      },
      "source": [
        "### 4.5.1 📊 Visualizing Fare Distribution / Fareの分布を可視化\n",
        "\n",
        "We plot the distribution of the original `Fare` values using a histogram with KDE (kernel density estimate).  \n",
        "This helps us understand how skewed the feature is and why a transformation is needed.\n",
        "\n",
        "---\n",
        "\n",
        "元の `Fare` の分布をヒストグラム＋カーネル密度推定（KDE）付きで可視化します。  \n",
        "このグラフから、`Fare` がいかに **右に大きく偏っているか（右裾が長い）** を確認できます。  \n",
        "こうした歪んだ分布は、モデルの学習において特定の値（高額運賃）を過剰に重視させる可能性があるため、後ほど対数変換などの処理が必要になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hWG_Fdi3Y7L"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['Fare'], bins=50, kde=True)\n",
        "plt.title('Distribution of Fare')\n",
        "plt.xlabel('Fare')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRVVBNmJ66MK"
      },
      "source": [
        "### 4.5.2💰 Fare Preprocessing: Handling Skewed and Zero Values / Fareの前処理：歪みと0値の対処\n",
        "\n",
        "To improve model performance, we apply the following preprocessing steps to the `Fare` feature:\n",
        "\n",
        "- **Replace zero fares** with the smallest positive fare in the training data.  \n",
        "  This avoids issues during logarithmic transformation.\n",
        "- **Apply log transformation** using `log1p`, which computes `log(Fare + 1)`.  \n",
        "  This compresses large values and reduces skewness in the distribution.\n",
        "- **Discretize Fare into 4 quantile-based bins** using `pd.qcut`.  \n",
        "  This creates a new feature called `FareBand`, which categorizes passengers based on relative fare levels.\n",
        "\n",
        "---\n",
        "\n",
        "モデル精度の向上のため、`Fare` に以下の前処理を行いました：\n",
        "\n",
        "- **0円のFare** を、訓練データ中の **最小の正のFare値** に置き換えました。  \n",
        "  これにより、対数変換時のエラー（log(0)）を防ぎます。\n",
        "- **対数変換**（`np.log1p`）を行い、`Fare` の大きな値を圧縮して分布の歪みを軽減しました。  \n",
        "  `np.log1p(x)` は `log(x + 1)` を計算するため、0にも対応可能です。\n",
        "- **等頻度の4つの区間（四分位）** に `Fare` を分割し、新たに `FareBand` というカテゴリ特徴量を作成しました。  \n",
        "  これにより、乗客を相対的な運賃レベルで分類できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3F4yvvtuJxh"
      },
      "outputs": [],
      "source": [
        "df_fe4 = df_fe3.copy()  # + Fare features\n",
        "df_fe4_test = df_fe3_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDKrRSCbvjTU"
      },
      "outputs": [],
      "source": [
        "# Replace 0 fare values with the minimum positive fare in the dataset\n",
        "# Fare が 0 の乗客は、データ中の最小の正の運賃で置き換える\n",
        "min_fare = df_fe4[df_fe4['Fare'] > 0]['Fare'].min()\n",
        "\n",
        "df_fe4['Fare'] = df_fe4['Fare'].replace(0, min_fare)\n",
        "df_fe4_test['Fare'] = df_fe4_test['Fare'].replace(0, min_fare)\n",
        "\n",
        "# Apply log transformation (log1p) to reduce skewness and handle outliers\n",
        "# 対数変換（log1p）を適用して、分布の歪みを軽減し、外れ値の影響を抑える\n",
        "df_fe4['Fare_log'] = np.log1p(df_fe4['Fare'])  # log(x + 1) is safe even for x = 0\n",
        "df_fe4_test['Fare_log'] = np.log1p(df_fe4_test['Fare'])\n",
        "\n",
        "# Discretize fare into 4 equal-sized quantile bins (FareBand)\n",
        "# 運賃を4つの等頻度ビン（四分位）に分割し、FareBand というカテゴリ特徴量を作成\n",
        "df_fe4['FareBand'] = pd.qcut(df_fe4['Fare'], q=4, labels=False)\n",
        "df_fe4_test['FareBand'] = pd.qcut(df_fe4_test['Fare'], q=4, labels=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUzCG0wM7a9b"
      },
      "source": [
        "### 4.5.3 🔍 Visualizing Log-Transformed Fare / 対数変換後のFare分布を可視化\n",
        "\n",
        "To check the effect of the log transformation, we plot the histogram of `log1p(Fare)`.  \n",
        "The transformation compresses the extreme values and helps the feature become more normally distributed, which is better for many models such as logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "`log1p(Fare)` による **対数変換の効果を確認**するため、ヒストグラムを描画します。  \n",
        "この変換により、運賃の極端に高い値が圧縮され、分布がより正規分布に近づきます。  \n",
        "これは、ロジスティック回帰のようなモデルにとって有利に働くことが多いです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELTVDz6z3n52"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(np.log1p(df_fe4['Fare']), bins=50, kde=True)\n",
        "plt.title('Log-Transformed Fare Distribution')\n",
        "plt.xlabel('log1p(Fare)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeJIWole7wVj"
      },
      "source": [
        "### 4.5.4 ⚖️ Standardizing Fare / Fare の標準化\n",
        "\n",
        "To prepare for models that are sensitive to feature scales (such as SVM or linear regression),  \n",
        "we apply **Z-score normalization** to the `Fare` feature using `StandardScaler`.\n",
        "\n",
        "---\n",
        "\n",
        "SVM や線形回帰など、スケールに敏感なモデルに対応するため、`Fare` を **Zスコア正規化（標準化）** しました。  \n",
        "`StandardScaler` により、Fare を「平均0・標準偏差1」に変換しています。\n",
        "\n",
        "> ※ ロジスティック回帰も標準化の効果がある程度期待できますが、決定木系モデル（ランダムフォレストなど）には不要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsO5HZdJ4mVF"
      },
      "outputs": [],
      "source": [
        "# Standardization (Z-score normalization) - especially useful for numerical models like SVM or linear models\n",
        "# 標準化（Zスコア正規化） - SVMや線形モデルなど、数値に敏感なモデルに特に有効\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "# 訓練データの平均・標準偏差を学習し、訓練データを標準化する\n",
        "df['Fare_scaled'] = scaler.fit_transform(df[['Fare']])\n",
        "\n",
        "# Apply the same transformation to the test data\n",
        "# テストデータにも同じルール（訓練データと同じ平均・標準偏差）で標準化を適用\n",
        "df_test['Fare_scaled'] = scaler.transform(df_test[['Fare']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZwHHvlm8BZf"
      },
      "source": [
        "### 4.5.5 📊 Logistic Regression with Fare Features / Fare特徴量を含めたロジスティック回帰\n",
        "\n",
        "We train a logistic regression model using all features including:\n",
        "- `Fare_log`: log-transformed fare (to reduce skewness)\n",
        "- `FareBand`: binned fare values\n",
        "- Other engineered features like `Family`, `Title`, and `AgeGroup`\n",
        "\n",
        "The dataset is split into training and validation sets (70/30) to evaluate generalization performance.\n",
        "\n",
        "---\n",
        "\n",
        "`Fare_log`（対数変換後の運賃）、`FareBand`（ビン分割された運賃）などの特徴量を含めてロジスティック回帰モデルを学習しました。  \n",
        "データは 70% 訓練用、30% 検証用 に分割し、汎化性能を評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blhPHwRtue1h"
      },
      "outputs": [],
      "source": [
        "# Prepare training and test features / 訓練データとテストデータを準備\n",
        "X_fe4 = df_fe4.drop(columns=['PassengerId', 'Survived'])  # Features for training\n",
        "y_fe4 = df_fe4['Survived']                                # Target variable\n",
        "\n",
        "X_fe4_test = df_fe4_test.drop(columns=['PassengerId'])    # Test features (no target)\n",
        "\n",
        "# Split training data into train and validation sets / 訓練データを訓練用と検証用に分割\n",
        "X_fe4_train, X_fe4_valid, y_fe4_train, y_fe4_valid = train_test_split(\n",
        "    X_fe4, y_fe4, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression model / ロジスティック回帰モデルの学習\n",
        "lr_fe4 = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_fe4.fit(X_fe4_train, y_fe4_train)\n",
        "\n",
        "# Evaluate model performance / モデルの性能評価\n",
        "print('Train Score: {}'.format(round(lr_fe4.score(X_fe4_train, y_fe4_train), 3)))\n",
        "print(' Test Score: {}'.format(round(lr_fe4.score(X_fe4_valid, y_fe4_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLTHFq2A80P-"
      },
      "source": [
        "### Insights / 考察\n",
        "\n",
        "- Applying `log1p(Fare)` helped reduce the skewness and stabilize the model.\n",
        "- `FareBand` adds interpretability by grouping passengers based on fare range.\n",
        "- Logistic Regression benefits from normalized or transformed continuous features.\n",
        "\n",
        "---\n",
        "\n",
        "- `Fare` の対数変換によって分布の偏りが改善され、モデルの安定性が向上しました。\n",
        "- `FareBand` により運賃に基づくグルーピングが可能になり、**解釈性が高まりました**。\n",
        "- ロジスティック回帰は連続値に敏感なため、今回のような変換は有効です。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8YB-pMg73O6"
      },
      "source": [
        "### 4.5.6 📊 Random Forest with Fare Features / Fare特徴量を含めたランダムフォレスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOUapJwg9LwG"
      },
      "outputs": [],
      "source": [
        "# ランダムフォレスト\n",
        "rfc_fe4 = RandomForestClassifier(max_depth=5, min_samples_leaf=10, n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rfc_fe4.fit(X_fe4_train, y_fe4_train)\n",
        "\n",
        "print('Train Score: {}'.format(round(rfc_fe4.score(X_fe4_train, y_fe4_train), 3)))\n",
        "print(' Test Score: {}'.format(round(rfc_fe4.score(X_fe4_valid, y_fe4_valid), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAVC8Ftz9_Ri"
      },
      "source": [
        "### Insights / 考察  \n",
        "- The Random Forest model, enhanced with engineered fare features, shows balanced performance with similar train and test accuracies. This suggests that the model effectively captures the relationship between fare and survival without overfitting. Limiting the tree depth and minimum samples per leaf helps control model complexity and improves generalization. Including fare bins and log-transformed fare helps the model handle the skewed fare distribution better.  \n",
        "---\n",
        "- Fare特徴量を追加したランダムフォレストモデルは、訓練データと検証データでほぼ同じ精度を示し、過学習が抑えられていることがわかります。木の深さや葉の最小サンプル数を制限することでモデルの複雑さが調整され、汎化性能が向上しています。対数変換や等頻度ビン化されたFareは、偏った運賃分布に対してモデルがうまく対応できるようにしています。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIckPDXsm2oO"
      },
      "source": [
        "### 4.5.6.1 Default Random Forest Model: Cross-Validation Performance  \n",
        "### / デフォルトモデルのクロスバリデーション評価  \n",
        "\n",
        "We evaluate the default Random Forest model using 10-fold Stratified Cross-Validation on the entire training dataset.  \n",
        "This provides baseline metrics (accuracy and ROC AUC) to compare against tuned models later.  \n",
        "\n",
        "全訓練データを使い、10分割の層化交差検証でデフォルトのランダムフォレストモデルを評価します。  \n",
        "これにより、後でチューニングモデルと比較するための基準となる精度とROC AUCの指標を得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNDZKMkwjtS_"
      },
      "outputs": [],
      "source": [
        "# Features and target / 特徴量と目的変数\n",
        "X = df_fe4.drop(columns=['PassengerId', 'Survived'])  # Features / 特徴量\n",
        "y = df_fe4['Survived']                                # Target / 目的変数\n",
        "\n",
        "# Model / モデル（デフォルトのランダムフォレスト）\n",
        "rfc_default = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Cross-validation strategy / クロスバリデーション戦略（層化 KFold）\n",
        "rfc_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "cv_acc_default = cross_val_score(rfc_default, X, y, cv=rfc_cv, scoring='accuracy')\n",
        "cv_auc_default = cross_val_score(rfc_default, X, y, cv=rfc_cv, scoring='roc_auc')\n",
        "\n",
        "print(\"Default Model - Mean CV Accuracy:\", np.mean(cv_acc_default))\n",
        "print(\"Default Model - Mean CV ROC AUC:\", np.mean(cv_auc_default))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYQomFlP_osd"
      },
      "source": [
        "### 4.5.7 Random Forest Hyperparameter Tuning with Randomized Search and Cross-Validation  \n",
        "#### / ランダムフォレストのランダムサーチと交差検証によるハイパーパラメータチューニング\n",
        "\n",
        "- Define a parameter distribution with `max_depth`, `min_samples_leaf`, and other key options.  \n",
        "- Use `RandomizedSearchCV` with 10-fold **Stratified** cross-validation to efficiently search for the best hyperparameters.  \n",
        "- Evaluate models using **accuracy** as the primary metric.  \n",
        "- Fit `RandomizedSearchCV` on the full training data (no `train_test_split`) to maximize data usage.  \n",
        "- Retrieve and display the best hyperparameters and the best cross-validation accuracy.  \n",
        "- Using the best estimator, calculate **mean ROC AUC** and **mean accuracy** via 10-fold CV to assess model stability and generalization.  \n",
        "- This process balances performance and efficiency, making it suitable when feature engineering is still evolving.\n",
        "\n",
        "---\n",
        "\n",
        "- `max_depth` や `min_samples_leaf` などの主要パラメータに対して、ランダムに候補を設定。  \n",
        "- `RandomizedSearchCV` を 10分割の **StratifiedKFold（層化交差検証）** とともに使い、効率的に最適パラメータを探索。  \n",
        "- 評価指標には `accuracy` を使用。  \n",
        "- データを最大限活用するため、`train_test_split` は使わず全データで学習。  \n",
        "- 最適なハイパーパラメータと、交差検証での最高精度（accuracy）を表示。  \n",
        "- ベスト推定器を使い、10分割交差検証による **平均ROC AUC** と **平均Accuracy** を算出し、モデルの安定性を評価。  \n",
        "- 特徴量がまだ変化している段階でも、効率よく汎化性能の高いモデルを得るための適切な方法。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksPIAhGA5svU"
      },
      "outputs": [],
      "source": [
        "# Test data / テストデータ（目的変数はなし）\n",
        "X_test = df_fe4_test.drop(columns=['PassengerId'])\n",
        "\n",
        "# Define parameter distribution for RandomizedSearch / ランダムサーチ用のパラメータ範囲を定義\n",
        "param_dist = {\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'min_samples_leaf': [1, 2, 4, 8]\n",
        "}\n",
        "\n",
        "# Cross-validation strategy / クロスバリデーション戦略（層化 KFold）\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Set up RandomizedSearchCV / ランダムサーチをセットアップ\n",
        "rfc_rs = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,                 # Number of parameter settings sampled / 試すパラメータ数\n",
        "    scoring='accuracy',       # Evaluation metric / 評価指標\n",
        "    cv=cv,                    # Cross-validation strategy / クロスバリデーション手法\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV / ランダムサーチの実行\n",
        "rfc_rs.fit(X, y)\n",
        "\n",
        "# Best parameters and CV score / 最良パラメータとCVスコアを表示\n",
        "print(\"Best Params:\", rfc_rs.best_params_)\n",
        "print(\"Best CV Accuracy:\", rfc_rs.best_score_)\n",
        "\n",
        "# Get best estimator / 最良モデルを取得\n",
        "rf_rs_fe4 = rfc_rs.best_estimator_\n",
        "\n",
        "# Cross-validated performance / クロスバリデーションでの性能評価\n",
        "cv_auc = cross_val_score(rf_rs_fe4, X, y, cv=cv, scoring='roc_auc')\n",
        "cv_acc = cross_val_score(rf_rs_fe4, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"Mean CV ROC AUC:\", np.mean(cv_auc))\n",
        "print(\"Mean CV Accuracy:\", np.mean(cv_acc))\n",
        "\n",
        "# # Create a DataFrame to compare the default and tuned model performances\n",
        "# 比較用のDataFrame作成\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Default Random Forest', 'Tuned Random Forest'],\n",
        "    'Mean CV Accuracy': [cv_acc_default.mean(), cv_acc.mean()],\n",
        "    'Mean CV ROC AUC': [cv_auc_default.mean(), cv_auc.mean()]\n",
        "})\n",
        "\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Ejp3jrqHd7"
      },
      "source": [
        "### 📊 Model Comparison Summary / モデル比較のまとめ\n",
        "| Model                 | Mean CV Accuracy | Mean CV ROC AUC |\n",
        "| --------------------- | ---------------- | --------------- |\n",
        "| Default Random Forest | 0.8125           | 0.8653          |\n",
        "| Tuned Random Forest   | 0.8383           | 0.8749          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMxPxr0_qWsd"
      },
      "source": [
        "### Insights / 考察\n",
        "\n",
        "The tuned Random Forest model outperforms the default model in both accuracy and ROC AUC. This indicates that hyperparameter tuning via RandomizedSearchCV helped improve both classification performance and the model's ability to distinguish between classes. The increase in ROC AUC suggests better ranking of predictions and improved generalization.\n",
        "\n",
        "チューニングされたランダムフォレストは、精度（Accuracy）とROC AUCの両方でデフォルトモデルを上回っています。これは、RandomizedSearchCVによるハイパーパラメータ調整が、分類性能およびクラス識別能力の向上に寄与したことを示します。特にROC AUCの改善は、予測のランク付けや汎化性能の向上を反映しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHn3dHIYCUQ7"
      },
      "source": [
        "## 4.6 Feature Importance Analysis Using Random Forest / ランダムフォレストによる特徴量重要度の分析\n",
        "\n",
        "Feature importance helps us understand which features have the most influence on the model's predictions. By identifying the most important variables, we can:\n",
        "\n",
        "- Gain insights into the underlying data and factors affecting the target variable.\n",
        "- Improve model interpretability and explainability.\n",
        "- Select or engineer better features to enhance model performance.\n",
        "- Potentially reduce the dimensionality of the dataset by removing less important features, leading to simpler and faster models.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "特徴量重要度は、どの特徴量がモデルの予測にどれだけ影響を与えているかを理解するための指標です。重要な特徴量を知ることで、\n",
        "\n",
        "- データの本質や目的変数に影響を与える要因の理解が深まります。\n",
        "- モデルの解釈性や説明力が向上します。\n",
        "- より良い特徴量の選択やエンジニアリングに役立ち、モデル性能を高めることができます。\n",
        "- 重要度の低い特徴量を削除することで、次元削減やモデルの軽量化が可能になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdx1ot84NZGg"
      },
      "outputs": [],
      "source": [
        "# Get feature names excluding 'PassengerId' and 'Survived'\n",
        "# 'PassengerId'と'生存'列は除く特徴量名を取得\n",
        "feature_names = df_fe4.drop(columns=['PassengerId', 'Survived']).columns\n",
        "\n",
        "# Retrieve feature importances from the best fitted Random Forest model\n",
        "# 最適モデルから特徴量の重要度を取得\n",
        "importances = rf_rs_fe4.feature_importances_\n",
        "\n",
        "# Create a DataFrame for features and their importance scores, then sort descending\n",
        "# 特徴量と重要度をまとめたデータフレームを作成し、重要度順にソート\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance as a horizontal bar chart\n",
        "# 水平棒グラフで特徴量の重要度を可視化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Feature Importance')  # X軸ラベル：特徴量の重要度\n",
        "plt.title('Feature Importance (Random Forest)')  # グラフタイトル：ランダムフォレストの特徴量重要度\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show most important feature at top（重要な特徴を上に表示）\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEr1VNMsDyH"
      },
      "source": [
        "###💡 Insights from Feature Importances /  特徴量重要度に基づく考察  \n",
        "\n",
        "According to the Random Forest model, the top contributing features were:  \n",
        "ランダムフォレストによる特徴量重要度では、以下の特徴が特に大きな影響を持つことがわかりました：  \n",
        "\n",
        "1.  Sex  （性別）\n",
        "2.  Title_Mr  （男性の敬称）\n",
        "3.  Fare  （運賃）\n",
        "\n",
        "\n",
        "These results align with known survival patterns on the Titanic:\n",
        "\n",
        "Sex is the most influential feature, with females having significantly higher survival rates.\n",
        "\n",
        "Title_Mr helps identify adult males, who historically had lower survival due to “women and children first” policies.\n",
        "\n",
        "Fare (log-transformed or binned) reflects passenger class and socio-economic status, both of which are closely tied to survival odds.\n",
        "\n",
        "\n",
        "これはタイタニックの生存傾向と整合的です：\n",
        "\n",
        "性別は最も重要で、女性の生存率が明らかに高いことがデータからも示されています。\n",
        "\n",
        "Title_Mrは成人男性を特定でき、\"女性と子供が優先\"という当時の救助方針を反映した結果です。\n",
        "\n",
        "Fare（対数変換またはビン化済み）は、乗客の階級や社会的地位を表しており、生存可能性と強く相関しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWjNj4Jm_jJ0"
      },
      "source": [
        "## 4.7 Decision Tree Visualization (First Tree of Random Forest) /         決定木の可視化（ランダムフォレスト1本目）\n",
        "\n",
        "Below is the structure of the first decision tree within the Random Forest model.  \n",
        "In this tree, **sex** is used as the first splitting condition, indicating its high importance in the model.\n",
        "\n",
        "以下はランダムフォレスト内の1本目の決定木の構造です。  \n",
        "この木では **sex** が最初の分岐条件となっており、モデルにおいて重要な特徴量であることを示しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrQf1ev_9Pq4"
      },
      "outputs": [],
      "source": [
        "# Extract one tree from the random forest and visualize it\n",
        "# ランダムフォレストから1本の決定木を抽出して可視化する\n",
        "\n",
        "estimator = rf_rs_fe4.estimators_[0]  # The first tree in the random forest / ランダムフォレスト内の最初の決定木を取得\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(estimator,\n",
        "          feature_names=df_fe4.drop(columns=['PassengerId', 'Survived']).columns,  # Feature names for display\n",
        "          class_names=[\"Died\", \"Survived\"],  # Class labels\n",
        "          filled=True,  # Color nodes by class purity\n",
        "          max_depth=3,  # Limit depth of visualization for readability\n",
        "          fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoLcFsyavDWw"
      },
      "source": [
        "Note: The features used in the early splits of a single decision tree may not perfectly match the global feature importance rankings. Feature importance is computed across all trees, while the decision tree visualization only represents one tree’s structure.  \n",
        "（注）1本の決定木で最初に使われる特徴量と、ランダムフォレスト全体の重要度上位の特徴量は、必ずしも一致しません。重要度はすべての木を通じて算出される統計的な指標であり、可視化された木はその中の一例にすぎません。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsvSRWBHJ89C"
      },
      "source": [
        "## 4.8 Top 10 Feature Importance Analysis in Random Forest Model / ランダムフォレストモデルにおける上位10特徴量の重要度分析  \n",
        "\n",
        "Understanding which features contribute most to model performance helps in interpretation, feature selection, and further improvements.\n",
        "Here, we extract the top 10 most important features based on the tuned Random Forest model.\n",
        "\n",
        "どの特徴量がモデル性能に最も貢献しているかを把握することで、モデルの解釈性が高まり、特徴量選択や改善に役立ちます。\n",
        "ここでは、チューニング済みのランダムフォレストモデルに基づいて、重要度の高い特徴量トップ10を抽出して可視化します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktF2J2cF1ktz"
      },
      "outputs": [],
      "source": [
        "top_n = 10  # Extract top 10 features\n",
        "top_features = feature_importance_df.head(top_n)\n",
        "\n",
        "# Plot the top 10 feature importances\n",
        "# 上位10件の特徴量重要度をプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=top_features, x='Importance', y='Feature', palette='viridis')\n",
        "plt.title(f'Top {top_n} Feature Importances (Random Forest)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdqOkS3It7LU"
      },
      "source": [
        "###💡 Insights / 考察\n",
        "Sex, Title_Mr, and Fare are among the top influential predictors.\n",
        "\n",
        "Visualizing feature importance helps us focus on key variables and potentially eliminate or transform low-importance ones.\n",
        "\n",
        "These results reinforce earlier interpretations and guide future feature refinement.\n",
        "\n",
        "Sex, Title_Mr, Fare などが上位に位置しており、モデルにとって非常に重要な特徴量であることがわかります。\n",
        "この可視化は、重要な変数に注目し、不要な特徴の削除や改善に向けた方向性を示してくれます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiS_ImE06-QR"
      },
      "source": [
        "## 4.9 Cumulative Feature Importance for Feature Selection / 特徴量選択のための累積重要度の可視化\n",
        "To perform feature selection, we visualized the cumulative feature importance.  \n",
        "This helps us identify how many top features are needed to cover over 80% of the total importance.  \n",
        "The cumulative importance plot serves as a guide to balance model complexity and performance.\n",
        "\n",
        "---\n",
        "特徴量選択を行うために、特徴量の累積重要度を可視化しました。  \n",
        "これにより、全体の80%以上の重要度をカバーするために必要な上位特徴量の数がわかります。  \n",
        "累積重要度のグラフは、モデルの複雑さと性能のバランスを取る際の指標として役立ちます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v82-V_Ma7At6"
      },
      "outputs": [],
      "source": [
        "# Calculate cumulative importance\n",
        "# 累積重要度を計算する\n",
        "feature_importance_df['Cumulative'] = feature_importance_df['Importance'].cumsum()\n",
        "\n",
        "# Limit the number of displayed features (Top 20)\n",
        "# 表示する特徴量数を制限（上位20件のみ表示）\n",
        "top_n_display = 20\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Line plot with feature names on the x-axis\n",
        "# 特徴量名をX軸に用いた折れ線グラフ\n",
        "sns.lineplot(\n",
        "    data=feature_importance_df.head(top_n_display),\n",
        "    x='Feature',\n",
        "    y='Cumulative',\n",
        "    marker='o'\n",
        ")\n",
        "\n",
        "# Rotate x-axis labels for readability\n",
        "# X軸ラベルを回転して読みやすくする\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Draw a horizontal line at 80% cumulative importance\n",
        "# 累積重要度80%のしきい値ラインを追加\n",
        "plt.axhline(0.8, color='red', linestyle='--', label='80% threshold')\n",
        "\n",
        "# Titles and labels / タイトルと軸ラベル\n",
        "plt.title('Cumulative Feature Importance (Top 20 Features)')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Cumulative Importance')\n",
        "plt.legend()\n",
        "\n",
        "# Adjust layout / レイアウト調整\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vErWshnjGEZl"
      },
      "source": [
        "## 4.10 Selected Features Covering Top 80% of Cumulative Importance / 累積重要度上位80%を占める特徴量の選択\n",
        "\n",
        "This code selects and lists the features whose cumulative importance is less than 80%.  \n",
        "These features represent the most important ones that contribute up to 80% of the total importance in the model.  \n",
        "Using these selected features can help reduce dimensionality while retaining most of the predictive power.  \n",
        "\n",
        "---\n",
        "このコードは累積重要度が80%未満の特徴量を選択し、リスト化しています。  \n",
        "これらの特徴量はモデルの全体重要度の80%までを占める最も重要な特徴量群です。  \n",
        "この特徴量群を使うことで、次元削減しつつ予測性能を維持することが期待できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MldIUfgJ9baE"
      },
      "outputs": [],
      "source": [
        "# Select features that contribute to cumulative importance less than 80%\n",
        "# 累積重要度が80%未満の特徴量を選択\n",
        "selected_features = feature_importance_df[feature_importance_df['Cumulative'] < 0.8]['Feature'].tolist()\n",
        "\n",
        "# Display the selected features\n",
        "# 選択された特徴量を表示\n",
        "print(\"Top 80% features:\", selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jT2EQ1fTWe"
      },
      "source": [
        "## 4.11 Grid Search Using Top 80% Features / 上位80%の特徴量を使ったグリッドサーチの実施\n",
        "\n",
        "We select only the features that cumulatively account for 80% of the importance and perform hyperparameter tuning (grid search) on the Random Forest model using these selected features.\n",
        "\n",
        "---\n",
        "累積重要度で上位80%を占める特徴量だけを選択し、その特徴量に絞ってランダムフォレストのハイパーパラメータ最適化（グリッドサーチ）を行います。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TgsMwG_AobB"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 上位80%の特徴量だけを特徴量に含める\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldの定義（shuffleあり）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Hyperparameter grid / ハイパーパラメータの候補設定\n",
        "# =====================================================\n",
        "param_grid_rf_sel = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Grid Search with cross-validation / クロスバリデーション付きグリッドサーチ\n",
        "# =====================================================\n",
        "grid_search_rf_sel = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid_rf_sel,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Fit on all data / 全データで学習（グリッドサーチで最適パラメータ探索）\n",
        "# =====================================================\n",
        "grid_search_rf_sel.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Get best estimator and parameters / 最良モデルとパラメータ取得\n",
        "# =====================================================\n",
        "best_model_rf_sel = grid_search_rf_sel.best_estimator_\n",
        "best_params_rf_sel = grid_search_rf_sel.best_params_\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluation function definition / クロスバリデーション評価関数定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluate model with cross-validation / クロスバリデーションによる性能評価\n",
        "# =====================================================\n",
        "scores_rf_sel = evaluate_model_cv(best_model_rf_sel, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Classification report with cross-validated predictions / CV予測を使った分類レポート\n",
        "# =====================================================\n",
        "y_pred_cv_rf_sel = cross_val_predict(best_model_rf_sel, X_selected, y_selected, cv=cv)\n",
        "class_report_rf_sel = classification_report(y_selected, y_pred_cv_rf_sel)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 Best Parameters (RF selected features):\", best_params_rf_sel)\n",
        "print(\"📈 Mean CV Accuracy:\", scores_rf_sel['accuracy'])\n",
        "print(\"📈 Mean CV ROC AUC:\", scores_rf_sel['roc_auc'])\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", class_report_rf_sel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWIYttpmHc9P"
      },
      "source": [
        "### Discussion / 考察\n",
        "\n",
        "- **Effectiveness of Feature Selection**  \n",
        "  Selecting only the top 80% of features based on cumulative importance retained strong predictive performance, demonstrating that dimensionality reduction by removing less important features is effective.\n",
        "\n",
        "- **Model Performance**  \n",
        "  The mean cross-validated accuracy (~0.85) and ROC AUC (~0.88) show that the model generalizes well and maintains stable performance.\n",
        "\n",
        "- **Class-wise Performance Differences**  \n",
        "  The recall for the deceased class (class 0) is high, whereas the recall for the survived class (class 1) is somewhat lower, indicating the model tends to miss some survivors.\n",
        "\n",
        "- **Improved Computational Efficiency**  \n",
        "  Reducing the number of features lowers training and inference computational costs, making the model more practical for deployment.\n",
        "\n",
        "Future work could focus on improving recall for the survived class, exploring further feature engineering, and considering ensemble methods to boost accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "- **特徴量選択の有効性**  \n",
        "  累積重要度の上位80%の特徴量だけを選択しても高い予測性能を維持しており、重要度の低い特徴量を除くことで次元削減が効果的であることが示されました。\n",
        "\n",
        "- **モデルの性能**  \n",
        "  クロスバリデーションの平均Accuracy（約0.85）およびROC AUC（約0.88）から、モデルの汎化性能が高く安定していることがわかりました。\n",
        "\n",
        "- **クラス別の性能差**  \n",
        "  死亡者クラス（クラス0）のリコールは高い一方、生存者クラス（クラス1）のリコールがやや低く、生存者を見逃しやすい傾向がみられました。\n",
        "\n",
        "- **計算効率の向上**  \n",
        "  特徴量を絞ることで学習・推論の計算コストが低減され、実用面での利便性が向上しています。\n",
        "\n",
        "今後は、生存者クラスのリコール改善、さらなる特徴量エンジニアリングの検討、およびアンサンブル手法の導入による精度向上を目指すことが重要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hua5zvQMOMbL"
      },
      "source": [
        "## 4.12 Stepwise Feature Selection from Remaining Features / 残りの特徴量から1つずつ選んでスコア比較を行う\n",
        "\n",
        "We start from the currently selected features (top 80% by importance) and evaluate the baseline model accuracy.  \n",
        "Then, we iteratively add one remaining feature at a time to see if it improves the model performance.  \n",
        "This helps us identify which additional features might be valuable to include.\n",
        "\n",
        "現在選択されている特徴量（重要度上位80%）でベースラインのスコアを計算します。  \n",
        "その後、残りの特徴量を一つずつ追加してモデルの精度を比較し、  \n",
        "どの特徴量を追加すると改善が見込めるかを確認します。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l03ULGDPBNg"
      },
      "outputs": [],
      "source": [
        "# Currently selected features (top 80% by importance)\n",
        "# 現在選択されている特徴量（重要度上位80%）\n",
        "selected_features = ['Sex', 'Title_Mr', 'Fare', 'Fare_log', 'Age', 'Title_Miss', 'Pclass_3', 'TicketGroupSize', 'Title_Mrs', 'Family', 'Has_Cabin']\n",
        "\n",
        "# All feature columns excluding 'PassengerId' and 'Survived'\n",
        "# 'PassengerId'と'生存'列を除いたすべての特徴量名\n",
        "all_features = df_fe4.drop(columns=['PassengerId', 'Survived']).columns.tolist()\n",
        "\n",
        "# Candidate features not yet used in the selected features\n",
        "# 現在の選択特徴量に含まれていない、未使用の候補特徴量\n",
        "remaining_features = [f for f in all_features if f not in selected_features]\n",
        "\n",
        "# Original target variable / 目的変数（生存フラグ）\n",
        "y = df_fe4['Survived']\n",
        "\n",
        "print(\"Number of current features:\", len(selected_features))  # 現在の特徴量数\n",
        "print(\"Number of candidate features:\", len(remaining_features))  # 候補特徴量数\n",
        "print()\n",
        "\n",
        "# Baseline score with currently selected features / 現在の特徴量のみでのベースラインスコア計算\n",
        "X_base = df_fe4[selected_features]\n",
        "X_train_base, X_valid_base, y_train, y_valid = train_test_split(\n",
        "    X_base, y, test_size=0.3, random_state=42\n",
        ")\n",
        "model_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_base.fit(X_train_base, y_train)\n",
        "base_score = model_base.score(X_valid_base, y_valid)\n",
        "print(f\"Baseline score with current features: {base_score:.4f}\")\n",
        "print()\n",
        "\n",
        "# List to store results of adding each remaining feature / 追加特徴ごとの結果格納リスト\n",
        "improvement_list = []\n",
        "\n",
        "for feature in remaining_features:\n",
        "    # Combine current features with one candidate feature / 現在の特徴量に1つの候補特徴量を追加\n",
        "    test_features = selected_features + [feature]\n",
        "    X = df_fe4[test_features].values\n",
        "\n",
        "    # Split train and validation sets / 学習・検証データに分割\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train Random Forest model / ランダムフォレストモデルの学習\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate accuracy on validation set / 検証データでの精度評価\n",
        "    score = model.score(X_valid, y_valid)\n",
        "\n",
        "    # Calculate score difference compared to baseline / ベースラインとの差分を計算\n",
        "    score_diff = score - base_score\n",
        "\n",
        "    # Append results to list / 結果をリストに追加\n",
        "    improvement_list.append((feature, score, score_diff))\n",
        "\n",
        "    print(f\"Added feature: {feature:<20} → Score: {score:.4f} (Diff: {score_diff:+.4f})\")\n",
        "\n",
        "# Sort results by score difference descending / スコア差分で降順にソート\n",
        "improvement_list.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\n=== Score Improvement Ranking / スコア改善ランキング ===\")\n",
        "for feature, score, diff in improvement_list:\n",
        "    print(f\"{feature:<20} → Score: {score:.4f} (Diff: {diff:+.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6_OG0WqJ70L"
      },
      "source": [
        "### Discussion / 考察\n",
        "\n",
        "### 1. Model Performance Improvement  \n",
        "By adding the features `S` (Embarked), `AgeGroup_Adult` (Adult Age Group), `Deck_B` (Deck B), `Pclass_1` (First Class), and `IsGroup` (Presence of Group), the model's accuracy improved compared to the baseline using the top 80% important features.  \n",
        "Notably, adding `S` led to the largest accuracy gain of about 2.2%, confirming that the boarding location plays a significant role in survival prediction.\n",
        "\n",
        "### 1. モデル性能の向上  \n",
        "`S`（乗船地）、`AgeGroup_Adult`（成人年齢グループ）、`Deck_B`（Bデッキ）、`Pclass_1`（ファーストクラス）、`IsGroup`（グループの有無）を追加したことで、重要度上位80%の特徴量のみを使ったベースラインよりもモデルの精度が向上しました。  \n",
        "特に`S`の追加による約2.2%の精度改善が最大であり、乗船地が生存予測に重要な因子であることを裏付けています。\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Interpretability and Practical Implications  \n",
        "- The `S` feature likely reflects differences in rescue priority or lifeboat access based on boarding location.  \n",
        "- `AgeGroup_Adult` captures survival pattern differences between adults and other age groups.  \n",
        "- `Deck_B` and `Pclass_1` represent spatial and social status information influencing survival chances.  \n",
        "- `IsGroup` suggests that traveling in groups may affect survival likelihood.\n",
        "\n",
        "### 2. 解釈と実用的意義  \n",
        "- `S`は、救命ボートの配置や救助の優先度の違いを反映している可能性があります。  \n",
        "- `AgeGroup_Adult`は、成人とその他の年齢層間での生存パターンの違いを捉えています。  \n",
        "- `Deck_B`や`Pclass_1`は、船内の位置や社会的地位に関わる情報で、生存確率に影響を与えています。  \n",
        "- `IsGroup`は、グループでの同行が生存率に影響する可能性を示唆しています。\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Future Directions  \n",
        "While the model accuracy improved, there remains room to enhance recall for the survived class. Future efforts could focus on:  \n",
        "- Further feature engineering, such as detailed group or family relationships.  \n",
        "- Ensemble methods to increase model robustness.  \n",
        "- Using balanced evaluation metrics like F1-score or AUC to better capture minority class performance.\n",
        "\n",
        "### 3. 今後の方向性  \n",
        "モデルの精度は向上しましたが、生存者クラスのリコール改善にはまだ課題があります。今後は以下の点に注力すると良いでしょう。  \n",
        "- グループや家族関係の詳細を含むさらなる特徴量エンジニアリング。  \n",
        "- モデルの堅牢性を高めるためのアンサンブル手法の検討。  \n",
        "- 少数クラスの性能を適切に評価するため、F1スコアやAUCなどのバランスの取れた評価指標の活用。\n",
        "\n",
        "---\n",
        "\n",
        "### Summary  \n",
        "The stepwise feature selection approach successfully identified additional valuable features, leading to a more interpretable and higher-performing model. This process enhances both the predictive power and the practical usability of the model.\n",
        "\n",
        "### まとめ  \n",
        "ステップワイズ特徴量選択により、重要度上位の特徴量だけでなく、有用な追加特徴量を特定でき、より解釈しやすく精度の高いモデルが構築できました。  \n",
        "これにより、予測性能と実用性の両方が向上しました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9JOfp9mqrVw"
      },
      "source": [
        "### 4.13 Grid Search with Extended Feature Set / 拡張特徴量セットを使ったグリッドサーチ\n",
        "\n",
        "We add the features that improved the score (`S`, `AgeGroup_Adult`, `Deck_B`, `Pclass_1`, `IsGroup`) to the original selected features, creating an extended feature set.  \n",
        "Using this extended set, we perform hyperparameter tuning for the Random Forest model with GridSearchCV to find the best parameters that maximize model performance.\n",
        "\n",
        "スコア改善に寄与した特徴量（`S`、`AgeGroup_Adult`、`Deck_B`、`Pclass_1`、`IsGroup`）を元の選択特徴量セットに追加し、拡張特徴量セットを作成します。  \n",
        "この拡張特徴量セットを使って、ランダムフォレストモデルのハイパーパラメータ最適化をGridSearchCVで行い、モデルの性能を最大化する最適なパラメータを探索します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFswPwAELZze"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define extended feature set / 拡張特徴量セットの定義\n",
        "# =====================================================\n",
        "extended_features = selected_features + ['S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup']\n",
        "X_extended = df_fe4[extended_features]\n",
        "y = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldの定義（shuffleあり）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define hyperparameter grid / ハイパーパラメータの候補設定\n",
        "# =====================================================\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Grid Search with cross-validation / グリッドサーチ実行\n",
        "# =====================================================\n",
        "rfc_extended = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Fit on all data / 全データで学習し最適パラメータを探索\n",
        "# =====================================================\n",
        "rfc_extended.fit(X_extended, y)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Get best estimator and parameters / 最良モデルとパラメータ取得\n",
        "# =====================================================\n",
        "best_model_rfc_extended = rfc_extended.best_estimator_\n",
        "best_params_rfc_extended = rfc_extended.best_params_\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluate model with cross-validation / クロスバリデーションで性能評価\n",
        "# =====================================================\n",
        "cv_acc_extended = cross_val_score(best_model_rfc_extended, X_extended, y, cv=cv, scoring='accuracy').mean()\n",
        "cv_auc_extended = cross_val_score(best_model_rfc_extended, X_extended, y, cv=cv, scoring='roc_auc').mean()\n",
        "\n",
        "# =====================================================\n",
        "# 8. Classification report with cross-validated predictions / CV予測を使った分類レポート作成\n",
        "# =====================================================\n",
        "y_pred_cv_extended = cross_val_predict(best_model_rfc_extended, X_extended, y, cv=cv)\n",
        "class_report_extended = classification_report(y, y_pred_cv_extended)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 Best Parameters (RF extended features):\", best_params_rfc_extended)\n",
        "print(\"📈 Mean CV Accuracy (extended features):\", cv_acc_extended)\n",
        "print(\"📈 Mean CV ROC AUC (extended features):\", cv_auc_extended)\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", class_report_extended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poPbKIilsNYZ"
      },
      "source": [
        "## Discussion / 考察\n",
        "\n",
        "### Model Performance / モデルの性能  \n",
        "The model using the extended feature set achieved:  \n",
        "- Mean cross-validated accuracy of approximately 0.844,  \n",
        "- ROC AUC of about 0.879,  \n",
        "showing a slight improvement compared to the baseline and simpler feature sets.\n",
        "\n",
        "拡張特徴量セットを用いたモデルは、  \n",
        "- 平均クロスバリデーション精度が約0.844、  \n",
        "- ROC AUCが約0.879となり、  \n",
        "ベースラインや単純な特徴量セットと比較して若干の性能向上が見られました。\n",
        "\n",
        "### Precision and Recall / 精度と再現率  \n",
        "- For class 0 (deceased), precision was 86% and recall was 90%, indicating strong performance.  \n",
        "- For class 1 (survived), precision was 82% but recall was lower at 76%, suggesting the model still tends to miss some survivors.\n",
        "\n",
        "クラス0（死亡者）に対しては、精度86%、再現率90%と高い性能を発揮。  \n",
        "クラス1（生存者）に対しては、精度82%だが再現率は76%とやや低く、生存者を見逃す傾向があります。\n",
        "\n",
        "### Implications and Future Work / 今後の示唆  \n",
        "The feature addition stably improved the model performance, but there remains room to improve recall for the survived class.  \n",
        "Future work could explore:  \n",
        "- Further feature engineering and data augmentation,  \n",
        "- More complex models such as ensemble.  \n",
        "\n",
        "今回の特徴量追加でモデル性能は安定的に向上しましたが、生存者クラスのリコール改善の余地があります。  \n",
        "今後は、  \n",
        "- さらなる特徴量エンジニアリングやデータ増強、  \n",
        "- アンサンブルのような複雑なモデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvvzTUjBsqwh"
      },
      "source": [
        "## 4.14 Confusion Matrix / 混同行列\n",
        "We use the confusion matrix to evaluate the classification performance of the best Random Forest model on the validation set.  \n",
        "It shows how many predictions were correct or incorrect for each class.  \n",
        "混同行列は、最良のランダムフォレストモデルを検証データに対して評価するために使\n",
        "います。  \n",
        "各クラスについて、正解・不正解の予測数が分かります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeHwq_dtBkfn"
      },
      "outputs": [],
      "source": [
        "# Predict labels with the best model\n",
        "# 最良モデルで予測ラベルを取得\n",
        "y_pred = best_model_rfc_extended.predict(X_extended)\n",
        "\n",
        "# Calculate confusion matrix / 混同行列を計算\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "# Print confusion matrix / 混同行列を表示\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualize confusion matrix / 混同行列を可視化（任意）\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model_rfc_extended.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4UEhcfV9d9f"
      },
      "source": [
        "# Confusion Matrix Explanation / 解説\n",
        "\n",
        "- **True Negative (TN):** 508 — Number of actual non-survivors correctly predicted as non-survivors.  \n",
        "- **False Positive (FP):** 41 — Number of actual non-survivors incorrectly predicted as survivors.  \n",
        "- **False Negative (FN):** 68 — Number of actual survivors incorrectly predicted as non-survivors.  \n",
        "- **True Positive (TP):** 274 — Number of actual survivors correctly predicted as survivors.  \n",
        "\n",
        "**Insights:**  \n",
        "- The model shows strong performance detecting non-survivors (high TN).  \n",
        "- However, it misses some survivors (relatively high FN), indicating room for improvement in recall for the survivor class.  \n",
        "- This aligns with previous observations of lower recall for class 1.  \n",
        "- Addressing class imbalance and improving model calibration could help improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **True Negative (TN):** 508 — 実際に死亡した人を正しく死亡と予測した数。  \n",
        "- **False Positive (FP):** 41 — 実際は死亡者だが、生存者と誤分類した数。  \n",
        "- **False Negative (FN):** 68 — 実際は生存者だが、死亡者と誤分類した数。  \n",
        "- **True Positive (TP):** 274 — 実際に生存した人を正しく生存と予測した数。  \n",
        "\n",
        "**考察：**  \n",
        "- 死亡者クラスの検出精度が高い（TNが多い）ことがわかります。  \n",
        "- 一方で生存者クラスの見逃し（FN）が一定数あり、リコール改善の余地があります。  \n",
        "- これは以前のリコールが低い結果とも整合しています。  \n",
        "- クラス不均衡への対応やモデルの調整を行うことで、性能向上が期待されます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Smsv-KBubgG"
      },
      "source": [
        "## 4.15 Analyzing Misclassified Samples / 誤分類サンプルの分析  \n",
        "\n",
        "This code identifies which validation samples the model misclassified.  \n",
        "By examining these errors, we can better understand the model's weaknesses and potentially improve it.  \n",
        "The process includes preparing the data, training the model, making predictions, restoring original feature values for clarity, and extracting the misclassified cases.\n",
        "\n",
        "このコードは、モデルが検証データで誤分類したサンプルを特定します。  \n",
        "誤分類したデータを調べることで、モデルの弱点を把握し、改善のヒントを得ることができます。  \n",
        "処理の流れは、データ準備、モデル訓練、予測、元の特徴量の復元（見やすさのため）、誤分類ケースの抽出です。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwMfIOcayfny"
      },
      "outputs": [],
      "source": [
        "# Features and target variable / 特徴量と目的変数の設定\n",
        "extended_features = selected_features + ['S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup']\n",
        "X = df_fe4[extended_features]\n",
        "y = df_fe4['Survived']\n",
        "\n",
        "# Split dataset while preserving index / インデックスを保持して学習・検証データに分割\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest model with best parameters / 最適パラメータでランダムフォレストを訓練\n",
        "rfc = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    max_features='sqrt',\n",
        "    min_samples_leaf=4,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rfc.fit(X_train.values, y_train.values)\n",
        "\n",
        "# Predict on validation set / 検証データで予測\n",
        "y_pred = rfc.predict(X_valid.values)\n",
        "\n",
        "# Recover validation dataset with original indices / 元のインデックス付き検証データを復元\n",
        "df_valid = df_fe4.loc[X_valid.index].copy()\n",
        "\n",
        "# Function to recover original Pclass from one-hot encoding / ワンホットエンコーディングされたPclassを元に戻す関数\n",
        "def recover_pclass(row):\n",
        "    if row.get('Pclass_1') == 1:\n",
        "        return 1\n",
        "    elif row.get('Pclass_2') == 1:\n",
        "        return 2\n",
        "    elif row.get('Pclass_3') == 1:\n",
        "        return 3\n",
        "    return None\n",
        "\n",
        "# Add recovered Pclass column if present / Pclass列を復元して追加（存在する場合）\n",
        "if 'Pclass_1' in df_valid.columns:\n",
        "    df_valid['Pclass'] = df_valid.apply(recover_pclass, axis=1)\n",
        "\n",
        "# Recover Sex as string for interpretability (0: male, 1: female) / 性別を文字列に復元（0: 男性, 1: 女性）\n",
        "df_valid['Sex_str'] = df_valid['Sex'].map({0: 'male', 1: 'female'})\n",
        "\n",
        "# Add prediction results and correctness flag / 予測結果と正解判定を追加\n",
        "df_valid['Predicted'] = y_pred\n",
        "df_valid['Correct'] = df_valid['Survived'] == df_valid['Predicted']\n",
        "\n",
        "# Extract misclassified samples / 誤分類されたサンプルを抽出\n",
        "mistakes = df_valid[df_valid['Correct'] == False]\n",
        "print(\"\\n=== Misclassified Validation Data / 誤分類された検証データ ===\")\n",
        "print(mistakes[['Survived', 'Predicted', 'Sex_str', 'Age', 'Fare', 'Pclass']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXvrZIWviRK"
      },
      "source": [
        "### Misclassification Analysis Insights / 誤分類の傾向分析\n",
        "\n",
        "Among the misclassified samples, we observe the following:\n",
        "\n",
        "- Many false negatives (actual survivors predicted as non-survivors) are **young males in 3rd class with low fares**.\n",
        "- Several false positives (actual non-survivors predicted as survivors) are **females in 2nd or 3rd class**, which might indicate the model over-prioritizes gender.\n",
        "- Passengers with ambiguous feature combinations (e.g., high fare but low class) are also frequently misclassified.\n",
        "\n",
        "These patterns suggest the model might be relying too heavily on individual features like `Sex` or `Fare`, rather than more complex interactions. Introducing more sophisticated features (e.g., family grouping, social status from titles) or using more advanced models (e.g., XGBoost) may improve performance.\n",
        "\n",
        "誤分類されたデータから以下の傾向が見られます：\n",
        "\n",
        "- 偽陰性（生存者を死亡と予測）には、**3等船室の若い男性で運賃が安いケース**が多く見られます。\n",
        "- 偽陽性（死亡者を生存と予測）には、**2〜3等船室の女性**が多く、性別の重み付けが強すぎる可能性があります。\n",
        "- 運賃が高いのに低クラスの乗客など、**特徴量の組み合わせが複雑なケース**も誤分類されやすいです。\n",
        "\n",
        "これらの傾向から、`Sex` や `Fare` といった単体の特徴量への依存が強く、より複雑な特徴量や上位モデル（例：XGBoost）の活用が改善につながる可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyu679d6YE6Z"
      },
      "source": [
        "## 4.16 Characteristics of Misclassified Samples / 誤分類サンプルの特徴分析\n",
        "In this section, we explore the characteristics of the misclassified samples in more detail.\n",
        "We compare their feature distributions with those of correctly classified passengers.\n",
        "このセクションでは、誤分類されたサンプルの特徴を詳しく分析します。\n",
        "正しく分類された乗客と比較することで、誤分類の原因となりやすい傾向を探ります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Zrsyz3e6Kq"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of Age and Fare for misclassified samples / 誤分類サンプルのAgeとFareをヒストグラム表示\n",
        "mistakes[['Age', 'Fare']].hist(bins=20, figsize=(10, 4))\n",
        "plt.suptitle('Distributions of Age and Fare in Misclassified Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nN8u8keMjKa"
      },
      "source": [
        "### From the plots, we observe:\n",
        "\n",
        "- Most misclassified passengers are in the 20–40 age range.\n",
        "- Many misclassified cases had relatively low fares (under 30), which may contribute to the model underestimating their survival chances.\n",
        "\n",
        "このグラフから以下のような傾向が見られます：\n",
        "\n",
        "- 誤分類された乗客は20〜40歳に集中している傾向があります。\n",
        "- 運賃が30未満のケースが多く、モデルが生存の可能性を過小評価している一因かもしれません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6703uaUM0VM"
      },
      "source": [
        "## 4.17 Crosstab of Pclass and Sex in Misclassified Samples / 誤分類サンプルのPclassと性別のクロス集計\n",
        "To understand the demographic patterns behind the misclassifications, we examine the relationship between passenger class (`Pclass`) and sex (`Sex`) among the misclassified validation samples.\n",
        "\n",
        "誤分類された検証サンプルにおいて、乗客の等級（`Pclass`）と性別（`Sex`）の関係を調べ、誤分類が特定の属性に偏っていないかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1394BkSOsn8"
      },
      "outputs": [],
      "source": [
        "# Stacked bar chart of misclassified Pclass × Sex　/ 誤分類サンプルのPclassと性別のクロス集計\n",
        "pd.crosstab(mistakes['Pclass'], mistakes['Sex_str']).plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    colormap='Set2',\n",
        "    figsize=(6,4)\n",
        ")\n",
        "plt.title('Sex Distribution in Misclassified Samples by Pclass')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Pclass')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lVHoXIVdzFp"
      },
      "source": [
        "### Interpretation of Crosstab / クロス集計の解釈\n",
        "\n",
        "From the misclassified samples:\n",
        "\n",
        "- In **1st class**, only **male passengers** were misclassified.\n",
        "- In **2nd class**, **mostly females** were misclassified.\n",
        "- In **3rd class**, both males and females were misclassified, with **females slightly more frequent**.\n",
        "\n",
        "このクロス集計から、以下のような傾向が見られます：\n",
        "\n",
        "- **1等船室では男性のみ**が誤分類されています。\n",
        "- **2等船室では女性の誤分類が多く**、モデルが女性の生存率を過信している可能性があります。\n",
        "- **3等船室では男女ともに誤分類**されていますが、**女性の方がやや多い**傾向があります。\n",
        "\n",
        "These patterns suggest that the model may **overestimate survival for females**, especially in lower classes, and **underestimate survival for 1st-class males**.\n",
        "\n",
        "この傾向から、モデルが特に下位等級の女性に対して**生存確率を過大評価**し、1等船室の男性に対しては**過小評価**している可能性があると考えられます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CUL1nGBRFQw"
      },
      "source": [
        "## 4.18 Comparison of Age and Fare between Correct and Misclassified Samples / 正解者と誤分類者のAge・Fare比較\n",
        "\n",
        "To explore whether certain numeric features differ between correctly and incorrectly classified samples, we compare the average `Age` and `Fare` values for each group.\n",
        "\n",
        "正しく分類されたサンプルと誤分類されたサンプルの間で、`Age`（年齢）と`Fare`（運賃）の平均に差があるかを調べます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRzaeavtfJxR"
      },
      "outputs": [],
      "source": [
        "# Separate correct and misclassified samples / 正解・誤分類サンプルを分ける\n",
        "correct = df_valid[df_valid['Correct'] == True]\n",
        "mistakes = df_valid[df_valid['Correct'] == False]\n",
        "\n",
        "# Compare average Age and Fare / 平均AgeとFareを比較\n",
        "print(\"Average Age - Mistakes:\", mistakes['Age'].mean(), \" / Correct:\", correct['Age'].mean())\n",
        "print(\"Average Fare - Mistakes:\", mistakes['Fare'].mean(), \" / Correct:\", correct['Fare'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rsn92Ife1R6"
      },
      "source": [
        "### 📊 Comparison Results / 比較結果\n",
        "\n",
        "| Feature  | Misclassified (誤分類) | Correctly Classified (正解) |\n",
        "| -------- | ------------------- | ------------------------- |\n",
        "| **Age**  | 30.97               | 30.02                     |\n",
        "| **Fare** | 31.22               | 33.52                     |\n",
        "\n",
        "###🧠 Interpretation / 解釈\n",
        "- The average Age is very similar between the two groups, indicating that age alone is not a strong factor in the model’s misclassification.\n",
        "\n",
        "- The average Fare is slightly lower among the misclassified samples, suggesting that lower fares might slightly increase misclassification risk, though the difference is small.\n",
        "\n",
        "- Age（年齢） の平均値には 大きな差はなく、モデルが年齢単独に強く依存しているわけではないと考えられます。\n",
        "\n",
        "- 一方で Fare（運賃） の平均値は誤分類された方が わずかに低く、運賃が低い乗客は やや誤分類されやすい 傾向が示唆されます（ただし、差は小さいです）。  \n",
        "\n",
        "###🔍 Conclusion / 結論\n",
        "While Age appears to have minimal influence on misclassification, the slightly lower Fare among misclassified passengers indicates that fare might subtly affect model decisions. This reinforces the idea that Fare, while informative, may not be sufficient in isolation and should be considered in combination with other features for robust predictions.\n",
        "\n",
        "Age の影響は限定的である一方、Fare はわずかながら誤分類に関連している可能性があります。\n",
        "これは、Fare 単体ではなく他の特徴量と組み合わせて使うことで、より正確な予測が可能になることを示唆しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDdWBzY9Xex2"
      },
      "source": [
        "## 4.19 Misclassification Rate by Age Group / 年齢帯ごとの誤分類率\n",
        "\n",
        "To identify which age groups the model tends to misclassify, we calculate the misclassification rate by age range.\n",
        "\n",
        "モデルがどの年齢層で誤分類しやすいかを把握するために、年齢帯ごとに誤分類率を算出・可視化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg-mD-wrfV48"
      },
      "outputs": [],
      "source": [
        "# Create age bins / 年齢を区切ってカテゴリ化\n",
        "df_valid['AgeBin'] = pd.cut(df_valid['Age'], bins=[0,10,20,30,40,50,60,70,80])\n",
        "\n",
        "# Group by age bin and calculate misclassification rate / 年齢帯ごとに誤分類率を計算\n",
        "age_mis = df_valid.groupby('AgeBin')['Correct'].agg(['count', 'sum'])\n",
        "age_mis['Misclassification Rate'] = 1 - age_mis['sum'] / age_mis['count']\n",
        "\n",
        "# Plot misclassification rate / 誤分類率を棒グラフで可視化\n",
        "age_mis['Misclassification Rate'].plot(kind='bar', title='Misclassification Rate by Age')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.xlabel('Age Bin')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkB1IxovSmsi"
      },
      "source": [
        "From the plot, we observe that the **misclassification rate is highest for passengers aged 40–50**.\n",
        "\n",
        "This suggests that the model may be struggling to correctly classify passengers in this age range, possibly due to mixed survival patterns or underrepresented training data for this group.\n",
        "\n",
        "グラフから、**40〜50歳の年齢帯で誤分類率が最も高い**ことが分かります。\n",
        "\n",
        "この年代の乗客は、生存・死亡のパターンが混在している可能性があり、モデルが正しく分類するのが難しいのかもしれません。また、この層の学習データが少ない可能性も考えられます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCQAUp2cTH4c"
      },
      "source": [
        "## 4.20 Misclassification Rate by Fare Range / 運賃帯ごとの誤分類率\n",
        "\n",
        "We analyze how the misclassification rate varies across fare ranges. This helps identify whether the model performs poorly for passengers in certain fare brackets.\n",
        "\n",
        "運賃帯ごとの誤分類率を算出・可視化することで、モデルが特定の価格帯で誤分類しやすい傾向がないかを調べます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRdsV-sYfZc2"
      },
      "outputs": [],
      "source": [
        "# Create fare bins every 10 units / 運賃を10刻みでビニング\n",
        "df_valid['FareBin'] = pd.cut(df_valid['Fare'], bins=range(0, 300, 10))\n",
        "\n",
        "# Calculate misclassification rate by fare bin / 運賃帯ごとの誤分類率を計算\n",
        "fare_mis = df_valid.groupby('FareBin')['Correct'].agg(['count', 'sum'])\n",
        "fare_mis['Misclassification Rate'] = 1 - fare_mis['sum'] / fare_mis['count']\n",
        "\n",
        "# Plot the misclassification rate / 棒グラフで可視化\n",
        "fare_mis['Misclassification Rate'].plot(kind='bar', title='Misclassification Rate by Fare')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.xlabel('Fare Bin')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJsSf57AWrVs"
      },
      "source": [
        "### Observation: High Misclassification in High-Fare Bins / 高額運賃帯での誤分類の傾向\n",
        "\n",
        "From the plot, we observe that misclassification rates spike in certain high-fare bins:\n",
        "\n",
        "- **130–140**\n",
        "- **240–250**\n",
        "\n",
        "This is surprising, as high fare typically correlates with higher survival (e.g. 1st-class passengers). The elevated error rate suggests that the model may:\n",
        "\n",
        "- Misclassify wealthy passengers who didn't survive\n",
        "- Overestimate survival for certain high-fare profiles\n",
        "\n",
        "These outliers might be special cases (e.g. solo 1st-class men, or passengers with rare combinations of features).\n",
        "\n",
        "グラフから、以下の高額運賃帯で誤分類率が高くなっていることが分かりました：\n",
        "\n",
        "- **130〜140**\n",
        "- **240〜250**\n",
        "\n",
        "高運賃の乗客（たとえば1等船室）は通常生存率が高いため、これは意外な結果です。モデルが以下のような誤りをしている可能性があります：\n",
        "\n",
        "- 実際には死亡した裕福な乗客を、生存と誤分類している\n",
        "- 特定の高額運賃のパターンで、生存確率を過大評価している\n",
        "\n",
        "これらのケースは、特殊な条件を持つ例外（例：1等船室で単独の男性など）かもしれません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HsIE4yRXftj"
      },
      "source": [
        "## 4.21 Detailed Misclassification Analysis by FareBin, Pclass, and Sex / 運賃帯 × 等級 × 性別による詳細な誤分類分析\n",
        "\n",
        "To detect specific passenger groups where the model performs poorly, we analyze misclassification rates grouped by `FareBin`, `Pclass`, and `Sex`.\n",
        "\n",
        "モデルが誤分類しやすい具体的な乗客グループを特定するために、`FareBin（運賃帯）× Pclass（乗客等級）× Sex（性別）`で誤分類率を算出し、誤分類率が高い順に並べて確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcdxpc5GfjUd"
      },
      "outputs": [],
      "source": [
        "# Group by FareBin, Pclass, and Sex / 運賃帯 × 等級 × 性別で集計\n",
        "grouped = df_valid.groupby(['FareBin', 'Pclass', 'Sex_str'])['Correct'].agg(['count', 'sum'])\n",
        "\n",
        "# Calculate misclassification rate / 誤分類率 = 1 - 正解率\n",
        "grouped['Misclassification Rate'] = 1 - grouped['sum'] / grouped['count']\n",
        "\n",
        "# Sort by highest misclassification rate / 誤分類率が高い順にソートして表示\n",
        "print(grouped.sort_values('Misclassification Rate', ascending=False).dropna())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af6NyKd1Z3Wp"
      },
      "source": [
        "### Key Insights from Misclassification by Fare, Pclass, and Sex / 運賃・等級・性別による誤分類分析の重要な気づき\n",
        "The detailed analysis of misclassification rate by FareBin, Pclass, and Sex reveals specific passenger groups that the model struggles with.\n",
        "\n",
        "#### 🚨 High Misclassification Groups:\n",
        "- **(130–140], Pclass 1, male** → 100% misclassified\n",
        "- **(240–250], Pclass 1, male** → 100% misclassified\n",
        "- **(50–60], Pclass 3, male** → 100%\n",
        "- **(70–80], Pclass 1, male** → 100%\n",
        "\n",
        "These are high-fare, male passengers in 1st class who the model predicted incorrectly.  \n",
        "This is surprising, as such passengers often had a higher chance of survival historically.\n",
        "\n",
        "→ **Hypothesis**: These may be exceptional cases (e.g., solo male travelers who perished), or the model overestimates survival based on fare/class alone.\n",
        "\n",
        "#### ✅ Low Misclassification Groups:\n",
        "- Many **high-fare females in 1st class** were correctly classified (0% error)\n",
        "- **Low-fare males in 3rd class** also had relatively low misclassification (e.g. 0.08 in (0–10] bin)\n",
        "\n",
        "These results suggest the model relies heavily on **Sex, Fare, and Pclass** features, and may struggle with **outliers or overlapping distributions**.\n",
        "\n",
        "---\n",
        "\n",
        "運賃・等級・性別ごとの誤分類率を確認したところ、以下のような明確な傾向が見られました。\n",
        "\n",
        "#### 🚨 誤分類率が高いグループ（モデルの弱点）:\n",
        "- **(130–140], 1等船室, 男性** → 100% 誤分類\n",
        "- **(240–250], 1等船室, 男性** → 100% 誤分類\n",
        "- **(50–60], 3等船室, 男性** → 100%\n",
        "- **(70–80], 1等船室, 男性** → 100%\n",
        "\n",
        "これらは「高運賃 × 男性 × 1等船室」で、本来なら生存率が高い傾向があるにもかかわらず、モデルが誤って予測しています。\n",
        "\n",
        "→ **仮説**：単独の男性旅行者など、他の特徴により生存率が下がっているが、モデルが `Fare` や `Pclass` に過度に依存してしまっている可能性があります。\n",
        "\n",
        "#### ✅ 正しく分類されたグループ（モデルの得意分野）:\n",
        "- 高運賃の女性（特に1等船室）は多くが正しく分類されている\n",
        "- 3等船室の低運賃男性も一部で誤分類率がかなり低い（例：(0–10]で 0.08）\n",
        "\n",
        "これらの傾向から、モデルは **性別・等級・運賃に強く依存**していることがわかります。  \n",
        "その一方で、これらが交差して「例外的な属性」になると誤分類する傾向があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9yxak3dC6X"
      },
      "source": [
        "## 4.22 Misclassification Rate by Pclass and Sex / 等級と性別による誤分類率の可視化  \n",
        "To understand which broad demographic groups the model struggles with,  \n",
        "we calculate and plot misclassification rates by `Pclass` and `Sex`.\n",
        "\n",
        "`Pclass（乗客等級）× Sex（性別）` の組み合わせごとの誤分類率を算出し、棒グラフで可視化します。  \n",
        "これにより、モデルがどのような属性に対して誤分類しやすいかが分かります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM4j0VpYgJpV"
      },
      "outputs": [],
      "source": [
        "# Group by Pclass and Sex / 等級と性別でグループ化\n",
        "total = df_valid.groupby(['Pclass', 'Sex_str'])['Correct'].agg(['count', 'sum'])\n",
        "\n",
        "# Misclassification Rate = 1 - 正解率\n",
        "total['Misclassification Rate'] = 1 - total['sum'] / total['count']\n",
        "\n",
        "# Pivot for bar chart and plot / バーチャート用にピボットして可視化\n",
        "total['Misclassification Rate'].unstack().plot(kind='bar', title='Misclassification Rate by Pclass & Sex')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzWazHjbZ5z"
      },
      "source": [
        "### Class-Gender Distribution and Its Impact / 等級 × 性別の偏りとモデル性能への影響  \n",
        "Upon analyzing the misclassification rates, we notice a **class-gender imbalance**:\n",
        "\n",
        "- In **1st class**, all misclassified passengers were **male**.\n",
        "- In **2nd and 3rd class**, **females outnumber males** in the misclassified samples.\n",
        "\n",
        "これは、モデルが生存率を判断する際に「1等 → 高運賃 → 生存」と単純に学習してしまい、  \n",
        "1等の男性（実際には死亡者も多い）を過剰に生存と予測してしまう一因となっていると考えられます。\n",
        "\n",
        "#### Key Observations / 主な観察結果:\n",
        "\n",
        "- **1st class passengers** in the misclassified group were predominantly male (17 males, 0 females).\n",
        "- **2nd and 3rd class** misclassified passengers had **more females than males**.\n",
        "- This gender imbalance varies **by class**, and may skew model behavior.\n",
        "\n",
        "モデルは `Pclass`, `Sex`, `Fare` を強く重視しているため、  \n",
        "このようなデータ分布の偏りが、モデルの **学習バイアス** につながっていると考えられます。\n",
        "\n",
        "---\n",
        "\n",
        "This insight suggests a need to either:\n",
        "- Engineer more nuanced features (e.g., family size, deck, group survival),\n",
        "- Or use methods that reduce overfitting to correlated features.\n",
        "\n",
        "→ つまり、**Pclass や Sex に依存しすぎない特徴量エンジニアリング**、  \n",
        "または **交差項や非線形モデル** の導入が有効になる可能性があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWpJaVkrjAB2"
      },
      "source": [
        "## 4.23 Misclassification Rate Heatmap by Pclass & Sex / 客室等級と性別ごとの誤分類率ヒートマップ\n",
        "This heatmap visualizes the misclassification rates by passenger class (Pclass) and gender (Sex).\n",
        "Darker colors indicate higher misclassification rates, helping us identify which groups the model struggles with.\n",
        "\n",
        "このヒートマップは、乗客の客室等級（Pclass）と性別（Sex）ごとの誤分類率を示しています。\n",
        "色が濃いほど誤分類率が高く、モデルがどのグループの予測で苦戦しているかを視覚的に理解できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQrmJL7fy_lM"
      },
      "outputs": [],
      "source": [
        "# データをピボットして、行: Pclass、列: Sex_str に変換\n",
        "heatmap_data = total['Misclassification Rate'].unstack()\n",
        "\n",
        "# ヒートマップ描画\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"Reds\", linewidths=0.5)\n",
        "plt.title('Misclassification Rate Heatmap by Pclass & Sex')\n",
        "plt.ylabel('Pclass')\n",
        "plt.xlabel('Sex')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SulCj_rk-8w"
      },
      "source": [
        "### 🔍 Interpretation of Heatmap / ヒートマップの解釈\n",
        "From the heatmap, we observe that 1st class males (Pclass=1, Sex=male) and 3rd class females (Pclass=3, Sex=female) have the highest misclassification rates, at 0.39 and 0.34 respectively.\n",
        "These darker cells indicate that the model struggles particularly with these two demographic groups.\n",
        "\n",
        "ヒートマップから、1等客室の男性（Pclass=1, Sex=male）と3等客室の女性（Pclass=3, Sex=female）の誤分類率が特に高く、それぞれ 0.39、0.34 であることがわかります。\n",
        "これらのセルの色が濃いことから、モデルがこの2つの属性の乗客に対して、特に予測を誤りやすい傾向があると分かります。  \n",
        "### 🛠️ Implications for Model Improvement / モデル改善への示唆\n",
        "These findings suggest that additional feature engineering — such as creating interaction terms (e.g., Pclass x Sex) or incorporating survival rate priors — may help the model better capture these complex patterns.\n",
        "\n",
        "これらの結果は、例えば Pclass × Sex のような交互作用特徴量を作成したり、生存率の事前知識を特徴に組み込むなど、さらなる特徴量エンジニアリングが有効であることを示唆しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8XNEhbbOo2S"
      },
      "source": [
        "## 4.24 📈 SHAP Analysis of Top Features + Key Additions / 上位特徴量＋追加特徴量のSHAP解析  \n",
        "To better understand the model's decision-making process, we analyze SHAP values for the top 80% most important features, along with the following features that significantly improved the model's score:\n",
        "\n",
        "\n",
        "*   S (Embarked = S)\n",
        "*   AgeGroup_Adult\n",
        "*   Deck_B\n",
        "*   Pclass_1\n",
        "*   IsGroup\n",
        "\n",
        "これらの特徴量は、スコア向上に大きく貢献したため、上位80%の重要特徴量に加えて SHAP 値を可視化して分析します。\n",
        "\n",
        "モデルがどの情報を重視しているか、そして各特徴が予測にどれほどの影響を与えているかを確認するのが目的です。\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) values allow us to:\n",
        "\n",
        "*   Visualize the magnitude and direction of each feature's contribution to theprediction\n",
        "*   Understand which features push the prediction higher (toward survival) or lower (toward not surviving) for each passenger\n",
        "\n",
        "This analysis provides interpretability and transparency, helping us not only to validate the model's reasoning but also to identify potential biases or over-reliance on specific features.  \n",
        "\n",
        "SHAP（SHapley加法的説明）値を用いることで、次のことが可能になります：\n",
        "\n",
        "*   各特徴量が予測に与える**影響の大きさと方向（正か負か）**を可視化\n",
        "*   各乗客において、どの特徴が生存の方向に予測を押し上げたか、または非生存の方向に押し下げたかを理解する\n",
        "\n",
        "この分析によってモデルの判断基準を解釈可能にし、妥当な予測をしているかの検証や、特定の特徴への過度な依存や偏りの検出が可能となります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YtHmjW0XPhU"
      },
      "outputs": [],
      "source": [
        "pip install shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rFkfJF6sRsn"
      },
      "source": [
        "### 4.24.1 🔍 SHAP Value Summary (Bar Plot Interpretation) / SHAP値サマリ（バー・プロットの解釈）\n",
        "The SHAP bar plot shows the mean absolute SHAP values for each feature, indicating how much each feature contributes, on average, to the model’s output (regardless of direction).\n",
        "This helps us understand which features the model relies on most heavily in making predictions.\n",
        "\n",
        "SHAP のバー・プロットは、各特徴量の平均絶対SHAP値を示しており、予測に対してその特徴量が平均してどれほど影響を与えているかを表しています（正方向・負方向を問わず）。\n",
        "これにより、モデルがどの情報に最も依存して予測しているかを理解できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xukwEjN6aX0n"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# -------------------------\n",
        "# 1. Data Preparation / モデル学習\n",
        "# -------------------------\n",
        "# Final feature set including selected features and manually added impactful ones\n",
        "# 選択された特徴量に加え、スコア向上に寄与した追加特徴量を含めた最終セット\n",
        "extended_features = selected_features + ['S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup']\n",
        "\n",
        "X_extended_df = df_fe4[extended_features]  # Feature matrix / 特徴量データフレーム\n",
        "y_extended = df_fe4['Survived']           # Target variable / 目的変数（生存フラグ）\n",
        "\n",
        "# -------------------------\n",
        "# 2. Model Training / モデル学習\n",
        "# -------------------------\n",
        "# Train a Random Forest model on the extended feature set\n",
        "# 拡張された特徴量セットでランダムフォレストを学習\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_extended_df, y_extended)\n",
        "\n",
        "# -------------------------\n",
        "# 3. SHAP\n",
        "# -------------------------\n",
        "# Use TreeExplainer for SHAP value calculation (compatible with tree-based models)\n",
        "# 木構造モデルに対応した TreeExplainer を使って SHAP 値を計算\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer(X_extended_df)  # Returns a shap.Explanation object\n",
        "                                        # shap.Explanation オブジェクトが返される（サンプル×特徴量×クラス）\n",
        "\n",
        "# -------------------------\n",
        "# 4. Verification and Debug Output / 検証と出力\n",
        "# -------------------------\n",
        "# Print the number of SHAP value samples and the shape of the first element\n",
        "# SHAP 値のサンプル数と最初の要素の形状を出力して確認\n",
        "print(\"✅ shap_values shape:\", len(shap_values), shap_values[0].shape)\n",
        "\n",
        "# Print the names of the features used\n",
        "# 使用された特徴量名を出力\n",
        "print(\"✅ 特徴量名:\", X_extended_df.columns.tolist())\n",
        "\n",
        "# -------------------------\n",
        "# 5. Visualization (Bar Plot) / 可視化（bar プロット\n",
        "# -------------------------\n",
        "# Since this is a binary classification, we use only the SHAP values for class 1 (Survived=1)\n",
        "# 2クラス分類のため、クラス1（=生存）の SHAP 値のみを使用する\n",
        "shap.summary_plot(shap_values[..., 1], X_extended_df, plot_type=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIeof-GpMSXr"
      },
      "source": [
        "### 📌 Top Features Identified /  特に影響の大きかった特徴量\n",
        "\n",
        "1. **`Title_Mr`** – This feature had the highest SHAP value. The model strongly uses this title to identify male adults, who historically had lower survival rates.\n",
        "\n",
        "2. **`Sex`** – The gender of the passenger remains one of the strongest predictors of survival. Female passengers had much higher survival rates due to the \"women and children first\" evacuation policy.\n",
        "\n",
        "3. **`Pclass_3`** – Being in 3rd class negatively influences survival predictions. This suggests the model learned that lower-class passengers had fewer chances of survival.\n",
        "\n",
        "These findings validate the model’s logic and are consistent with known patterns from the Titanic dataset.  \n",
        "\n",
        "\n",
        "---\n",
        "1. **`Title_Mr`** – 最もSHAP値が高かった特徴です。モデルはこの敬称を利用して、大人の男性（歴史的に生存率が低かった）を特定していると考えられます。\n",
        "\n",
        "2. **`Sex`** – 性別は依然として生存予測において非常に強力な特徴です。女性は「女性と子供を優先する」という避難方針の影響で、生存率が高かったためです。\n",
        "\n",
        "3. **`Pclass_3`** – 3等客室の乗客であることは、生存予測にマイナスの影響を与えています。これはモデルが「下層クラスの乗客は生存の可能性が低い」という傾向を学習していることを示唆します。\n",
        "\n",
        "これらの結果は、モデルの予測ロジックが妥当であり、Titanicデータセットで知られている傾向とも一致していることを裏付けています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-i3w_OdtuHo"
      },
      "source": [
        "## 4.24.2 📈 SHAP Summary Plot (Dot Plot) / SHAPサマリープロット（ドットプロット）  \n",
        "\n",
        "This plot shows the distribution of SHAP values for each feature across all samples.  \n",
        "Each point represents a SHAP value for a single passenger, and its color reflects the feature value (e.g., red = high, blue = low).\n",
        "\n",
        "From the plot, we can observe not only which features are important, but also **how** they affect the prediction direction (positive → survival, negative → non-survival).  \n",
        "For example, higher values of `Sex` (i.e., male = 1) tend to push predictions toward non-survival (left), while lower values (female = 0) push toward survival (right).\n",
        "\n",
        "This helps reveal **nonlinear relationships and interaction effects** that may not be obvious from feature importance alone.  \n",
        "このプロットは、各特徴量における全サンプルのSHAP値の分布を示しています。  \n",
        "1つ1つの点が1人の乗客におけるSHAP値を表しており、色はその特徴量の値を示しています（赤 = 高い値、青 = 低い値）。\n",
        "\n",
        "このプロットを通して、重要な特徴量だけでなく、それらが**予測にどのような方向で影響しているか**（右 = 生存方向、左 = 非生存方向）も確認できます。  \n",
        "例えば、`Sex`の値が高い（= 男性）と予測が非生存側（左）に押され、低い（= 女性）と生存側（右）に押される傾向が見られます。\n",
        "\n",
        "単なる特徴量の重要度では見えにくい、**非線形な関係性や特徴量間の相互作用**を理解する手がかりにもなります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfyvK9rcL7d6"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[..., 1], X_extended_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Wd-2UBwgJU"
      },
      "source": [
        "### 🔍 Detailed Interpretation of SHAP Summary Plot / SHAPサマリープロットの詳細な解釈\n",
        "\n",
        "From the SHAP dot plot, we can derive how specific feature values push the model’s predictions toward survival or non-survival.\n",
        "\n",
        "- **Title_Mr**: Most of the red points (high value = Mr) are concentrated on the **left side**, indicating a strong negative impact on survival prediction. The model consistently associates this title with lower survival probability.\n",
        "\n",
        "- **Sex**: Blue points (female = 0) are mainly located on the **left**, meaning that in some cases the model predicts lower survival probability for females. Red points (male = 1) are often on the **right**, showing that being male sometimes increases the prediction. This may indicate complex interactions with other features like `Title` or `Pclass`.\n",
        "\n",
        "- **Pclass_3**: Red points (3rd class = 1) are slightly more concentrated on the **left**, meaning that being in 3rd class tends to lower the survival prediction. Blue points (not 3rd class) are more on the **right**, pushing predictions toward survival. However, the distribution is more balanced compared to the other features, suggesting that the impact of `Pclass_3` is less extreme but still significant.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "SHAPのドットプロットを分析することで、特定の特徴値がモデルの予測を「生存」「非生存」のどちらに押しているかを理解できます。\n",
        "\n",
        "- **Title_Mr**：赤い点（高い値 = Mr）が**左側**に多く分布しており、生存予測に対して強いマイナスの影響を持っていることが分かります。モデルはこの敬称を「生存率が低い人物」として一貫して扱っているようです。\n",
        "\n",
        "- **Sex**：青い点（女性 = 0）が**左側**に多く分布し、女性が生存しにくいと予測されたケースがあることを示しています。逆に赤い点（男性 = 1）は**右側**に分布しており、男性であっても生存予測が高くなるケースがあることを示しています。これは、`Title` や `Pclass` など他の特徴量との**相互作用**の影響が反映されている可能性があります。\n",
        "\n",
        "- **Pclass_3**：赤い点（3等客室 = 1）は**左側**にやや多く分布し、生存予測を下げる傾向があります。青い点（それ以外）は**右側**に多く、生存の方向に影響しています。ただし、他の特徴に比べて分布はよりバランスが取れており、`Pclass_3`の影響は極端ではないものの、依然として重要であることが分かります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtwClIjTyyii"
      },
      "source": [
        "## 4.24.3 🔎 SHAP Waterfall Plot: Individual Prediction Breakdown / SHAPウォーターフォールプロット：個別予測の内訳\n",
        "\n",
        "To understand **how the model makes predictions for a single passenger**, we used the SHAP waterfall plot.  \n",
        "It visualizes how each feature pushed the prediction score up or down starting from the base value.\n",
        "\n",
        "Below is the waterfall plot for passenger **#300**.\n",
        "\n",
        "- Red bars show features that **increase** the predicted probability of survival.\n",
        "- Blue bars show features that **decrease** the probability.\n",
        "\n",
        "This plot clearly shows **which features were most responsible** for this particular prediction.  \n",
        "It complements the summary plot by adding a case-level explanation.  \n",
        "\n",
        "---\n",
        "\n",
        "モデルが**特定の乗客に対してどのように予測を行っているか**を理解するために、SHAPのウォーターフォールプロットを使いました。  \n",
        "このプロットでは、ベースライン（平均的な予測値）からスタートして、各特徴量が予測をどれだけ押し上げた／押し下げたかを視覚的に確認できます。\n",
        "\n",
        "以下は、乗客**#300**のウォーターフォールプロットです。\n",
        "\n",
        "- 赤いバーは、生存の予測スコアを**上げた要因**  \n",
        "- 青いバーは、生存の予測スコアを**下げた要因**\n",
        "\n",
        "このプロットにより、「なぜこの乗客がこう予測されたのか」を**特徴量ごとに明確に可視化**できます。  \n",
        "全体傾向を示すsummary plotと組み合わせることで、分析の深みと納得感が大きく増します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR-xV0KLzuWh"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 1. Extract SHAP values for class 1 (Survived) / クラス1（生存）に対するSHAP値を抽出\n",
        "# -------------------------\n",
        "# The shape of shap_values.values is (891, 14, 2): 891 passengers × 14 features × 2 classes\n",
        "# shap_values.values の形は (891, 14, 2)：891人 × 14特徴量 × 2クラス\n",
        "# So we extract only the SHAP values for class 1 (survived)\n",
        "# そのうち「クラス1（生存）」に対するSHAP値だけを取り出す\n",
        "values = shap_values.values[:, :, 1]  # Resulting shape is (891, 14) / 結果の形は (891, 14)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Extract base values (baseline prediction per sample) / ベース値（サンプルごとの予測ベースライン）を抽出\n",
        "# -------------------------\n",
        "# Base values represent the expected output of the model before seeing any features\n",
        "# ベース値は、特徴量を考慮する前の「平均的な予測値（ベースライン）」を表す\n",
        "# If the SHAP explanation is for multiple classes, we extract class 1's base values\n",
        "# 複数クラスの場合は、生存クラス（クラス1）のベース値を取り出す\n",
        "base_values = shap_values.base_values[:, 1] if shap_values.base_values.ndim > 1 else shap_values.base_values\n",
        "\n",
        "# -------------------------\n",
        "# 3. Rebuild a SHAP Explanation object for class 1 only / クラス1（生存）だけの SHAP Explanation オブジェクトを再構築\n",
        "# -------------------------\n",
        "# This allows us to use SHAP plots like waterfall for class 1 (survived) only\n",
        "# これにより、生存クラスのSHAP値だけを使ったウォーターフォールなどのプロットが可能になる\n",
        "class1_shap = shap.Explanation(\n",
        "    values=values,                     # SHAP values for class 1 / クラス1のSHAP値\n",
        "    base_values=base_values,          # Base values for class 1 / クラス1のベース値\n",
        "    data=X_extended_df,               # Original input data / 元の入力データ\n",
        "    feature_names=X_extended_df.columns  # Feature names / 特徴量名\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4. Plot SHAP waterfall for a specific passenger / 特定の乗客に対するSHAPウォーターフォールプロットを描画\n",
        "# -------------------------\n",
        "# Waterfall plot shows how each feature contributes to the final prediction step by step\n",
        "# ウォーターフォールプロットは、各特徴量が最終予測に段階的にどのように寄与したかを示す\n",
        "shap.plots.waterfall(class1_shap[300]) # Example: passenger #300 / 例：300番目の乗客"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m27GwirBy4Vf"
      },
      "source": [
        "### 🚩 Key Positive Contributors for This Passenger / この乗客に対する主要なプラスの寄与特徴量\n",
        "\n",
        "For this passenger (#300), the top features that increased the predicted survival probability were:\n",
        "\n",
        "- **Title_Mr** with a SHAP value of +0.18  \n",
        "- **Sex** with a SHAP value of +0.13  \n",
        "- **S** (embarked at Southampton) with a SHAP value of +0.08  \n",
        "\n",
        "These features pushed the model’s prediction towards survival.\n",
        "\n",
        "---\n",
        "\n",
        "この乗客（#300）において、生存予測を押し上げた主な特徴量は以下の通りです：\n",
        "\n",
        "- **Title_Mr**（敬称Mr）: SHAP値 +0.18  \n",
        "- **Sex**（性別）: SHAP値 +0.13  \n",
        "- **S**（サウサンプトン乗船）: SHAP値 +0.08  \n",
        "\n",
        "これらの特徴量がモデルの予測を生存方向へと後押ししました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xb79dZl1ooB"
      },
      "source": [
        "## 4.24.4 SHAP Dependence Plots / SHAP依存プロット\n",
        "\n",
        "For each feature in our extended set, we plotted SHAP dependence plots.  \n",
        "These plots show how the value of a feature relates to its impact on the prediction, helping us understand feature interactions and nonlinear effects.\n",
        "\n",
        "拡張特徴量セットの各特徴量について、SHAP依存プロットを作成しました。  \n",
        "このプロットは、特徴量の値が予測にどのように影響するかを示し、特徴間の相互作用や非線形効果の理解に役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQz5kTZK2E6B"
      },
      "outputs": [],
      "source": [
        "# Extract only SHAP values for class 1 (Survived=1)\n",
        "# クラス1（生存=1）に対するSHAP値だけを抽出\n",
        "class1_shap = shap_values[..., 1]\n",
        "\n",
        "# Create dependence plots for each feature in extended_features\n",
        "# extended_features に含まれる各特徴量について、SHAP依存プロットを作成\n",
        "for feat in extended_features:\n",
        "    shap.dependence_plot(\n",
        "        feat,                         # Feature name / 特徴量名\n",
        "        shap_values=class1_shap.values,  # SHAP values for class 1 / クラス1のSHAP値\n",
        "        features=X_extended_df,          # Original input features / 元の特徴量データ\n",
        "        feature_names=X_extended_df.columns  # Feature names / 特徴量名リスト\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nefWbEhI2Qr9"
      },
      "source": [
        "### Interaction Effects Observed / 観察された特徴量の相互作用効果\n",
        "\n",
        "- **Sex × Pclass_3**: Males in 3rd class have relatively higher survival rates compared to others.  \n",
        "  This indicates that the **interaction between gender and passenger class** strongly influences the prediction.\n",
        "\n",
        "- **Title_Mr × Family**: Females with family members present tend to have lower survival rates.  \n",
        "  This suggests a strong **interaction between title (gender/age) and family presence** affecting survival predictions.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **性別 × 3等客室（Sex × Pclass_3）**: 3等客室の男性は他と比べて比較的生存率が高い傾向があります。  \n",
        "  これは、**性別と客室階級の相互作用**が予測に強く影響していることを示しています。\n",
        "\n",
        "- **敬称Mr × 家族の有無（Title_Mr × Family）**: 家族がいる女性は生存率が低い傾向が見られます。  \n",
        "  これは、**敬称（性別や年齢）と家族の有無の相互作用**が生存予測に大きな影響を与えていることを示唆しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-uB7iWy2evt"
      },
      "source": [
        "## 4.24.5 SHAP Dependence of Family by Title / タイトル別の家族特徴量のSHAP依存関係\n",
        "\n",
        "We plotted the SHAP values of the 'Family' feature separately for passengers with titles Mr, Mrs, and Miss.  \n",
        "This allows us to compare how family size affects survival predictions differently depending on age and gender group.\n",
        "\n",
        "家族の特徴量（Family）のSHAP値を、敬称（Title）がMr、Mrs、Missの乗客別に分けて描画しました。  \n",
        "これにより、年齢・性別の異なるグループで家族構成が生存予測にどのように影響するかを比較できます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txM93P_tnjmY"
      },
      "outputs": [],
      "source": [
        "for title in ['Title_Mr', 'Title_Mrs', 'Title_Miss']:\n",
        "    mask = X_extended_df[title] == 1  # Filter samples by title / 敬称でフィルタ\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(\n",
        "        X_extended_df.loc[mask, 'Family'],  # x: Family size / x軸：家族人数\n",
        "        class1_shap.values[mask, X_extended_df.columns.get_loc('Family')],  # y: SHAP value for Family / y軸：FamilyのSHAP値\n",
        "        alpha=0.6,\n",
        "        c='skyblue',  # 色を統一すると見やすい\n",
        "        edgecolor='k'\n",
        "    )\n",
        "    plt.title(f'SHAP Dependence of Family - {title}')  # タイトル\n",
        "    plt.xlabel('Family Size')     # x軸ラベル\n",
        "    plt.ylabel('SHAP Value')      # y軸ラベル\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3bGpfDE3syb"
      },
      "source": [
        "### Observed Trends in Family Size SHAP Values by Title / タイトル別の家族サイズSHAP値の傾向\n",
        "\n",
        "- For **Mr**, family sizes between 1 and 3 are most common and have higher SHAP values than family sizes 4 to 10.  \n",
        "- For **Mrs**, family sizes between 1 and 4 are most common and have higher SHAP values than family sizes 5 to 10.  \n",
        "- For **Miss**, family sizes between 1 and 4 are most common and have higher SHAP values than family sizes 5 to 10.  \n",
        "\n",
        "This suggests that smaller family sizes tend to contribute more positively to survival predictions across these titles.  \n",
        "\n",
        "---\n",
        "\n",
        "- **Mr**では、家族人数が1〜3のグループが多く、4〜10のグループよりも高いSHAP値を示しています。  \n",
        "- **Mrs**では、家族人数が1〜4のグループが多く、5〜10のグループより高いSHAP値です。  \n",
        "- **Miss**でも、家族人数1〜4のグループが多く、5〜10より高いSHAP値の傾向があります。  \n",
        "\n",
        "これは、これらの敬称の乗客において、家族人数が少ない方が生存予測によりポジティブに寄与しやすいことを示唆しています。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQ3dmINwwDN"
      },
      "source": [
        "## 4.25 Combined Feature: Sex_Pclass / 性別とPclassの組み合わせ特徴量\n",
        "\n",
        "We created a new categorical feature by combining `Sex` and `Pclass`, named `Sex_Pclass` (e.g., `female_P1`, `male_P3`).  \n",
        "This feature captures the interaction between gender and passenger class, which is known to be strongly related to survival probability on the Titanic.  \n",
        "To evaluate its effectiveness, we examined the prediction accuracy for each group and found noticeable differences.\n",
        "\n",
        "性別 (`Sex`) とチケットクラス (`Pclass`) を組み合わせて、新たなカテゴリ変数 `Sex_Pclass`（例: `female_P1`, `male_P3`）を作成しました。  \n",
        "この特徴量は、性別と社会階級の相互作用を表現しており、タイタニック号における生存率と強く関連していると考えられます。  \n",
        "有効性を評価するために、各グループの予測正解率を調べたところ、顕著な違いが確認できました。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxwII-5n74tX"
      },
      "outputs": [],
      "source": [
        "# Create a combined categorical feature 'Sex_Pclass' by combining Sex and Pclass\n",
        "# 性別とチケットクラスを組み合わせて、新しいカテゴリ変数 'Sex_Pclass' を作成\n",
        "df_valid['Sex_Pclass'] = df_valid['Sex_str'] + '_P' + df_valid['Pclass'].astype(str)\n",
        "\n",
        "# Calculate the accuracy (percentage of correct predictions) for each Sex_Pclass group\n",
        "# 各 'Sex_Pclass' グループごとに予測の正解率（Correct列の平均）を算出\n",
        "group_accuracy = df_valid.groupby('Sex_Pclass')['Correct'].mean().sort_values()\n",
        "\n",
        "# Display the sorted accuracy by group\n",
        "# グループごとの正解率を昇順で表示\n",
        "print(group_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6_dPnJfydkH"
      },
      "source": [
        "##4.26 Heatmap: Accuracy by Sex and Pclass / 性別とPclassごとの正解率ヒートマップ  \n",
        "To visualize the overall pattern, we created a heatmap showing accuracy across Sex and Pclass.\n",
        "\n",
        "次に、全体の傾向を視覚的に確認するため、性別とPclassごとの正解率をヒートマップで可視化しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viwz9h9-uoUa"
      },
      "outputs": [],
      "source": [
        "heatmap_df = df_valid.pivot_table(index='Sex_str', columns='Pclass', values='Correct', aggfunc='mean')\n",
        "\n",
        "sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.title(\"Accuracy by Sex and Pclass\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqlWF1q8I-YE"
      },
      "source": [
        "### Observations on Prediction Accuracy by Sex_Pclass / Sex_Pclassごとの正解率に関する考察\n",
        "\n",
        "The prediction accuracy varies significantly depending on the combination of Sex and Pclass:\n",
        "\n",
        "- `female_P1`: **100% accuracy**, suggesting the model perfectly predicts survival for first-class female passengers.\n",
        "- `male_P2`: **96% accuracy**, which is surprisingly high for male passengers, indicating that second-class males may have clearer survival patterns in the data.\n",
        "- `male_P3`: **88.8% accuracy**, indicating good performance even in third class.\n",
        "- `female_P2`: **81.5% accuracy**, still quite strong, possibly due to both gender and class advantages.\n",
        "- `female_P3`: **65.9% accuracy**, showing that third-class females are harder to predict correctly.\n",
        "- `male_P1`: **61.4% accuracy**, the lowest among all groups, suggesting that first-class males have less consistent survival patterns from the model's perspective.\n",
        "\n",
        "These results show that the combination of gender and class has a strong influence on the model's ability to make accurate predictions. Particularly, female passengers in higher classes are the most predictable, while male passengers in first class are the least.\n",
        "\n",
        "---\n",
        "\n",
        "性別とPclassを組み合わせた `Sex_Pclass` によって、モデルの予測正解率には大きな差があることがわかります。\n",
        "\n",
        "- `female_P1` は **100%の正解率** で、1等船室の女性に対してモデルが非常に強く学習できていることを示しています。\n",
        "- `male_P2` は **96%の正解率** で、男性としては非常に高い精度で予測されています。2等船室の男性は比較的予測しやすい傾向があると考えられます。\n",
        "- `male_P3` も **88.8%** と高精度で、予測が安定していることがわかります。\n",
        "- `female_P2` は **81.5%** で、性別・クラス両方の影響を受けていると考えられます。\n",
        "- `female_P3` は **65.9%** とやや低めで、3等船室の女性の予測はやや難しい傾向があります。\n",
        "- `male_P1` は **61.4%** と最も低く、1等船室の男性はデータ上で生存・非生存の傾向がはっきりしないのかもしれません。\n",
        "\n",
        "これらの結果から、**性別とクラスの組み合わせはモデルの予測精度に大きな影響を与えている**ことがわかります。特に高等クラスの女性は予測しやすく、1等船室の男性は最も難しいグループであるといえます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXKYRrPeAD2O"
      },
      "source": [
        "## 4.27 Prediction Accuracy by Title_Mr × Family Size / Mr敬称と家族人数の組み合わせによる予測正解率\n",
        "\n",
        "To explore whether family size affects prediction performance differently for male passengers (Title = Mr),  \n",
        "we created a new combined categorical feature: `Title_Family`, which concatenates the `Title_Mr` flag (1 if Mr, 0 otherwise) and the number of family members.  \n",
        "We then calculated the prediction accuracy for each group defined by this combined feature.\n",
        "\n",
        "男性乗客（敬称がMr）の予測精度が家族人数によって異なるかを調べるため、  \n",
        "`Title_Mr`（Mrであれば1、それ以外は0）と`Family`（家族人数）を結合した交差特徴量 `Title_Family` を作成しました。\n",
        "\n",
        "次に、この新しい組み合わせ特徴量ごとに予測の正解率を算出しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxDMQL2p8IIb"
      },
      "outputs": [],
      "source": [
        "# Create a new categorical feature 'Title_Family' by combining Title_Mr (1 or 0) and Family size\n",
        "# Title_Mr（敬称がMrかどうか）とFamily（家族人数）を組み合わせて、新しいカテゴリ変数 'Title_Family' を作成\n",
        "df_valid['Title_Family'] = df_valid['Title_Mr'].astype(str) + '_F' + df_valid['Family'].astype(str)\n",
        "\n",
        "# Calculate prediction accuracy (mean of 'Correct') for each Title_Family group\n",
        "# 各 'Title_Family' グループごとに予測の正解率（Correct列の平均）を算出\n",
        "group_accuracy = df_valid.groupby('Title_Family')['Correct'].mean().sort_values()\n",
        "\n",
        "# Display the accuracy by group in ascending order\n",
        "# グループごとの正解率を昇順で表示\n",
        "print(group_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIDHR_YiIm-Z"
      },
      "source": [
        "## 4.28 Misclassification Rate (Bar Plot) / 誤分類率の棒グラフ\n",
        "\n",
        "To better understand which Title-Family groups are most challenging for the model,  \n",
        "we calculated and visualized the **misclassification rate** (1 - accuracy) for each `Title_Family` category.  \n",
        "This visualization clearly highlights where the model is struggling the most.  \n",
        "\n",
        "どの `Title_Family`（敬称Mrと家族人数の組み合わせ）グループがモデルにとって難しいかを明確にするために、  \n",
        "各グループの**誤分類率（1 - 正解率）**を算出し、棒グラフで可視化しました。  \n",
        "この可視化により、モデルがどのグループの予測に最も苦戦しているかが一目でわかります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNlpP94HAcdT"
      },
      "outputs": [],
      "source": [
        "# Calculate misclassification rate for each group\n",
        "# グループごとの誤分類率を計算\n",
        "title_family_error = 1 - df_valid.groupby('Title_Family')['Correct'].mean()\n",
        "\n",
        "# Sort by misclassification rate in descending order\n",
        "# 誤分類率が高い順にソート\n",
        "title_family_error = title_family_error.sort_values(ascending=False)\n",
        "\n",
        "# Visualize the misclassification rates with a bar plot\n",
        "# 誤分類率を棒グラフで可視化\n",
        "plt.figure(figsize=(12, 6))\n",
        "title_family_error.plot(kind='bar', color='tomato')\n",
        "plt.title('Misclassification Rate by Title_Mr × Family')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.xlabel('Title_Mr_Family')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikIZiKkKIrGc"
      },
      "source": [
        "### Observations / 考察\n",
        "\n",
        "- The group **`1_F4`** (Mr with 4 family members) had the **highest misclassification rate (50%)**,  \n",
        "  suggesting that this group may have more ambiguous or mixed survival signals.\n",
        "- Other Mr groups like `1_F2` and `1_F1` also showed moderate misclassification rates,  \n",
        "  indicating that male passengers with small families are more prone to incorrect predictions.\n",
        "- In contrast, non-Mr groups (`0_F4`, `0_F6`, etc.) generally had **very low error rates**,  \n",
        "  further supporting the idea that male title groups are harder for the model to classify.  \n",
        "\n",
        "This kind of group-wise error analysis helps identify areas where the model might need additional features, adjustments, or focused attention.\n",
        "\n",
        "---\n",
        "\n",
        "- **`1_F4`**（家族4人のMr）が**誤分類率50%**と最も高く、  \n",
        "  このグループの生存パターンは曖昧で、モデルにとって難しいと考えられます。\n",
        "- 他にも、`1_F2`, `1_F1` などの少人数のMrグループでも誤分類率がやや高く、  \n",
        "  小規模な家族を持つ男性乗客の予測が難しい傾向があります。\n",
        "- 一方で、`0_F4`, `0_F6` などの**非Mrグループは誤分類率が非常に低く**、  \n",
        "  やはり**男性乗客の分類が相対的に難しい**ことが確認されました。  \n",
        "  \n",
        "このようなグループごとの誤分類分析は、モデルの改善余地や追加の特徴量設計のヒントを与えてくれます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx75NAerO_Vl"
      },
      "source": [
        "## 🔷 4.29 Feature Selection for Final Model / 最終モデルに向けた特徴量選定\n",
        "\n",
        "To build a more interpretable and efficient model, we reduce the feature set to only the most impactful ones.\n",
        "\n",
        "モデルの解釈性と効率性を高めるため、使用する特徴量を**影響の大きいもののみに絞り込み**ます。\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Top Features Based on SHAP / SHAPによる主要特徴量の選定\n",
        "\n",
        "We start by selecting the top **~80% of cumulative importance** from SHAP values and/or feature importances.  \n",
        "This forms the core of our model, containing features with the strongest direct impact on prediction.\n",
        "\n",
        "まず、SHAP値や特徴量重要度に基づいて、累積で約**上位80%**を占める主要な特徴量を選定します。  \n",
        "これがモデルの基盤となり、**予測に直接的な影響を与える特徴**を中心に構成されます。\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Add Selected High-Impact Interaction Features / 効果的な組合せ特徴の追加\n",
        "\n",
        "In addition to the top features, we add a **small number of carefully chosen interaction features**,  \n",
        "based on their effectiveness observed in misclassification analysis and SHAP dependence plots.\n",
        "Examples of these include:\n",
        "\n",
        "- `Sex_Pclass_male_1`: 1st class males with distinct survival patterns  \n",
        "- `Sex_Pclass_female_3`: 3rd class females with lower survival accuracy  \n",
        "- `Title_Mr_Family_4`: Mr passengers with 4 family members – a high-error group\n",
        "\n",
        "By doing this, we preserve the model's simplicity while still incorporating insights from deeper error analysis.  \n",
        "\n",
        "また、SHAP依存プロットや誤分類分析で**効果的だった交差特徴量**の中から、  \n",
        "**特に強い傾向が見られた少数の組み合わせ**だけを選抜して追加します。\n",
        "\n",
        "追加された具体例：\n",
        "\n",
        "- `Sex_Pclass_male_1`：明確な生存パターンを持つ1等男性  \n",
        "- `Sex_Pclass_female_3`：生存予測が難しかった3等女性  \n",
        "- `Title_Mr_Family_4`：誤分類率が特に高かった、家族4人のMr  \n",
        "\n",
        "このようにすることで、**シンプルなモデル構造を維持しながら、誤分類や相互作用の分析結果を適切に反映**することができます。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dIYKZGAqgTI"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1. 特徴量セットをコピーして構築開始\n",
        "# =========================\n",
        "df_fe5 = df_fe4.copy()\n",
        "df_fe5_test = df_fe4_test.copy()\n",
        "\n",
        "# 学習・テスト結合（特徴量整合のため）\n",
        "df_fe5_all = pd.concat([df_fe5, df_fe5_test], axis=0).reset_index(drop=True)\n",
        "\n",
        "# =========================\n",
        "# 2. 相互作用特徴量の追加\n",
        "# =========================\n",
        "df_fe5_all['male_1_flag'] = ((df_fe5_all['Sex'] == 0) & (df_fe5_all['Pclass_1'] == 1)).astype(int)\n",
        "df_fe5_all['female_3_flag'] = ((df_fe5_all['Sex'] == 1) & (df_fe5_all['Pclass_3'] == 1)).astype(int)\n",
        "df_fe5_all['mr_family_4_flag'] = ((df_fe5_all['Title_Mr'] == 1) & (df_fe5_all['Family'] == 4)).astype(int)\n",
        "\n",
        "# =========================\n",
        "# 3. 使用する特徴量の定義\n",
        "# =========================\n",
        "selected_features = [\n",
        "    'Sex', 'Title_Mr', 'Fare', 'Fare_log', 'Age',\n",
        "    'Title_Miss', 'Pclass_3', 'TicketGroupSize', 'Title_Mrs',\n",
        "    'Family', 'Has_Cabin'\n",
        "]\n",
        "\n",
        "new_features = [\n",
        "    'S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup',\n",
        "    'male_1_flag', 'female_3_flag', 'mr_family_4_flag'\n",
        "]\n",
        "\n",
        "final_features = list(dict.fromkeys(selected_features + new_features))\n",
        "\n",
        "# =========================\n",
        "# 4. データ再分割\n",
        "# =========================\n",
        "df_fe5 = df_fe5_all.iloc[:len(df_fe4)].reset_index(drop=True)\n",
        "df_fe5_test = df_fe5_all.iloc[len(df_fe4):].reset_index(drop=True)\n",
        "\n",
        "X_selected_plus = df_fe5[final_features]\n",
        "y_selected_plus = df_fe5['Survived']\n",
        "\n",
        "# =========================\n",
        "# 5. StratifiedKFold の定義\n",
        "# =========================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =========================\n",
        "# 6. ハイパーパラメータグリッド定義\n",
        "# =========================\n",
        "param_grid_rf_plus = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# 7. GridSearchCV セットアップ\n",
        "# =========================\n",
        "grid_search_rf_plus = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid_rf_plus,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 8. モデル学習（グリッドサーチ）\n",
        "# =========================\n",
        "grid_search_rf_plus.fit(X_selected_plus, y_selected_plus)\n",
        "\n",
        "# =========================\n",
        "# 9. 最良モデルとパラメータ取得\n",
        "# =========================\n",
        "best_model_rf_plus = grid_search_rf_plus.best_estimator_\n",
        "best_params_rf_plus = grid_search_rf_plus.best_params_\n",
        "\n",
        "# =========================\n",
        "# 10. モデル評価（Accuracy / ROC-AUC）\n",
        "# =========================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "scores_rf_plus = evaluate_model_cv(best_model_rf_plus, X_selected_plus, y_selected_plus, cv)\n",
        "\n",
        "# =========================\n",
        "# 11. クロスバリデーション予測 & レポート出力\n",
        "# =========================\n",
        "y_pred_cv_rf_plus = cross_val_predict(best_model_rf_plus, X_selected_plus, y_selected_plus, cv=cv)\n",
        "class_report_rf_plus = classification_report(y_selected_plus, y_pred_cv_rf_plus)\n",
        "\n",
        "# =========================\n",
        "# 12. 結果の出力\n",
        "# =========================\n",
        "print(\"📌 Best Parameters (with interaction features):\", best_params_rf_plus)\n",
        "print(\"📈 Mean CV Accuracy:\", scores_rf_plus['accuracy'])\n",
        "print(\"📈 Mean CV ROC AUC:\", scores_rf_plus['roc_auc'])\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", class_report_rf_plus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0deivh02pTzn"
      },
      "source": [
        "## Insights / 考察\n",
        "\n",
        "### 1. Recall for Survived Class is Relatively Low  \n",
        "- The recall for survivors (class 1) is 0.74, meaning about 26% of survivors are misclassified as non-survivors.  \n",
        "- This may be caused by class imbalance or insufficient feature representation.\n",
        "\n",
        "### 1. 生存者クラスのリコールがやや低い  \n",
        "- 生存者のリコールが0.74で、約26%の生存者が非生存と誤分類されています。  \n",
        "- クラスの不均衡や特徴量の不足が原因の可能性があります。\n",
        "\n",
        "---\n",
        "\n",
        "### 2. High Precision and Recall for Non-Survived Class  \n",
        "- The model detects non-survivors well, with precision 0.85 and recall 0.91.  \n",
        "- It is effective at minimizing false negatives for the non-survivor class.\n",
        "\n",
        "### 2. 非生存者クラスの精度・再現率は高い  \n",
        "- 非生存者の精度が0.85、再現率が0.91と高く、誤って生存者と判定するケースが少ないことを示します。\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Excellent ROC AUC  \n",
        "- The ROC AUC of 0.8768 indicates strong discrimination between survivors and non-survivors.  \n",
        "- Adding interaction features improved the model’s ability to distinguish classes.\n",
        "\n",
        "### 3. ROC AUCが非常に優秀  \n",
        "- ROC AUC 0.8768 は、生存者と非生存者をよく識別できていることを示します。  \n",
        "- 交差特徴量の追加が判別能力を向上させました。\n",
        "\n",
        "---\n",
        "\n",
        "## Overall Strengths and Areas for Improvement / 強みと改善点\n",
        "\n",
        "| Strengths (強み)                          | Areas for Improvement (改善点)            |\n",
        "|-----------------------------------------|-----------------------------------------|\n",
        "| ✅ High discrimination power (高い識別能力・AUCが高い) | 🔺 Recall for survivors can be improved (生存者クラスのリコール改善が課題) |\n",
        "| ✅ Effective use of interaction features (交差特徴量の効果的な活用) | 🔺 Consider exploring more auxiliary features (さらなる補助特徴量の探索が必要) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jrik6eTImdbs"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# 1. Create dictionary of model results\n",
        "#    モデルの結果を辞書形式で定義\n",
        "# =============================================\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\n",
        "        \"Top 80% Features Only\",              # 上位80%の特徴量のみ使用したモデル\n",
        "        \"Top 80% + Interaction Features\"       # 上位80% + 交差特徴量を使用したモデル\n",
        "    ],\n",
        "    \"Best Parameters\": [\n",
        "        best_params_rf_sel,                    # 上位80%モデルの最適パラメータ\n",
        "        best_params_rf_plus                    # Interaction特徴追加モデルの最適パラメータ\n",
        "    ],\n",
        "    \"Mean CV Accuracy\": [\n",
        "        scores_rf_sel['accuracy'],             # 上位80%モデルのCV平均Accuracy\n",
        "        scores_rf_plus['accuracy']             # Interaction追加モデルのCV平均Accuracy\n",
        "    ],\n",
        "    \"Mean CV ROC AUC\": [\n",
        "        scores_rf_sel['roc_auc'],              # 上位80%モデルのCV平均ROC AUC\n",
        "        scores_rf_plus['roc_auc']              # Interaction追加モデルのCV平均ROC AUC\n",
        "    ]\n",
        "}\n",
        "\n",
        "# =============================================\n",
        "# 2. Convert dictionary to DataFrame\n",
        "#    結果をDataFrame形式に変換\n",
        "# =============================================\n",
        "\n",
        "comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# =============================================\n",
        "# 3. Display comparison table\n",
        "#    比較表を表示\n",
        "# =============================================\n",
        "\n",
        "print(\"🔍 Model Comparison Table / モデル比較表：\\n\")\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0PAvcoYqjwe"
      },
      "source": [
        "### Analysis / 考察\n",
        "\n",
        "- Both models achieved similar hyperparameters, indicating that model complexity optimality did not change with the addition of interaction features.\n",
        "- The \"Top 80% Features Only\" model showed a slightly higher mean CV accuracy (84.7%) compared to the model with interaction features (84.2%), suggesting that interaction features did not improve accuracy in this case.\n",
        "- However, the model with interaction features achieved a marginally better mean CV ROC AUC (0.8768 vs 0.8760), which indicates a slight improvement in the model’s ability to discriminate between classes.\n",
        "- This implies that while interaction features might not boost overall accuracy, they can help the model better rank predictions by confidence, potentially improving decision thresholds.\n",
        "- Further feature engineering or tuning might be needed to better exploit the potential of interaction terms for accuracy gains.\n",
        "\n",
        "---\n",
        "\n",
        "- 両モデルは同じ最適ハイパーパラメータを示しており、交差特徴量の追加によってモデルの複雑さの最適解が変わらなかったことを示しています。\n",
        "- 「上位80%の特徴量のみ」モデルはわずかに高い平均CV精度（約84.7%）を示し、交差特徴量を追加したモデル（約84.2%）より精度が若干良好でした。つまり、今回の条件下では交差特徴量が精度向上にはつながらなかった可能性があります。\n",
        "- 一方で、交差特徴量を追加したモデルは平均CV ROC AUCがわずかに高く（0.8768 vs 0.8760）、クラス間の判別力は少し向上していることを示しています。\n",
        "- これは、交差特徴量が全体の精度向上には寄与しなくとも、予測の信頼度のランク付けには役立ち、閾値設定などの調整で性能改善につながる可能性があることを示唆しています。\n",
        "- 今後は、さらに特徴量の工夫やモデルチューニングを行い、交差特徴量の効果をより引き出すアプローチが望まれます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTswIdPCR2WV"
      },
      "source": [
        "## 4.30 Evaluating Contribution of Additional Features / 追加特徴量の貢献度評価\n",
        "\n",
        "We evaluated the impact of each newly added feature by removing them one at a time  \n",
        "and measuring the change in validation accuracy compared to the baseline model.\n",
        "\n",
        "新たに追加した各特徴量について、1つずつ除外してバリデーション精度の変化を確認し、  \n",
        "モデルへの貢献度を定量的に評価しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2TLouyvGB-v"
      },
      "outputs": [],
      "source": [
        "y_fe5 = df_fe5['Survived'] # Target variable　/ 目的変数\n",
        "\n",
        "# Split the data into training and validation sets / データを訓練用と検証用に分割\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(df_fe5[final_features], y_fe5, test_size=0.3, random_state=42)\n",
        "\n",
        "# Baseline model using all features / すべての特徴量を使ったベースラインモデルの作成\n",
        "base_model = RandomForestClassifier(\n",
        "    n_estimators=100,          # Number of trees in the forest / 森の木の数\n",
        "    max_depth=None,            # No limit on the depth of the trees / 木の最大深さは制限なし\n",
        "    max_features='sqrt',       # Number of features to consider at each split / 分割時に考慮する特徴量の数\n",
        "    min_samples_leaf=3,        # Minimum samples required at leaf nodes / 葉に必要な最小サンプル数\n",
        "    min_samples_split=2,       # Minimum samples required to split a node / 分割に必要な最小サンプル数\n",
        "    random_state=42            # Random seed for reproducibility / 再現性のための乱数シード\n",
        ")\n",
        "base_model.fit(X_train, y_train)  # Train the model / モデルを訓練\n",
        "base_score = accuracy_score(y_valid, base_model.predict(X_valid))  # Calculate accuracy on validation set / 検証データで精度を計算\n",
        "\n",
        "# List of newly added handcrafted features / 新しく追加した手作り特徴量のリスト\n",
        "new_features = [\n",
        "    'S', 'AgeGroup_Adult', 'Deck_B', 'Pclass_1', 'IsGroup',\n",
        "    'male_1_flag','female_3_flag', 'mr_family_4_flag'\n",
        "]\n",
        "\n",
        "# Compare accuracy by removing one new feature at a time\n",
        "# 1つずつ新しい特徴量を除外して精度を比較\n",
        "results = []\n",
        "\n",
        "for feature in new_features:\n",
        "    # Features excluding the current one / 現在除外中の特徴量以外を選択\n",
        "    reduced_features = [f for f in final_features if f != feature]\n",
        "    X_train_reduced = X_train[reduced_features]\n",
        "    X_valid_reduced = X_valid[reduced_features]\n",
        "\n",
        "    # Train model on reduced feature set / 除外した特徴量なしでモデル訓練\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        max_features='sqrt',\n",
        "        min_samples_leaf=3,\n",
        "        min_samples_split=2,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_reduced, y_train)\n",
        "    score = accuracy_score(y_valid, model.predict(X_valid_reduced))  # Evaluate accuracy / 精度評価\n",
        "    results.append((feature, score, score - base_score))  # Save feature name, accuracy, and difference from baseline / 結果を保存\n",
        "\n",
        "# Display the results sorted by difference from baseline (descending)\n",
        "# ベースラインとの差分でソートして結果を表示\n",
        "results_df = pd.DataFrame(results, columns=[\"Removed Feature\", \"Accuracy\", \"Difference\"])\n",
        "print(results_df.sort_values(by=\"Difference\", ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrT-uDtPCjVN"
      },
      "source": [
        "### Feature Selection and Model Performance Analysis / 特徴量選択とモデル性能の分析\n",
        "\n",
        "We tested adding several handcrafted features suggested by SHAP and misclassification analysis,  \n",
        "but the validation accuracy decreased compared to using only the top 80% of features selected by feature importance.\n",
        "\n",
        "Removing features one by one showed that some new features, including `AgeGroup_Adult` and `mr_family_4_flag`,  \n",
        "actually decreased model accuracy, suggesting that not all handcrafted features contribute positively.\n",
        "\n",
        "These results indicate that adding too many candidate features may introduce noise or redundancy,  \n",
        "and careful feature selection focusing on the most important features leads to better model performance.\n",
        "\n",
        "Therefore, we decided to use only the top 80% features selected by importance for the final model.\n",
        "\n",
        "---\n",
        "\n",
        "SHAPや誤分類分析で示唆された複数の手作り特徴量を追加して検証しましたが、  \n",
        "特徴量重要度で選んだ上位80%の特徴量だけを使用した場合と比べて、検証精度は低下しました。\n",
        "\n",
        "特徴量を一つずつ除外しながら検証した結果、`AgeGroup_Adult`や`mr_family_4_flag`などの新しい特徴量は  \n",
        "モデルの精度を下げており、すべての手作り特徴量が効果的であるとは限らないことがわかりました。\n",
        "\n",
        "このことから、多くの候補特徴量を追加するとノイズや冗長性が増え、  \n",
        "重要な特徴量に絞って選択することがより良いモデル性能につながると考えられます。\n",
        "\n",
        "よって、最終的なモデルには特徴量重要度で選んだ上位80%の特徴量のみを使うことに決めました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqzI6lbYaVrA"
      },
      "source": [
        "## 📌 4.31 Feature Selection by Feature Importance / 特徴量重要度による特徴選択の検証\n",
        "We tested a simple incremental feature selection strategy: starting from the most important feature and adding one at a time based on the feature importances from a Random Forest model.\n",
        "The goal was to observe how validation accuracy changes as more features are introduced.\n",
        "\n",
        "We trained a model with 1 to all features, recorded the validation accuracy at each step, and plotted the results.\n",
        "This approach can help determine an optimal number of features based solely on their importance scores.\n",
        "\n",
        "ランダムフォレストの特徴量重要度に基づいて、もっとも重要な特徴量から1つずつ追加し、検証用データでの精度がどう変化するかを確認しました。\n",
        "1個から全特徴量まで段階的にモデルを学習し、それぞれの正解率を記録して可視化しました。\n",
        "\n",
        "この方法により、重要度ベースで選んだ場合に最もパフォーマンスの良い特徴量数がどのあたりかを判断できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XhfwvUADM8"
      },
      "outputs": [],
      "source": [
        "# Define features and target variable / 特徴量とターゲット変数を定義\n",
        "X = df_fe5[final_features]\n",
        "y = y_fe5\n",
        "\n",
        "# Train a Random Forest to get initial feature importances\n",
        "# 初期の特徴量重要度を得るためにランダムフォレストを学習\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Sort features by their importance (from highest to lowest) / 特徴量を重要度の高い順にソート\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "sorted_features = feat_importances.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "# Fix the train/validation split to ensure consistency / 一貫性のある比較のために学習データと検証データの分割を固定\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Prepare to record accuracy as we increase the number of features / 特徴量数を増やしながら精度を記録するリストを準備\n",
        "num_features_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Loop through 1 to all features and evaluate model accuracy\n",
        "# 1個から全ての特徴量まで段階的にモデルを学習し、精度を評価\n",
        "for i in range(1, len(sorted_features) + 1):\n",
        "    selected_feats = sorted_features[:i]  # Use top i features / 上位i個の特徴量を使用\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train[selected_feats], y_train)  # Train model / モデルを学習\n",
        "    y_pred = model.predict(X_valid[selected_feats])  # Predict on validation set / 検証データで予測\n",
        "    acc = accuracy_score(y_valid, y_pred)  # Calculate accuracy / 精度を算出\n",
        "    num_features_list.append(i)  # Number of features used / 使用した特徴量の数\n",
        "    accuracy_list.append(acc)    # Accuracy result / 精度の結果\n",
        "\n",
        "# Plot the accuracy as a function of number of features used / 使用する特徴量数ごとの精度をプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(num_features_list, accuracy_list, marker='o')  # Line plot with markers / マーカー付き折れ線グラフ\n",
        "plt.xlabel(\"Number of Features Used\")  # X軸ラベル：使用した特徴量の数\n",
        "plt.ylabel(\"Validation Accuracy\")     # Y軸ラベル：検証精度\n",
        "plt.title(\"Accuracy vs Number of Features (Feature Importance Order)\")  # タイトル：特徴量数と精度の関係\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_JHFEif_fr"
      },
      "source": [
        "## 4.32 Best Accuracy and Number of Features Used / 最高精度と使用した特徴量の数  \n",
        "In this section, we identify the number of features and the corresponding accuracy that achieved the highest performance as we gradually increased the number of features. This is an important step in finding the optimal number of features, which helps in understanding which feature set maximizes the model's performance.  \n",
        "\n",
        "このセクションでは、特徴量数を増やしていく過程で得られた精度結果から、最高精度を達成した特徴量の数とその精度を特定しています。これは、最適な特徴量数を見つけるための重要なステップであり、どの特徴量数がモデルのパフォーマンスを最大化するかを理解するために役立ちます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-mRC0-OGRUZ"
      },
      "outputs": [],
      "source": [
        "# Find the highest accuracy from the recorded accuracies\n",
        "# 記録した精度リストから最も高い精度を見つける\n",
        "best_acc = max(accuracy_list)\n",
        "\n",
        "# Find the number of features corresponding to the highest accuracy\n",
        "# 最高精度が出たときの特徴量の数を取得する\n",
        "best_num_feats = num_features_list[accuracy_list.index(best_acc)]\n",
        "\n",
        "# Print the best accuracy and the number of features used\n",
        "# ベストな精度とその時の特徴量の数を表示する\n",
        "print(f\"✅ Best Accuracy: {best_acc:.4f} at {best_num_feats} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l024owd7K1Dn"
      },
      "source": [
        "### Best Accuracy and Number of Features Used / 最高精度と使用した特徴量の数\n",
        "\n",
        "### Discussion / 考察\n",
        "\n",
        "In this section, we identified the optimal number of features for the model based on the highest validation accuracy achieved. The best accuracy of **0.8209** was obtained when **14 features** were used. This suggests that **14 features** strike the best balance between model complexity and predictive performance.\n",
        "\n",
        "- **Key Insights:**\n",
        "  - The model's accuracy increased as more features were added, but after a certain point, the addition of new features did not lead to a significant improvement.\n",
        "  - Using **14 features** appears to provide the best predictive performance, indicating that these features contain the most relevant information for predicting the target variable.\n",
        "  - This finding suggests that using more than **14 features** could lead to overfitting or unnecessarily increased complexity without adding meaningful improvements to model performance.  \n",
        "\n",
        "The optimal number of features, based on the highest achieved accuracy, is **14**. By selecting these top features, we can achieve a more efficient and effective model without unnecessarily increasing the computational cost or risk of overfitting.  \n",
        "\n",
        "---\n",
        "\n",
        "このセクションでは、特徴量数を増やしていく過程で得られた精度結果から、最高精度を達成した特徴量の数とその精度を特定しました。最適な精度 **0.8209** は **14個の特徴量** で得られました。これは、**14個の特徴量** がモデルの複雑さと予測精度のバランスを最適化することを示しています。\n",
        "\n",
        "- **重要なインサイト:**\n",
        "  - 特徴量を追加することでモデルの精度は向上しましたが、一定の特徴量数を超えると、精度向上はほとんど見られませんでした。\n",
        "  - **14個の特徴量** は予測精度を最も高めるため、これらの特徴量がターゲット変数を予測するために最も関連性のある情報を含んでいると考えられます。\n",
        "  - この結果は、**14個以上の特徴量** を使用すると、過学習や不必要に複雑化する可能性があり、モデル性能の改善にはつながらないことを示唆しています。\n",
        "\n",
        "### 結論\n",
        "最適な特徴量数は、最高精度を達成した **14個の特徴量** です。この特徴量を選定することで、計算コストや過学習のリスクを抑えながら、効率的で効果的なモデルを構築することができます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbh7UAJ98qNJ"
      },
      "source": [
        "### 4.33 Train and evaluate a Random Forest model using the top 14 features by importance / 特徴量重要度上位14個を用いたランダムフォレストモデルの学習と評価  \n",
        "We trained a model using the top 14 features based on feature importance, and compared it with a model using the top 80% of cumulative importance to examine how different selection strategies affect predictive performance.  \n",
        "\n",
        "特徴量重要度の上位14個を用いたモデルを構築し、累積重要度ベースで選んだ上位80%の特徴量によるモデルと比較することで、選択方法の違いが予測性能に与える影響を検証しました。。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W7wDrF9oKBp"
      },
      "outputs": [],
      "source": [
        "# Use the top 14 features selected based on feature importance for training the model.\n",
        "# 特徴量重要度で選ばれた上位14個の特徴量を使ってモデルを学習。\n",
        "top14_features = sorted_features[:14]\n",
        "print(top14_features)\n",
        "\n",
        "# =====================================================\n",
        "# 1. Define features and target variable / 上位14個の特徴量だけを特徴量に含める\n",
        "# =====================================================\n",
        "X_top14 = df_fe5[top14_features]\n",
        "y_top14 = df_fe5['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldの定義（shuffleあり）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Hyperparameter grid / ハイパーパラメータの候補設定\n",
        "# =====================================================\n",
        "param_grid_rf_top14 = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Grid Search with cross-validation / クロスバリデーション付きグリッドサーチ\n",
        "# =====================================================\n",
        "grid_search_rf_top14 = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    param_grid=param_grid_rf_top14,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Fit on all data / 全データで学習（グリッドサーチで最適パラメータ探索）\n",
        "# =====================================================\n",
        "grid_search_rf_top14.fit(X_top14, y_top14)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Get best estimator and parameters / 最良モデルとパラメータ取得\n",
        "# =====================================================\n",
        "best_model_rf_top14 = grid_search_rf_top14.best_estimator_\n",
        "best_params_rf_top14 = grid_search_rf_top14.best_params_\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluation function definition / クロスバリデーション評価関数定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluate model with cross-validation / クロスバリデーションによる性能評価\n",
        "# =====================================================\n",
        "scores_rf_top14 = evaluate_model_cv(best_model_rf_top14, X_top14, y_top14, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Classification report with cross-validated predictions / CV予測を使った分類レポート\n",
        "# =====================================================\n",
        "y_pred_cv_rf_top14 = cross_val_predict(best_model_rf_top14, X_top14, y_top14, cv=cv)\n",
        "class_report_rf_top14 = classification_report(y_top14, y_pred_cv_rf_top14)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 Best Parameters (RF selected features):\", best_params_rf_top14)\n",
        "print(\"📈 Mean CV Accuracy:\", scores_rf_top14['accuracy'])\n",
        "print(\"📈 Mean CV ROC AUC:\", scores_rf_top14['roc_auc'])\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", class_report_rf_top14)\n",
        "\n",
        "# =============================================\n",
        "# 11. Create dictionary of model results\n",
        "#    モデルの結果を辞書形式で定義\n",
        "# =============================================\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\n",
        "        \"Top 80% Features\",              # 上位80%の特徴量のみ使用したモデル\n",
        "        \"Top14　 Features\"                     # 上位14の特徴量を使用したモデル\n",
        "    ],\n",
        "    \"Best Parameters\": [\n",
        "        best_params_rf_sel,                    # 上位80%モデルの最適パラメータ\n",
        "        best_params_rf_top14                    # Interaction特徴追加モデルの最適パラメータ\n",
        "    ],\n",
        "    \"Mean CV Accuracy\": [\n",
        "        scores_rf_sel['accuracy'],             # 上位80%モデルのCV平均Accuracy\n",
        "        scores_rf_top14['accuracy']             # Interaction追加モデルのCV平均Accuracy\n",
        "    ],\n",
        "    \"Mean CV ROC AUC\": [\n",
        "        scores_rf_sel['roc_auc'],              # 上位80%モデルのCV平均ROC AUC\n",
        "        scores_rf_top14['roc_auc']              # Interaction追加モデルのCV平均ROC AUC\n",
        "    ]\n",
        "}\n",
        "\n",
        "# =============================================\n",
        "# 12. Convert dictionary to DataFrame\n",
        "#    結果をDataFrame形式に変換\n",
        "# =============================================\n",
        "comparison_df = pd.DataFrame(results)\n",
        "\n",
        "# =============================================\n",
        "# 13. Display comparison table\n",
        "#    比較表を表示\n",
        "# =============================================\n",
        "\n",
        "print(\"🔍 Model Comparison Table / モデル比較表：\\n\")\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrkPHxPmxeUS"
      },
      "source": [
        "###🔍 Model Comparison and Evaluation / モデル比較と評価\n",
        "Feature Set: Top 80% Features vs Top 14 Features  \n",
        "特徴量セット：上位80%特徴量 vs 上位14特徴量  \n",
        "\n",
        "| Model / モデル      | Mean CV Accuracy | Mean CV ROC AUC | Best Parameters / 最適パラメータ         |\n",
        "| ---------------- | ---------------- | --------------- | --------------------------------- |\n",
        "| Top 80% Features | **0.8473**       | **0.8760**      | `max_depth=7, min_samples_leaf=4` |\n",
        "| Top 14 Features  | 0.8440           | 0.8758          | `max_depth=7, min_samples_leaf=2` |\n",
        "\n",
        "- The Top 80% Features model slightly outperformed the Top 14 model in both accuracy and ROC AUC.\n",
        "\n",
        "- The classification report also showed a marginally higher recall for the Top 80% model.\n",
        "\n",
        "- 上位80%特徴量モデルは、精度とROC AUCの両方で上位14特徴量モデルをわずかに上回りました。\n",
        "\n",
        "- 分類レポートでは、生存者クラスの recall（再現率）も上位80%モデルの方がやや高い傾向が見られました。  \n",
        "\n",
        "###💡 Considerations / 考察\n",
        "\n",
        "- The **Top14 model** uses 14 features selected by descending feature importance, which is more than the **Top 80% model (11 features)**.\n",
        "- However, based on cross-validation, the **Top14 model performed slightly worse** in both accuracy (0.844 vs 0.847) and ROC-AUC.\n",
        "- This suggests that **the additional lower-importance features (12–14) may have introduced noise**, slightly reducing generalization performance.\n",
        "- In contrast, the **Top 80% model achieved higher accuracy with fewer features**, indicating a more efficient use of relevant information and better regularization.\n",
        "- The fact that `min_samples_leaf=4` was optimal for the Top 80% model further supports the idea that **stronger regularization helped prevent overfitting with fewer features**.\n",
        "\n",
        "✅ In conclusion, while the Top14 model is still solid, the **Top 80% model offers better performance with fewer features**, making it a more practical and robust choice.\n",
        "\n",
        "\n",
        "- **Top14モデル**は、特徴量重要度の高い順に14個使用したモデルであり、**上位80%モデル（11個）よりも多くの特徴量を使っています**。\n",
        "- しかし、交差検証の結果、**Top14モデルは若干精度が劣り（Accuracy: 0.844 vs 0.847）**、**ROC-AUCもわずかに低くなりました**。\n",
        "- これは、**追加された下位の特徴量（12〜14位）がノイズになってしまい、モデルの汎化性能をわずかに損なった可能性**を示しています。\n",
        "- 一方、**上位80%モデルは少ない特徴量でより高い精度**を達成しており、**必要な情報を効率よく捉えつつ過学習を防いでいる**と考えられます。\n",
        "- また、最適なハイパーパラメータとして `min_samples_leaf=4` が選ばれたことも、**特徴量が少ない分、より強めの正則化が効果的だった**可能性を示唆しています。\n",
        "\n",
        "✅ 結論として、**Top14は性能が悪いわけではないが、Top 80%モデルの方が少ない特徴量でより高精度かつ安定した結果**を出しており、より実用的であると評価できます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLxJRf0Jig6V"
      },
      "source": [
        "## 4.34 Feature Selection and Model Accuracy Evaluation / 特徴量選択とモデル精度の評価  \n",
        "1. 📌 Initial Feature Selection (Cumulative Importance)  \n",
        "We initially selected features based on cumulative contribution using SHAP values and feature importance scores from a Random Forest model. We set the threshold at 80% of cumulative importance, which resulted in the best validation accuracy of 84.73%.\n",
        "\n",
        "2. 🧪 Testing Handcrafted Features  \n",
        "We then introduced handcrafted features inspired by domain knowledge and misclassification analysis—such as male_1_flag, female_3_flag, and mr_family_4_flag.\n",
        "However, when we added all these features to the base model, validation accuracy slightly decreased.\n",
        "\n",
        "3. 🧮 Evaluating Top 14 Features by Importance  \n",
        "We also tested a model using the top 14 features ranked by Random Forest's feature importance. The validation accuracy was 84.4%, close to the best but still slightly below the cumulative importance model.\n",
        "\n",
        "4. ✅ Conclusion and Final Feature Set  \n",
        "Based on both predictive performance and model simplicity, we selected the feature set derived from the top 80% cumulative importance as our final choice.\n",
        "This approach avoids unnecessary complexity and potential overfitting introduced by less effective handcrafted features.\n",
        "\n",
        "---\n",
        "1. 📌 初期選択（累積重要度ベース）  \n",
        "まず、SHAP値やランダムフォレストによる特徴量重要度に基づき、累積重要度が80％に達するまでの特徴量を選択しました。この特徴量セットが、84.73％の最高検証精度を記録しました。\n",
        "\n",
        "2. 🧪 手作り特徴量の追加と検証  \n",
        "次に、ドメイン知識や誤分類パターンに着想を得て作成した手作り特徴量（例：male_1_flag, female_3_flag, mr_family_4_flag）を追加しました。\n",
        "しかし、これらをすべて加えた場合、モデルの検証精度はわずかに低下しました。\n",
        "\n",
        "3. 🧮 特徴量重要度上位14個によるモデルの評価  \n",
        "さらに、ランダムフォレストの特徴量重要度で上位14個を選んだモデルも評価しました。こちらの精度は84.4％と、かなり高かったものの、累積重要度ベースのモデルにはわずかに届きませんでした。\n",
        "\n",
        "4. ✅ 結論と最終的な特徴量選択  \n",
        "予測精度とモデルのシンプルさのバランスを考慮し、累積重要度80％に基づく特徴量セットを最終モデルとして採用しました。\n",
        "この選択により、効果の不確かな特徴量を排除し、過学習や複雑化を回避できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkQddlqfwPSW"
      },
      "source": [
        "# 🤖 5. Model Comparison / モデル比較\n",
        "In this section, we compared several machine learning models using the final feature set (based on the top 80% cumulative importance).\n",
        "We tried each model one by one with hyperparameter tuning, and evaluated their performance using accuracy, AUC, and classification report on the validation set.\n",
        "\n",
        "このセクションでは、最終的な特徴量セット（累積重要度上位80％）を用いて、複数のモデルを段階的に比較しました。\n",
        "それぞれのモデルでハイパーパラメータ調整を行い、精度（Accuracy）、AUCスコア、および分類レポート（Classification Report）を用いて検証データでの性能を評価しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptd0nb45X1v6"
      },
      "source": [
        "##🧪 5.1 Models Tried / 試したモデル一覧\n",
        "✅ Random Forest (RF)\n",
        "→ This model was already tested during feature engineering.  / 特徴量エンジニアリングの過程で既に使用・検証済み\n",
        "\n",
        "1.🔶 XGBoost  \n",
        " A powerful gradient boosting algorithm known for its performance in structured data.\n",
        "構造化データに強く、コンペティションでも定番のブースティングモデル。\n",
        "\n",
        "2.🔷 LightGBM  \n",
        "A lightweight and fast gradient boosting framework developed by Microsoft.\n",
        "高速で軽量な勾配ブースティングフレームワーク。XGBoostより高速なことも。\n",
        "\n",
        "3.🔸 Support Vector Machine (SVM)  \n",
        "Effective in high-dimensional spaces. Requires careful feature scaling.\n",
        "高次元空間に強いが、特徴量のスケーリングが重要。\n",
        "\n",
        "4.🔺 Voting Classifier (Hard Voting)  \n",
        "An ensemble method combining multiple base models using majority rule.\n",
        "複数の基本モデルを組み合わせて多数決で予測を決定するアンサンブル手法。\n",
        "\n",
        "5.🔰 Stacking Classifier (with RF, XGB, LightGBM, SVM.)  \n",
        "A meta-model that learns to combine the predictions of several base models (RF, XGB, LightGBM, SVM).\n",
        "複数のモデル（RF, XGB, LightGBM, SVM）の出力をメタモデルで統合する高度なアンサンブル手法。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmkgnE32d_g"
      },
      "source": [
        "### ⚙️ **5.2 Evaluation Metrics / 評価指標**\n",
        "\n",
        "To evaluate and compare the performance of each machine learning model, we used the following metrics calculated through **cross-validation (CV)**:\n",
        "\n",
        "- **CV Accuracy**: The average proportion of correct predictions across all folds. This gives an estimate of the model’s generalization performance.\n",
        "- **CV ROC AUC (Receiver Operating Characteristic - Area Under the Curve)**: Measures the model’s ability to distinguish between the two classes (Survived / Not Survived) across CV folds. This is especially important when the class distribution is imbalanced.\n",
        "- **Classification Report (based on CV predictions)**: Reports class-specific metrics such as precision, recall, and F1-score. This helps evaluate how well the model performs on each class (0 = Not Survived, 1 = Survived), beyond overall accuracy.\n",
        "\n",
        "これらのモデルの性能を評価・比較するため、以下の指標を**交差検証（CV）**に基づいて使用しました：\n",
        "\n",
        "- **CV Accuracy（交差検証の正解率）**：各分割での正解率の平均を算出し、モデルの汎化性能を評価しました。\n",
        "- **CV ROC AUC（交差検証におけるROC曲線下面積）**：生存者・非生存者の識別性能を評価する指標であり、クラス不均衡がある場面でも有効です。\n",
        "- **分類レポート（CV予測に基づく）**：各クラス（0 = 非生存、1 = 生存）ごとに、**適合率（Precision）**、**再現率（Recall）**、**F1スコア**を出力し、単なる精度では見えにくいモデルの偏りやクラスごとの性能を詳細に分析しました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-z4XChz3Z9Z"
      },
      "source": [
        "## 5.3 Model Training and Evaluation / モデル学習と評価  \n",
        "### 5.3.1 XGBoost (Before Hyperparameter Tuning) / ハイパーパラメータ調整前のXGBoost\n",
        "\n",
        "We started by training an XGBoost classifier using its default hyperparameters, except for two settings:\n",
        "- `use_label_encoder=False` (to suppress the deprecation warning)\n",
        "- `eval_metric='logloss'` (for binary classification)\n",
        "\n",
        "初期段階では、XGBoostのデフォルト設定を使用しつつ、以下の2点のみ明示的に指定しました：\n",
        "- `use_label_encoder=False`（非推奨警告を回避するため）  \n",
        "- `eval_metric='logloss'`（2値分類のための評価指標）  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haW39S745DbN"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量とターゲット変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use top 80% important features / 上位80%重要度の特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable: Survived / 目的変数：生存者\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define Stratified K-Fold CV / StratifiedKFoldクロスバリデーションを定義（層化抽出）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Initialize XGBoost classifier (Before Hyperparameter Tuning) / ハイパーパラメータ調整前のXGBoost初期化\n",
        "# =====================================================\n",
        "model_xgb = XGBClassifier(\n",
        "    use_label_encoder=False,  # Suppress deprecation warning / 非推奨警告を抑制\n",
        "    eval_metric='logloss',    # Use logloss for binary classification / 2値分類の損失関数としてloglossを指定\n",
        "    random_state=42           # Ensure reproducibility / 結果の再現性を確保するため乱数シードを設定\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Evaluate model with cross-validation / 交差検証でモデル評価（AccuracyとROC AUC）\n",
        "# =====================================================\n",
        "accuracy = cross_val_score(model_xgb, X_selected, y_selected, cv=cv, scoring='accuracy').mean()\n",
        "roc_auc = cross_val_score(model_xgb, X_selected, y_selected, cv=cv, scoring='roc_auc').mean()\n",
        "\n",
        "print(f\"📌 Default XGBoost Evaluation\")\n",
        "print(f\"📈 Mean CV Accuracy: {accuracy:.4f}\")  # 平均正解率を表示\n",
        "print(f\"📈 Mean CV ROC AUC: {roc_auc:.4f}\")    # 平均ROC AUCを表示\n",
        "\n",
        "# =====================================================\n",
        "# 5. Get cross-validated predictions and classification report / 交差検証予測値を取得し分類レポートを表示\n",
        "# =====================================================\n",
        "y_pred = cross_val_predict(model_xgb, X_selected, y_selected, cv=cv)\n",
        "print(\"📝 Classification Report:\\n\", classification_report(y_selected, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvaiUFYl51Kc"
      },
      "source": [
        "### Analysis of Default XGBoost Performance / デフォルトXGBoostの性能に関する考察\n",
        "\n",
        "The default XGBoost classifier achieves a strong performance on this dataset, with a mean cross-validated accuracy of 82.6% and a ROC AUC score of 0.8741. These results indicate that the model is effective at distinguishing between the classes.\n",
        "\n",
        "Looking at the classification report:\n",
        "- Precision and recall are balanced fairly well between the classes, with class 0 (majority class) showing slightly higher precision and recall (0.85 and 0.87) compared to class 1 (minority class) with precision 0.78 and recall 0.76.\n",
        "- The F1-scores, which balance precision and recall, also reflect this trend (0.86 for class 0 and 0.77 for class 1).\n",
        "- Overall, the model shows robust predictive performance, but there is still room for improvement, especially for the minority class (class 1), which tends to have lower recall.\n",
        "\n",
        "These results provide a solid baseline before hyperparameter tuning and further optimization.\n",
        "\n",
        "---\n",
        "\n",
        "このデフォルトのXGBoost分類器は、交差検証で平均正解率82.6%、ROC AUCスコア0.8741と良好な性能を示しています。モデルはクラスの識別に効果的であることがわかります。\n",
        "\n",
        "分類レポートを見ると：\n",
        "- クラス0（多数派クラス）は精度（0.85）と再現率（0.87）がやや高いのに対し、クラス1（少数派クラス）は精度0.78、再現率0.76となっています。\n",
        "- 精度と再現率のバランスを示すF1スコアも同様に、クラス0は0.86、クラス1は0.77です。\n",
        "- 全体的に堅牢な予測性能を持っていますが、特に少数派クラスに対する再現率向上の余地があります。\n",
        "\n",
        "これらの結果は、ハイパーパラメータ調整やさらなる最適化を行う前の良いベースラインとなります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l32HClrJ5-8V"
      },
      "source": [
        "### 📈 5.3.2 XGBoost (After Hyperparameter Tuning) / ハイパーパラメータ調整後のXGBoost\n",
        "\n",
        "We improved the XGBoost model by performing grid search over several hyperparameters, including tree depth, learning rate, regularization terms, and `scale_pos_weight` to handle class imbalance.\n",
        "\n",
        "XGBoostモデルの性能を向上させるために、木の深さ・学習率・正則化項・`scale_pos_weight`（クラス不均衡対応）などのハイパーパラメータについてグリッドサーチを行いました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_7capfE5IIA"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量とターゲット変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use top 80% important features / 上位80%重要度の特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable: Survived / 目的変数：生存者\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define Stratified K-Fold CV / StratifiedKFoldの定義（層化抽出）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Initialize XGBoost classifier / XGBoost分類器を初期化\n",
        "# =====================================================\n",
        "model_xgb = XGBClassifier(\n",
        "    use_label_encoder=False,  # Suppress warning / 非推奨警告を抑制\n",
        "    eval_metric='logloss',    # Binary classification metric / 2値分類の損失関数\n",
        "    random_state=42           # Seed for reproducibility / 再現性確保のため乱数シード設定\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define hyperparameter grid including scale_pos_weight / ハイパーパラメータグリッドを定義\n",
        "# =====================================================\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.7],\n",
        "    'colsample_bytree': [0.7],\n",
        "    'gamma': [0, 1],\n",
        "    'scale_pos_weight': [1, 2]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 5. Perform Grid Search with cross-validation / グリッドサーチで最適化（交差検証）\n",
        "# =====================================================\n",
        "xgb_gs_fe4 = GridSearchCV(\n",
        "    estimator=model_xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',       # Optimize ROC AUC / ROC AUCで最適化\n",
        "    cv=cv,\n",
        "    n_jobs=-1,               # Use all processors / 並列処理\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Fit the model with GridSearchCV / 全データでグリッドサーチ実行\n",
        "# =====================================================\n",
        "xgb_gs_fe4.fit(X_selected , y_selected )  # Train with hyperparameter tuning / 学習＋チューニング実行\n",
        "\n",
        "# =====================================================\n",
        "# 7. Get best estimator and parameters / 最良モデルとパラメータを取得\n",
        "# =====================================================\n",
        "best_model_xgb = xgb_gs_fe4.best_estimator_  # Best model after grid search / グリッドサーチ後の最良モデル\n",
        "best_params_xgb = xgb_gs_fe4.best_params_    # Best hyperparameters / 最良ハイパーパラメータ\n",
        "xgb_best_cv_acc = xgb_gs_fe4.best_score_     # Best cross-validation accuracy / 最良交差検証精度\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluation function (shared with XGBoost) / 評価関数（XGBoostと共通化）\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    モデルの性能を交差検証で評価する関数\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Accuracy score / 精度スコア\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # ROC AUC score / ROC AUCスコア\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}  # Return mean scores / 平均スコアを返す\n",
        "\n",
        "# =====================================================\n",
        "# 9. Evaluate best LGBM model with CV / クロスバリデーションによる性能評価\n",
        "# =====================================================\n",
        "scores_xgb = evaluate_model_cv(best_model_xgb, X_selected, y_selected, cv)  # Evaluate the best LightGBM model / 最良LightGBMモデルの評価\n",
        "\n",
        "# =====================================================\n",
        "# 10. Classification report with cross-validated predictions / CV予測による分類レポート\n",
        "# =====================================================\n",
        "y_pred_cv_xgb = cross_val_predict(best_model_xgb, X_selected, y_selected, cv=cv)  # CV予測（予測値を取得）\n",
        "class_report_xgb = classification_report(y_selected, y_pred_cv_xgb)  # Classification report / 分類レポートの作成\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 Best Parameters (LGBM):\", best_params_xgb)  # Best parameters output / 最良パラメータの表示\n",
        "print(\"📈 Mean CV Accuracy:\", scores_xgb['accuracy'])  # Mean CV accuracy output / 平均CV精度の表示\n",
        "print(\"📈 Mean CV ROC AUC:\", scores_xgb['roc_auc'])  # Mean CV ROC AUC output / 平均CV ROC AUCの表示\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", class_report_xgb)  # Classification report output / 分類レポートの表示"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📊 Model Performance Comparison / モデルの性能比較（XGBoost）\n",
        "\n",
        "| Evaluation Metric / 評価指標            | ① Default XGBoost / デフォルト設定 | ② Tuned XGBoost / チューニング後 |\n",
        "| ----------------------------------- | --------------------------- | ------------------------- |\n",
        "| 📈 Mean CV Accuracy / 平均正解率         | 0.8260                      | **0.8395**                |\n",
        "| 📈 Mean CV ROC AUC / 平均ROC AUC      | 0.8741                      | **0.8868**                |\n",
        "| 🧮 Precision (Class 1) / 適合率（クラス1）  | 0.78                        | **0.81**                     |\n",
        "| 🧮 Recall (Class 1) / 再現率（クラス1）     | 0.76                        | 0.76                      |\n",
        "| 🧮 F1-score (Class 1) / F1スコア（クラス1） | 0.77                        | **0.78**    \n",
        "\n",
        "\n",
        "### 📌 XGBoost Performance Comparison / XGBoostの性能比較\n",
        "\n",
        "**English:**After tuning the XGBoost hyperparameters using Grid Search with a reduced parameter grid (16 combinations), we observed improvements in both the mean cross-validation accuracy (from 0.8260 to 0.8395) and the ROC AUC (from 0.8741 to 0.8868). The F1-score for the minority class (class 1) also slightly improved, indicating better overall balance in classification performance. The gain in precision for class 1 (from 0.78 to 0.81) suggests reduced false positives.\n",
        "\n",
        "\n",
        "XGBoostのハイパーパラメータを（組み合わせ数を絞って）グリッドサーチで調整した結果、平均正解率は0.8260から0.8395に、ROC AUCは0.8741から0.8868に向上しました。特に、クラス1（生存者）のF1スコアと適合率がわずかに改善しており、モデルがよりバランスよくクラス分類できていることを示しています。誤分類（False Positive）の減少も期待されます。"
      ],
      "metadata": {
        "id": "J7OOE6OP4IFg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPAJYMYU2wJH"
      },
      "source": [
        "### 5.3.3 LightGBM (Before Hyperparameter Tuning) / ハイパーパラメータ調整前のLightGBM  \n",
        "Before tuning, we trained a basic LightGBM model with fixed parameters to establish a performance baseline.  \n",
        "Although these are not the full defaults (e.g., `max_depth=3`, `learning_rate=0.2`), no tuning was performed and the setup was consistent with our initial XGBoost test.\n",
        "\n",
        "チューニング前のベースラインとして、LightGBMモデルをシンプルなパラメータ構成で学習しました。  \n",
        "ここでは `max_depth=3` や `learning_rate=0.2` など一部の設定はしていますが、チューニングは行っていません。XGBoostの初期設定と同様の条件で評価することが目的です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsN8brHgmZZ0"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / 選択された特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable / 目的変数\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define Stratified K-Fold CV / StratifiedKFoldの定義（層化抽出）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Initialize LightGBM classifier with basic parameters / LightGBM分類器を初期化（基本的なパラメータ設定)\n",
        "# =====================================================\n",
        "model_lgb = lgb.LGBMClassifier(\n",
        "    max_depth=3,               # Tree max depth / 木の最大深さ\n",
        "    learning_rate=0.2,         # Learning rate / 学習率\n",
        "    n_estimators=100,          # Number of trees / 木の数\n",
        "    random_state=42            # 再現性を確保\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Evaluate model with cross-validation (Accuracy and ROC AUC) / 交差検証でモデル評価（正解率とROC AUC）\n",
        "# =====================================================\n",
        "accuracy = cross_val_score(model_lgb, X_selected, y_selected, cv=cv, scoring='accuracy').mean()\n",
        "roc_auc = cross_val_score(model_lgb, X_selected, y_selected, cv=cv, scoring='roc_auc').mean()\n",
        "\n",
        "print(f\"Mean CV Accuracy: {accuracy:.4f}\")  # 平均正解率を表示\n",
        "print(f\"Mean CV ROC AUC: {roc_auc:.4f}\")    # 平均ROC AUCを表示\n",
        "\n",
        "# =====================================================\n",
        "# 5. Get cross-validated predictions and classification report / 交差検証予測値を取得し、分類レポートを表示\n",
        "# =====================================================\n",
        "y_pred = cross_val_predict(model_lgb, X_selected, y_selected, cv=cv)\n",
        "print(\"Classification Report:\\n\", classification_report(y_selected, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c9HcrK47cDM"
      },
      "source": [
        "### 📝 Observations / 考察  \n",
        "Even without hyperparameter tuning, the model achieved solid performance with ~80.6% accuracy.\n",
        "\n",
        "Class 1 (survived) had slightly lower recall, a common issue in imbalanced datasets.\n",
        "\n",
        "ハイパーパラメータを調整していない段階でも、80%を超える精度を達成。\n",
        "\n",
        "生存者クラス（1）の再現率はやや低いが、今後のチューニングで改善の余地あり。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zHV4mQrAiyH"
      },
      "source": [
        "### 5.3.4 LightGBM (After Hyperparameter Tuning) / ハイパーパラメータ調整後のLightGBM  \n",
        "To improve LightGBM’s performance, we conducted a grid search over multiple hyperparameters including tree depth, learning rate, and sampling strategies. Additionally, `scale_pos_weight` was tuned to mitigate class imbalance.\n",
        "\n",
        "LightGBMの性能を最大化するため、木の深さ、学習率、サンプリングの戦略など、複数のハイパーパラメータについてグリッドサーチを行いました。また、クラスの不均衡に対応するため `scale_pos_weight` も調整対象としました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymyCHEVp6aZi"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量とターゲット変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use top 80% important features / 上位80%重要度の特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable: Survived / 目的変数：生存者\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define StratifiedKFold CV / StratifiedKFoldの定義（シャッフルあり）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# 5分割交差検証（シャッフルあり）：データを5つに分け、各部分で訓練と検証を行います。\n",
        "# シャッフルにより、データがランダムに分割され、モデル評価のバイアスを減少させます。\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define hyperparameter grid for LightGBM / LightGBMのハイパーパラメータ設定\n",
        "# =====================================================\n",
        "lgb_param_grid = {\n",
        "    'max_depth': [3, 5, 7],               # Depth of the trees / 木の深さ\n",
        "    'learning_rate': [0.05, 0.1, 0.2],    # Learning rate / 学習率\n",
        "    'n_estimators': [50, 100, 200],       # Number of trees / 木の数\n",
        "    'subsample': [0.8, 1.0],               # Fraction of samples used per tree / 各木の学習に使用するサンプルの割合\n",
        "    'colsample_bytree': [0.8, 1.0],        # Fraction of features used per tree / 各木で使用する特徴量の割合\n",
        "    'scale_pos_weight': [1, 5, 10]         # Weight for positive class (class imbalance adjustment) / 正のクラスの重み（不均衡調整）\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 4. Initialize LightGBM model / モデルの初期化\n",
        "# =====================================================\n",
        "lgb_model_fe4 = LGBMClassifier(random_state=42)  # LightGBMのモデル初期化（再現性のためrandom_stateを設定）\n",
        "\n",
        "# =====================================================\n",
        "# 5. Grid Search with CV / グリッドサーチ（交差検証付き）\n",
        "# =====================================================\n",
        "\n",
        "lgb_gs_fe4 = GridSearchCV(\n",
        "    estimator=lgb_model_fe4,              # Estimator / モデル\n",
        "    param_grid=lgb_param_grid,             # Hyperparameter grid / ハイパーパラメータの探索範囲\n",
        "    cv=cv,                                 # Cross-validation strategy / クロスバリデーション戦略\n",
        "    scoring='accuracy',                    # Evaluation metric: Accuracy / 評価指標：精度\n",
        "    n_jobs=-1,                             # Use all available CPUs / 利用可能なCPUをすべて使用\n",
        "    verbose=1,                             # Show progress of grid search / グリッドサーチの進捗を表示\n",
        "    error_score='raise'                    # Raise error if any occurs / エラーが発生した場合はエラーを表示\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Fit the model with GridSearchCV / 全データでグリッドサーチ実行\n",
        "# =====================================================\n",
        "lgb_gs_fe4.fit(X_selected, y_selected)  # Fit the model using grid search / グリッドサーチを用いてモデルを学習\n",
        "\n",
        "# =====================================================\n",
        "# 7. Get best estimator and parameters / 最良モデルとパラメータを取得\n",
        "# =====================================================\n",
        "best_model_lgb = lgb_gs_fe4.best_estimator_  # Best model after grid search / グリッドサーチ後の最良モデル\n",
        "best_params_lgb = lgb_gs_fe4.best_params_    # Best hyperparameters / 最良ハイパーパラメータ\n",
        "lgb_best_cv_acc = lgb_gs_fe4.best_score_     # Best cross-validation accuracy / 最良交差検証精度\n",
        "\n",
        "# =====================================================\n",
        "# 8. Evaluation function (shared with XGBoost) / 評価関数（XGBoostと共通化）\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    モデルの性能を交差検証で評価する関数\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Accuracy score / 精度スコア\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # ROC AUC score / ROC AUCスコア\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}  # Return mean scores / 平均スコアを返す\n",
        "\n",
        "# =====================================================\n",
        "# 9. Evaluate best LGBM model with CV / クロスバリデーションによる性能評価\n",
        "# =====================================================\n",
        "scores_lgb = evaluate_model_cv(best_model_lgb, X_selected, y_selected, cv)  # Evaluate the best LightGBM model / 最良LightGBMモデルの評価\n",
        "\n",
        "# =====================================================\n",
        "# 10. Classification report with cross-validated predictions / CV予測による分類レポート\n",
        "# =====================================================\n",
        "y_pred_cv_lgb = cross_val_predict(best_model_lgb, X_selected, y_selected, cv=cv)  # CV予測（予測値を取得）\n",
        "class_report_lgb = classification_report(y_selected, y_pred_cv_lgb)  # Classification report / 分類レポートの作成\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 Best Parameters (LGBM):\", best_params_lgb)  # Best parameters output / 最良パラメータの表示\n",
        "print(\"📈 Mean CV Accuracy:\", scores_lgb['accuracy'])  # Mean CV accuracy output / 平均CV精度の表示\n",
        "print(\"📈 Mean CV ROC AUC:\", scores_lgb['roc_auc'])  # Mean CV ROC AUC output / 平均CV ROC AUCの表示\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", class_report_lgb)  # Classification report output / 分類レポートの表示"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📋 LightGBM Performance Comparison Table / LightGBM性能比較表   \n",
        "\n",
        "| Evaluation Metric / 評価指標         | Before Tuning / 調整前 | After Tuning / 調整後 |\n",
        "| -------------------------------- | ------------------- | ------------------ |\n",
        "| Mean CV Accuracy / 平均精度          | 0.8316              | 0.8384             |\n",
        "| Mean ROC AUC / 平均ROC AUC         | 0.8798              | 0.8803             |\n",
        "| Precision (Class 1) / 適合率（クラス1）  | 0.79                | 0.81               |\n",
        "| Recall (Class 1) / 再現率（クラス1）     | 0.76                | 0.76               |\n",
        "| F1-score (Class 1) / F1スコア（クラス1） | 0.79                | 0.78               |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qr7x8IY26Ufx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrRy4OUy_MtQ"
      },
      "source": [
        "### 📊 LightGBM Performance Comparison (Before vs After Hyperparameter Tuning) / LightGBMの性能比較（ハイパーパラメータ調整 前後）  \n",
        "\n",
        "\n",
        "After hyperparameter tuning, LightGBM showed a slight improvement in performance:\n",
        "\n",
        "- Mean CV Accuracy improved from 0.8316 to 0.8384\n",
        "\n",
        "- Mean ROC AUC increased slightly from 0.8798 to 0.8803\n",
        "\n",
        "- F1-score for class 1 (survivors) remained steady at 0.78, but precision improved (from 0.79 to 0.81)\n",
        "\n",
        "These results suggest that the tuning helped the model make more confident and accurate predictions for the positive class, especially by slightly reducing false positives.\n",
        "\n",
        "Overall, the performance gain is modest but meaningful, especially considering class imbalance. The optimized model maintains strong performance for both classes.  \n",
        "\n",
        "---\n",
        "\n",
        "ハイパーパラメータ調整後のLightGBMは、わずかに性能が向上しました：\n",
        "\n",
        "- Mean CV Accuracyは 0.8316 → 0.8384 に改善\n",
        "\n",
        "- Mean ROC AUCも 0.8798 → 0.8803 に微増\n",
        "\n",
        "- クラス1（生存者）のF1スコアは 0.78で安定しているものの、precisionは0.79 → 0.81と改善\n",
        "\n",
        "これらの結果から、モデルが クラス1の予測において、より確信をもって正しく判定できるようになった（偽陽性の減少） と考えられます。\n",
        "\n",
        "全体として、改善幅は小さいながらも意味のあるものであり、不均衡データにおいても安定した性能を維持していることがわかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJEXzw2sEifG"
      },
      "source": [
        "### 5.3.5 Default SVM Model Evaluation / デフォルトSVMモデルの性能確認\n",
        "\n",
        "To evaluate the baseline performance of a Support Vector Machine (SVM), we trained a default SVM model using selected features. Because SVMs are sensitive to feature scales, we standardized all numerical variables (Fare_log, Age, Family, TicketGroupSize) using StandardScaler.\n",
        "\n",
        "SVMの基本性能を確認するために、選択された特徴量を用いてデフォルト設定のSVMモデルを学習しました。  \n",
        "SVMは特徴量のスケールに敏感であるため、数値特徴量（Fare_log, Age, Family, TicketGroupSize）をStandardScalerで標準化しました。  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / 選択された特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable / 目的変数\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features / 数値特徴量\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / カテゴリ特徴量\n",
        "\n",
        "# =====================================================\n",
        "# 3. Create ColumnTransformer to scale only numerical features / 数値特徴量のみを標準化（カテゴリはそのまま通す）\n",
        "# =====================================================\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),   # Apply StandardScaler to numerical features / 数値特徴量にStandardScalerを適用\n",
        "        ('cat', 'passthrough', categorical_features)     # Leave categorical features unchanged / カテゴリ特徴量はそのまま\n",
        "    ]\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Set up Stratified K-Fold Cross-Validation / StratifiedKFoldの設定（クラスの割合を保つ）\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV with shuffling / シャッフルありの5分割交差検証\n",
        "\n",
        "# =====================================================\n",
        "# 5. Create pipeline with LinearSVC / LinearSVCを使ったパイプラインの作成\n",
        "# =====================================================\n",
        "pipeline_linear_svc = Pipeline([\n",
        "    ('preprocessor', preprocessor),                         # Preprocessing step / 前処理（標準化＋パススルー）\n",
        "    ('linear_svc', LinearSVC(random_state=42, max_iter=10000))  # LinearSVC classifier / 線形SVM分類器\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 6. Define evaluation function / モデル評価関数の定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Mean accuracy / 平均正解率\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # Mean ROC AUC / 平均ROC AUCスコア\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluate LinearSVC with cross-validation / LinearSVCを交差検証で評価\n",
        "# =====================================================\n",
        "scores_linear_svc = evaluate_model_cv(pipeline_linear_svc, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 8. Get CV predictions and generate classification report / 交差検証の予測値から分類レポートを生成\n",
        "# =====================================================\n",
        "y_pred_cv_linear_svc = cross_val_predict(pipeline_linear_svc, X_selected, y_selected, cv=cv)  # Cross-validated predictions / 交差検証予測値\n",
        "report_linear_svc = classification_report(y_selected, y_pred_cv_linear_svc)                   # Generate report / 分類レポート作成\n",
        "\n",
        "# =====================================================\n",
        "# 9. Output evaluation results / 評価結果を出力\n",
        "# =====================================================\n",
        "print(\"\\n==== Default LinearSVC ====\")\n",
        "print(\"📈 Mean CV Accuracy:\", scores_linear_svc['accuracy'])  # Mean accuracy score / 平均正解率\n",
        "print(\"📈 Mean CV ROC AUC:\", scores_linear_svc['roc_auc'])    # Mean ROC AUC score / 平均ROC AUC\n",
        "print(\"📝 Classification Report:\\n\", report_linear_svc)       # Classification report / 分類レポート"
      ],
      "metadata": {
        "id": "BEi6vvglytf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWWh63oRg-iF"
      },
      "source": [
        "## 📌 Default SVM Evaluation (Pipeline) - Consideration / 考察\n",
        "\n",
        "The default LinearSVC model achieved a mean cross-validated accuracy of approximately 81.5% and a ROC AUC score of about 0.87. These results indicate a strong performance for this baseline model on the Titanic dataset.\n",
        "\n",
        "Looking at the classification report, the model shows balanced precision and recall for both classes. The precision and recall for the non-survivor class (class 0) are particularly high, suggesting the model is effective at identifying passengers who did not survive. For the survivor class (class 1), precision and recall are slightly lower but still satisfactory, indicating the model reasonably predicts survivors.\n",
        "\n",
        "Given these results, the default LinearSVC provides a reliable baseline. Future improvements may come from hyperparameter tuning. Overall, this model can serve as a solid foundation for further experiments.\n",
        "\n",
        "---\n",
        "\n",
        "デフォルトのLinearSVCモデルは、交差検証で約81.5％の正解率とROC AUCスコア約0.87を達成しました。これらの結果は、Titanicデータセットにおけるベースラインモデルとして十分な性能を示しています。\n",
        "\n",
        "分類レポートを見ると、非生存者クラス（クラス0）に対しては精度と再現率が高く、このクラスの識別に優れていることがわかります。一方で、生存者クラス（クラス1）については精度・再現率がやや低いものの、妥当な予測ができています。\n",
        "\n",
        "この結果から、デフォルトのLinearSVCは信頼できるベースラインモデルと言えます。今後はハイパーパラメータの調整を通じて、さらなる性能向上が期待できます。全体として、本モデルは次の実験の良い出発点となります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph772VJwiEeV"
      },
      "source": [
        "## 5.3.6 SVM with Class Weight Adjustment / クラス重み調整付きSVM\n",
        "To mitigate the effects of class imbalance in the Titanic dataset, we added class_weight='balanced' as one of the parameters during hyperparameter tuning of the SVM model.\n",
        "This setting instructs the model to automatically assign higher weights to minority classes, such as survivors, which helps improve recall and F1-score for that class.\n",
        "We compared models with and without class weight adjustment using GridSearchCV and selected the best-performing configuration based on cross-validation accuracy.\n",
        "\n",
        "タイタニックデータのクラス不均衡（生存者と非生存者の比率の差）に対処するため、SVMのハイパーパラメータチューニング時に class_weight='balanced' を候補として追加しました。\n",
        "この設定により、少数派クラス（生存者など）に自動で重みが与えられ、再現率やF1スコアの改善が期待できます。\n",
        "GridSearchCV によって重みの有無を含めた複数の設定を比較し、クロスバリデーションの精度に基づいて最良モデルを選択しました。  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features for training / 選択した特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable: survival status / 目的変数：生存の有無\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize'] # 数値特徴量（標準化対象）\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # その他をカテゴリ扱い\n",
        "\n",
        "# =====================================================\n",
        "# 3. ColumnTransformer (only scale numerical features) / 数値特徴量のみスケーリング\n",
        "# =====================================================\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),     # 数値特徴量にStandardScalerを適用\n",
        "    ('cat', 'passthrough', categorical_features)       # カテゴリ特徴量はそのまま通す\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define Stratified K-Fold / StratifiedKFold を定義\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 層化5分割交差検証（シャッフルあり）\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define LinearSVC pipeline / LinearSVC パイプライン（高速モデル）\n",
        "# =====================================================\n",
        "pipeline_linsvc = Pipeline([\n",
        "    ('preprocessor', preprocessor),                             # 前処理（標準化＋カテゴリ通過）\n",
        "    ('svm', LinearSVC(random_state=42, max_iter=5000))          # LinearSVC モデル\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 6. Define hyperparameter grid / ハイパーパラメータの範囲を定義\n",
        "# =====================================================\n",
        "param_grid_linsvc = {\n",
        "    'svm__C': [0.1, 1, 10],                      # 正則化パラメータC\n",
        "    'svm__class_weight': [None, 'balanced']      # クラス重みの有無（不均衡対策）\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 7. Grid Search with ROC AUC scoring / グリッドサーチ（ROC AUCで評価）\n",
        "# =====================================================\n",
        "grid_linsvc = GridSearchCV(\n",
        "    pipeline_linsvc,\n",
        "    param_grid=param_grid_linsvc,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',     # ROC AUC を指標に最適化\n",
        "    n_jobs=-1,             # CPUすべて使う\n",
        "    verbose=1              # 進捗表示あり\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 8. Fit GridSearchCV / グリッドサーチ実行\n",
        "# =====================================================\n",
        "grid_linsvc.fit(X_selected, y_selected)  # 最適パラメータを探索しながら学習\n",
        "\n",
        "# =====================================================\n",
        "# 9. Evaluate best model / 最良モデルの性能評価\n",
        "# =====================================================\n",
        "best_model_svm = grid_linsvc.best_estimator_   # 最良モデル\n",
        "best_params_svm = grid_linsvc.best_params_     # 最適パラメータ\n",
        "\n",
        "# =====================================================\n",
        "# 10. Evaluation function / 評価関数\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    モデルの性能を交差検証で評価する関数\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()  # Accuracy\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()    # ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# 評価スコアを取得\n",
        "cv_results = evaluate_model_cv(best_model_svm, X_selected, y_selected, cv=cv)\n",
        "cv_accuracy_svm = cv_results['accuracy']\n",
        "cv_roc_auc_svm = cv_results['roc_auc']\n",
        "\n",
        "# =====================================================\n",
        "# 11. Classification report / 分類レポート（交差検証予測を用いて）\n",
        "# =====================================================\n",
        "y_pred_cv = cross_val_predict(best_model_svm, X_selected, y_selected, cv=cv)\n",
        "report_svm = classification_report(y_selected, y_pred_cv)\n",
        "\n",
        "# =====================================================\n",
        "# 12. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 Tuned LinearSVC Evaluation\")\n",
        "print(\"📌 Best Parameters:\", best_params_svm)               # 最適パラメータ表示\n",
        "print(\"📈 Mean CV Accuracy:\", cv_accuracy_svm)              # 平均Accuracy\n",
        "print(\"📈 Mean CV ROC AUC:\", cv_roc_auc_svm)                # 平均ROC AUC\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", report_svm)  # 分類レポート"
      ],
      "metadata": {
        "id": "aee54xdmDZre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📋 SVM Performance Comparison Table / SVM性能比較表  \n",
        "\n",
        "| Evaluation Metric / 評価指標             | Before Tuning / 調整前 | After Tuning / 調整後 |\n",
        "| ------------------------------------ | ------------------- | ------------------ |\n",
        "| **Mean CV Accuracy / 平均精度**          | 0.8148              | 0.8114             |\n",
        "| **Mean ROC AUC / 平均ROC AUC**         | 0.8657              | 0.8667             |  \n",
        "| **Precision (Class 1) / 適合率（クラス1）**  | 0.77                | 0.74               |\n",
        "| **Recall (Class 1) / 再現率（クラス1）**     | 0.73                | 0.78               |\n",
        "| **F1-score (Class 1) / F1スコア（クラス1）** | 0.75                | 0.76               |\n",
        "\n",
        "\n",
        "###🔍 Interpretation / 考察\n",
        "- Specifying class_weight='balanced' improved recall for the minority class (Class 1: survivors).\n",
        "\n",
        "- While the overall accuracy remained nearly the same, the F1-score for Class 1 increased, indicating better sensitivity to the minority class.\n",
        "\n",
        "- The ROC AUC score also improved slightly, showing a more balanced overall discrimination ability of the model.  \n",
        "\n",
        "- class_weight='balanced' を指定したことで、少数派クラス（クラス1：生存者）の再現率が向上しました。\n",
        "\n",
        "- 全体の精度（accuracy）はほぼ同じでしたが、クラス1のF1スコアが改善しており、少数派クラスへの感度が高まりました。\n",
        "\n",
        "- また、ROC AUC スコアもわずかに改善しており、モデル全体の識別性能がよりバランスの取れたものになっています。"
      ],
      "metadata": {
        "id": "3ue8TCCCAxlx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDni4bfrzfrp"
      },
      "source": [
        "### Tuned SVM Performance Summary / チューニング後のSVMモデルの性能\n",
        "\n",
        "By adjusting the class weight, the model becomes more sensitive to the minority class. Although accuracy slightly dropped, recall and F1-score for the survivors improved, which is crucial in imbalanced classification tasks like this one.\n",
        "\n",
        "クラス重みを調整したことで、生存者クラスに対する感度が高まり、再現率・F1スコアが改善しました。わずかな精度の低下は許容範囲内であり、少数クラスの誤判定を減らせたことは実用上重要です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3M81UE5xizg"
      },
      "source": [
        "##5.3.7 Ensemble Models: Voting & Stacking / アンサンブルモデル：VotingとStacking\n",
        "To enhance prediction performance, we implemented two ensemble learning methods: Voting and Stacking.\n",
        "\n",
        "- Voting combines predictions from multiple base models (e.g., SVM, Random Forest) using majority voting or averaged predicted probabilities.\n",
        "\n",
        "- Stacking is a method that uses the output predictions from base models as input features for a meta-model, which makes the final prediction. In this study, we compared four types of meta-models: (1) Logistic Regression, (2) Random Forest, (3) XGBoost, and (4) LightGBM.\n",
        "\n",
        "Since tree-based models such as Random Forest, XGBoost, and LightGBM are not affected by feature scaling, we applied standardization only to the SVM model via a pipeline. This allowed us to maintain consistency across models while preserving optimal preprocessing for each algorithm.\n",
        "\n",
        "予測性能の向上を目的として、Voting および Stacking の2種類のアンサンブル手法を実装しました。\n",
        "\n",
        "- Voting は、複数のベースモデル（例：SVM、ランダムフォレスト）の予測を、多数決または確率の平均により統合する方法です。\n",
        "\n",
        "- Stacking は、ベースモデルの出力結果を特徴量としてメタモデルに入力し、最終的な予測を行う手法です。今回、メタモデルとして（1）ロジスティック回帰、（2）ランダムフォレスト（3）XGBoost、(4)LightGBM の4種類を比較しました。\n",
        "\n",
        "ランダムフォレスト、XGBoost、LightGBMなどの決定木系モデルは、特徴量のスケーリングに影響されないため、SVMモデルのみに標準化処理を適用したパイプラインを使用しました。これにより、各モデルの特性に応じた前処理を行いつつ、アンサンブル全体としての整合性を確保しました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9MJYSjLGHuq"
      },
      "source": [
        "### 5.3.8 🧠 Voting Ensemble (Soft Voting with Standardized SVM)\n",
        "To fairly combine SVM with tree-based models (RF, XGBoost, LGBM), standardization was applied only to the SVM model using a pipeline. The final prediction is made by averaging the predicted probabilities (soft voting).\n",
        "\n",
        "ツリーベースのモデル（RF、XGBoost、LGBM）とSVMを適切に統合するため、SVMのみに標準化処理を施したパイプラインを使用しています。最終的な予測は、各モデルの予測確率の平均（ソフト投票）によって行います。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Feature and Target Setup / 特徴量と目的変数の準備\n",
        "# =====================================================\n",
        "# Define numerical and categorical features\n",
        "# 数値特徴量とカテゴリ特徴量を定義\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# Select features and target from preprocessed DataFrame\n",
        "# 前処理済みデータフレームから特徴量と目的変数を抽出\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Calibrated LinearSVC Pipeline / キャリブレーション付き SVM パイプライン\n",
        "# =====================================================\n",
        "# Build a preprocessing pipeline:\n",
        "# 数値特徴量は標準化し、カテゴリ特徴量はそのまま使う\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),   # Apply standardization to numerical features / 数値特徴量に標準化を適用\n",
        "    ('cat', 'passthrough', categorical_features)     # Use categorical features as is / カテゴリ特徴量はそのまま通過\n",
        "])\n",
        "\n",
        "# Build a pipeline with Calibrated LinearSVC\n",
        "# CalibratedClassifierCV で SVM に確率出力を付加（必要：VotingClassifier の soft voting 対応）\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),  # Preprocessing pipeline / 前処理パイプライン\n",
        "    ('svc', CalibratedClassifierCV(      # Calibration wrapper for probability estimates / 確率推定用のラッパー\n",
        "        LinearSVC(random_state=42, max_iter=5000),  # Base model: LinearSVC / 線形サポートベクターマシン\n",
        "        method='sigmoid',                           # Sigmoid calibration / シグモイド関数によるキャリブレーション\n",
        "        cv=5                                        # Inner CV for calibration / キャリブレーション用の内部交差検証\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 3. VotingClassifier (Soft Voting) / アンサンブル学習（ソフト投票）\n",
        "# =====================================================\n",
        "# Combine pre-trained and optimized models using soft voting\n",
        "# 最適化済みのモデルをソフト投票で結合し、アンサンブル学習を構築\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', best_model_rf_sel),             # Random Forest model / ランダムフォレストモデル\n",
        "        ('xgb', best_model_xgb),               # XGBoost model / XGBoost モデル\n",
        "        ('lgb', best_model_lgb),               # LightGBM model / LightGBM モデル\n",
        "        ('svm', svm_pipeline_calibrated)       # Calibrated SVM pipeline / キャリブレーション済みSVM\n",
        "    ],\n",
        "    voting='soft',   # Soft voting: use predicted probabilities / ソフト投票（確率で平均化）\n",
        "    n_jobs=-1        # Use all CPU cores / 全CPUコア使用\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define Cross-Validation and Evaluation / 交差検証と評価関数の定義\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified 5-fold CV / 層化5分割交差検証\n",
        "\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    モデルの性能を交差検証で評価する関数\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()     # Accuracy / 精度\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()       # ROC AUC / AUCスコア\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 5. Run Evaluation / モデルの評価を実行\n",
        "# =====================================================\n",
        "scores_voting = evaluate_model_cv(voting_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Generate Classification Report / 分類レポートの作成\n",
        "# =====================================================\n",
        "# Generate predictions using cross-validation / 交差検証を用いた予測を取得\n",
        "y_pred_voting = cross_val_predict(voting_clf, X_selected, y_selected, cv=cv)\n",
        "\n",
        "# Create classification report / 分類レポートを作成\n",
        "report_voting = classification_report(y_selected, y_pred_voting)\n",
        "\n",
        "# =====================================================\n",
        "# 7. Output Results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 VotingClassifier with Calibrated LinearSVC\")\n",
        "print(\"📈 CV Accuracy (mean):\", scores_voting['accuracy'])  # 平均精度\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_voting['roc_auc'])    # 平均ROC AUC\n",
        "print(\"📝 Classification Report (CV predictions):\\n\", report_voting)  # 分類レポートの表示"
      ],
      "metadata": {
        "id": "UNZZ6QVYot7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCnl4SH8E4Xj"
      },
      "source": [
        "### Voting Ensemble Model Evaluation / Votingアンサンブルモデルの評価と考察\n",
        "\n",
        "\n",
        "The VotingClassifier with Calibrated LinearSVC achieved strong performance with a mean cross-validated accuracy of approximately 84.2% and a ROC AUC of 0.88. This indicates the model effectively distinguishes between survivors and non-survivors in the Titanic dataset.\n",
        "\n",
        "The classification report shows:\n",
        "- High precision and recall for the majority class (non-survivors), with an F1-score of 0.87.\n",
        "- Reasonably good performance on the minority class (survivors), with a precision of 0.82 and recall of 0.76, resulting in an F1-score of 0.79.\n",
        "\n",
        "Overall, the model balances well between sensitivity and specificity, providing reliable predictions for both classes. This suggests that the ensemble approach combining Random Forest, XGBoost, LightGBM, and calibrated SVM successfully leverages complementary strengths of each model.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "VotingClassifier（キャリブレーション済みLinearSVCを含む）は、平均交差検証精度約84.2％、ROC AUCは0.88と高い性能を示しました。これはタイタニックの生存者予測において、良好にクラスを識別できていることを意味します。\n",
        "\n",
        "分類レポートからは以下が読み取れます：\n",
        "- 非生存者（多数クラス）に対して高い精度と再現率を持ち、F1スコアは0.87。\n",
        "- 生存者（少数クラス）に対しても比較的良い性能で、精度0.82、再現率0.76、F1スコアは0.79でした。\n",
        "\n",
        "全体として、感度（再現率）と特異度（精度）のバランスが良く、両クラスに対して信頼できる予測が可能です。このことから、Random Forest、XGBoost、LightGBM、キャリブレーション済みSVMを組み合わせたアンサンブル手法は、それぞれのモデルの強みを効果的に活かしていると考えられます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRo8XTHXx02g"
      },
      "source": [
        "### 📚 5.4.9 Stacking Ensemble (Meta-model: Logistic Regression, passthrough=False) / スタッキングアンサンブル（メタモデル：ロジスティック回帰、元特徴量は使用しない）\n",
        "\n",
        "In this stacking approach, the predictions of base models (Random Forest, XGBoost, LightGBM, SVM with standardization) are used as inputs to a logistic regression meta-model. The original raw features are not passed to the meta-model (passthrough=False), so the meta-model relies purely on the outputs of the base learners. This setup helps focus learning on the interactions among model predictions rather than original feature values.\n",
        "\n",
        "このスタッキングでは、各ベースモデル（RF、XGBoost、LGBM、標準化済みSVM）の予測結果のみをロジスティック回帰のメタモデルに入力し、最終予測を行います。passthrough=False により、生の特徴量はメタモデルに渡されず、ベースモデルの出力のみに基づいて学習が行われます。これは、生特徴量のノイズの影響を避けつつ、各モデルの予測の関係性を活かす構成です。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / 選択した特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable / 目的変数：生存か否か\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / 数値特徴量とカテゴリ特徴量の分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / 数値特徴量（標準化対象）\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / 残りはカテゴリ特徴量とする\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVM用の前処理付きパイプラインを定義\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),   # Standardize numerical features / 数値特徴量に標準化を適用\n",
        "    ('cat', 'passthrough', categorical_features)     # Pass categorical features without transformation / カテゴリ特徴量はそのまま通す\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),  # Preprocessing step / 前処理ステップ\n",
        "    ('svc', CalibratedClassifierCV(      # Calibration wrapper for probability output / 確率出力用のキャリブレーションラッパー\n",
        "        LinearSVC(random_state=42, max_iter=5000),  # Linear SVM model / 線形SVMモデル\n",
        "        method='sigmoid',                           # Sigmoid calibration / シグモイドキャリブレーション\n",
        "        cv=5                                        # Inner CV for calibration / キャリブレーション用の内部CV\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ベース学習器を定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),             # Random Forest / ランダムフォレスト\n",
        "    ('xgb', best_model_xgb),               # XGBoost / XGBoost\n",
        "    ('lgb', best_model_lgb),               # LightGBM / LightGBM\n",
        "    ('svm', svm_pipeline_calibrated)       # Calibrated SVM pipeline / キャリブレーション済みSVM\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model / メタモデルを定義\n",
        "# =====================================================\n",
        "final_model = LogisticRegression(max_iter=1000, random_state=42)  # Meta-model: Logistic Regression / メタモデル：ロジスティック回帰\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / スタッキング分類器の構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified 5-Fold Cross Validation / 層化5分割交差検証\n",
        "\n",
        "stacking_LR_clf = StackingClassifier(\n",
        "    estimators=estimators,          # Base models / ベースモデル群\n",
        "    final_estimator=final_model,    # Meta-model / メタモデル\n",
        "    passthrough=False,              # Do not pass raw features / 生の特徴量は渡さない\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Evaluation function / 評価用関数\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model using cross-validation / モデルの性能を交差検証で評価\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()     # Mean accuracy / 平均正解率\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()       # Mean ROC AUC / 平均ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / 評価の実行\n",
        "# =====================================================\n",
        "scores_stacking_LR = evaluate_model_cv(stacking_LR_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / 分類レポートの作成\n",
        "# =====================================================\n",
        "y_pred_stacking_LR = cross_val_predict(stacking_LR_clf, X_selected, y_selected, cv=cv)  # Cross-validated predictions / 交差検証予測\n",
        "report_stacking_LR = classification_report(y_selected, y_pred_stacking_LR)              # Create report / レポート作成\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"📌 StackingClassifier with Calibrated LinearSVC (Logistic Regression meta-model)\")\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_LR['accuracy'])   # Print mean accuracy / 平均正解率の表示\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_LR['roc_auc'])     # Print mean ROC AUC / 平均ROC AUCの表示\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_LR)          # Print classification report / 分類レポートの表示"
      ],
      "metadata": {
        "id": "qvyOEjAdZWPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5sH3et3yK_2"
      },
      "source": [
        "### 🔍 Analysis: Stacking with Logistic Regression (passthrough=False)\n",
        "\n",
        "The stacking ensemble using a logistic regression meta-model and calibrated LinearSVC as one of the base learners achieved a mean cross-validated accuracy of about 84.1% and an ROC AUC of approximately 0.88. This indicates strong overall predictive performance.\n",
        "\n",
        "Looking at the classification report, the model shows slightly better precision and recall for the majority class (label 0), achieving 0.85 precision and 0.90 recall, while for the minority class (label 1), precision is 0.82 and recall is 0.75. This suggests the model is more confident and accurate at identifying non-survivors but still performs well in detecting survivors.\n",
        "\n",
        "The macro-averaged F1 score of 0.83 confirms a balanced performance across classes, making this stacking approach effective in leveraging complementary strengths of different base models.\n",
        "\n",
        "---\n",
        "\n",
        "ロジスティック回帰のメタモデルとキャリブレーション付きLinearSVCを含むスタッキングアンサンブルは、平均交差検証精度が約84.1％、ROC AUCが約0.88と高い予測性能を示しました。\n",
        "\n",
        "分類レポートを見ると、多数クラス（0）の精度と再現率はそれぞれ0.85と0.90であり、少数クラス（1）は精度0.82、再現率0.75となっています。これはモデルが生存しなかった人（クラス0）をより確実に識別できている一方で、生存者（クラス1）も十分に検出できていることを意味します。\n",
        "\n",
        "マクロ平均のF1スコア0.83はクラス間のバランスの良い性能を示しており、多様なベースモデルの強みを効果的に組み合わせたスタッキング手法の有効性が確認できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEw9oeXp-Sd_"
      },
      "source": [
        "### 🔧 5.4.10 Tuned Logistic Regression Meta-Model for Stacking Ensemble / チューニング後のロジスティック回帰メタモデル（スタッキングアンサンブル）\n",
        "\n",
        "This section implements a stacking ensemble model using multiple base learners—including a calibrated linear SVM (LinearSVC)—and tunes a logistic regression model as the meta-learner using GridSearchCV. The base models are pre-trained and already tuned individually, and their predictions are stacked and passed to the logistic regression meta-model.\n",
        "The pipeline includes preprocessing for numerical features and handles categorical features directly. Cross-validation is used both for stacking and evaluation.\n",
        "\n",
        "このセクションでは、キャリブレーション済みの線形SVM（LinearSVC）を含む複数のベース学習器を用いたスタッキングアンサンブルを構築し、ロジスティック回帰をメタ学習器として GridSearchCV でチューニングします。\n",
        "各ベースモデルは個別にチューニング済みで、それらの予測結果を統合してロジスティック回帰に渡します。数値特徴量には前処理（標準化）を施し、カテゴリ特徴量はそのまま処理します。評価には交差検証を用います。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable\n",
        "#    特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features\n",
        "#    数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / 数値特徴量（標準化対象）\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / 残りはカテゴリ特徴量とする\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and calibrated SVM pipeline\n",
        "#    前処理とCalibrated SVMパイプラインを定義\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),      # Standardize numerical features / 数値特徴量の標準化\n",
        "    ('cat', 'passthrough', categorical_features)        # Pass categorical features / カテゴリ特徴量はそのまま通す\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),                 # Preprocessing step / 前処理ステップ\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),      # Linear SVC model / 線形SVMモデル\n",
        "        method='sigmoid',                               # Use sigmoid calibration / シグモイドキャリブレーション\n",
        "        cv=5                                             # Use 5-fold CV for calibration / キャリブレーション用CV\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models for stacking\n",
        "#    スタッキング用のベースモデルを定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),             # Random Forest / ランダムフォレスト\n",
        "    ('xgb', best_model_xgb),               # XGBoost / XGBoost\n",
        "    ('lgb', best_model_lgb),               # LightGBM / LightGBM\n",
        "    ('svm', svm_pipeline_calibrated)       # Calibrated LinearSVC / キャリブレーション済みLinearSVC\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model\n",
        "#    メタモデルを定義（ロジスティック回帰）\n",
        "# =====================================================\n",
        "final_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier\n",
        "#    スタッキング分類器を構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_LR_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model,\n",
        "    passthrough=False,             # Do not use original features / 生の特徴量は使わない\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Tune hyperparameters using GridSearchCV\n",
        "#    GridSearchCVでハイパーパラメータをチューニング\n",
        "# =====================================================\n",
        "param_grid = {\n",
        "    'final_estimator__C': [0.01, 0.1, 1, 10],\n",
        "    'final_estimator__penalty': ['l2'],\n",
        "    'final_estimator__solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "grid_stacking_stacking_LR = GridSearchCV(stacking_LR_clf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "grid_stacking_stacking_LR.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 8. Use best estimator from GridSearchCV\n",
        "#    GridSearchCVで得た最良モデルを使用\n",
        "# =====================================================\n",
        "best_model_stacking_LR_tuned = grid_stacking_stacking_LR.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 9. Define evaluation function\n",
        "#    評価用関数を定義（交差検証で性能確認）\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()     # Mean accuracy / 平均精度\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()       # Mean ROC AUC / 平均ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 10. Evaluate the best stacking model\n",
        "#     最良スタッキングモデルを評価\n",
        "# =====================================================\n",
        "scores_stacking_LR_tuned = evaluate_model_cv(best_model_stacking_LR_tuned, X_selected, y_selected, cv=cv)\n",
        "\n",
        "y_pred_stacking_LR_tuned = cross_val_predict(best_model_stacking_LR_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_LR_tuned = classification_report(y_selected, y_pred_stacking_LR_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results\n",
        "#     結果を出力\n",
        "# =====================================================\n",
        "print(\"📌 Tuned StackingClassifier with Calibrated LinearSVC (Logistic Regression meta-model)\")\n",
        "print(\"✅ Best Parameters:\", grid_stacking_stacking_LR.best_params_)\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_LR_tuned['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_LR_tuned['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_LR_tuned)"
      ],
      "metadata": {
        "id": "iANRHPLlmLj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywNvOpgXV9-b"
      },
      "source": [
        "### 🔍 Model Comparison / モデル比較結果\n",
        "\n",
        "| Evaluation Metric / 評価指標             | Before Tuning / 調整前 | After Tuning / 調整後 |\n",
        "| ------------------------------------ | ------------------- | ------------------ |\n",
        "| **Mean CV Accuracy / 平均精度**          | 0.8406              | 0.8406             |\n",
        "| **Mean ROC AUC / 平均ROC AUC**         | 0.8821              | 0.8818             |\n",
        "| **Precision (Class 1) / 適合率（クラス1）**  | 0.82                | 0.82               |\n",
        "| **Recall (Class 1) / 再現率（クラス1）**     | 0.75                | 0.74               |\n",
        "| **F1-score (Class 1) / F1スコア（クラス1）** | 0.78                | 0.78               |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MijKhYxYHEkv"
      },
      "source": [
        "### 🧠 🔍 Analysis: Tuned Stacking with Logistic Regression (passthrough=False) / チューニング済みスタッキング（ロジスティック回帰、passthrough=False）\n",
        "\n",
        "### Discussion\n",
        "\n",
        "- **Minimal difference in performance:**  \n",
        "  The accuracy and ROC AUC values are almost identical before and after tuning, indicating stable performance.\n",
        "\n",
        "- **Limited impact of hyperparameter tuning:**  \n",
        "  Tuning the logistic regression meta-model's parameters brought only marginal improvements, suggesting the default settings were already effective.\n",
        "\n",
        "- **Base learners are already well-optimized:**  \n",
        "  Since the base models perform strongly, fine-tuning the meta-model yields little gain. Exploring different meta-models or feature engineering may help improve performance further.\n",
        "\n",
        "- **Practical stability:**  \n",
        "  Both models demonstrate robust and reliable results suitable for practical use.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. チューニング前モデル  \n",
        "- CV精度とROC AUCはそれぞれ約0.84、0.88で安定している。  \n",
        "- クラス0のリコールは高く、クラス1はやや低めである。  \n",
        "\n",
        "### 2. チューニング後モデル  \n",
        "- ベストパラメータはC=0.1のL2正則化、solverはlbfgsと決定。  \n",
        "- 精度・ROC AUCともにチューニング前とほぼ変わらない。  \n",
        "- クラス1のリコールがわずかに下がったが、ほぼ同等の結果。\n",
        "\n",
        "### 総合考察  \n",
        "- ハイパーパラメータのチューニングは性能に大きな影響を与えず、モデルは安定している。  \n",
        "- ベースモデルがすでに良くチューニングされているため、メタモデルの調整効果は限定的。  \n",
        "- 性能向上を狙うには、別のメタモデルの導入や特徴量改善の検討が有効と思われる。  \n",
        "- 実務上はどちらのモデルも十分実用的で信頼できる。\n",
        "\n",
        "以上より、今回のチューニングは大幅な性能改善には繋がらなかったが、モデルの安定性を再確認できたと言える。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C82LMVbNH87m"
      },
      "source": [
        "###🌲 5.4.11 Stacking with Random Forest as Meta-Model / メタモデルにランダムフォレストを用いたスタッキング\n",
        "To explore the effect of non-linear meta-models in stacking, we replaced the logistic regression with a Random Forest classifier as the final estimator.\n",
        "This approach allows the stacking model to learn complex interactions among base model predictions, potentially improving classification performance.\n",
        "\n",
        "スタッキングのメタモデルにおいて、線形モデルであるロジスティック回帰の代わりに、非線形の学習が可能なランダムフォレストを採用しました。\n",
        "この変更により、各ベースモデルの予測出力に含まれる複雑な相互作用を学習し、より高い予測性能を引き出すことを目指しました。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m76l7resBkJi"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / 数値特徴量（標準化対象）\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / 残りはカテゴリ特徴量とする\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and Calibrated SVM pipeline / SVM用の前処理＋パイプラインを定義\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models / ベースモデルを定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (Random Forest) / メタモデルを定義（ランダムフォレスト）\n",
        "# =====================================================\n",
        "final_model_rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,           # 過学習抑制のため適度に制限\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier / スタッキング分類器を構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_RF_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_rf,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define evaluation function / 評価関数を定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / 評価の実行\n",
        "# =====================================================\n",
        "scores_stacking_RF = evaluate_model_cv(stacking_RF_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / 分類レポートの作成\n",
        "# =====================================================\n",
        "y_pred_stacking_RF = cross_val_predict(stacking_RF_clf, X_selected, y_selected, cv=cv)\n",
        "report_stacking_RF = classification_report(y_selected, y_pred_stacking_RF)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"🌲 StackingClassifier with Calibrated LinearSVC (Random Forest meta-model)\")\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_RF['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_RF['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_RF)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Interpretation / 考察\n",
        "\n",
        "- **Solid Overall Performance**  \n",
        "  The model achieves strong and balanced performance, with high accuracy and a good ROC AUC, indicating reliable discrimination between classes.\n",
        "\n",
        "- **Class 1 (Survived) Detection**  \n",
        "  While slightly behind in recall compared to class 0, the precision (0.80) and F1-score (0.78) for class 1 show that the model handles minority class prediction reasonably well.\n",
        "\n",
        "- **Effectiveness of Random Forest as Meta-Model**  \n",
        "  Using a shallow Random Forest as the meta-learner captures non-linear relationships among base model outputs, without significantly increasing overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "- **堅実なモデル性能**  \n",
        "  精度・ROC AUC ともに高く、ベースモデルの出力をうまく活かしたスタッキングが機能していることがわかります。\n",
        "\n",
        "- **クラス1（生存者）の識別**  \n",
        "  Recall（再現率）はやや低め（0.77）ですが、Precision（適合率）0.80、F1スコア0.78と、バランスの取れた結果を示しています。\n",
        "\n",
        "- **メタモデルにランダムフォレストを採用した効果**  \n",
        "  Logistic Regression よりも柔軟にベースモデルの非線形な関係を捉えることができ、浅い深さで過学習も抑えています。\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fx9PDBN0Puvr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVNXsSfZQztX"
      },
      "source": [
        "###🌲 5.4.12 Stacking with Tuned Random Forest as Meta-Model / チューニング済みランダムフォレストを用いたスタッキング  \n",
        "To enhance the flexibility of the stacking ensemble, we replaced the logistic regression meta-model with a Random Forest and applied hyperparameter tuning. This approach allows the meta-model to learn non-linear relationships among base model outputs, potentially improving performance on complex decision boundaries.\n",
        "\n",
        "スタッキングアンサンブルの柔軟性を高めるため、メタモデルにランダムフォレストを採用し、ハイパーパラメータのチューニングを行いました。これにより、ベースモデルの出力間にある非線形な関係性を捉え、より複雑な判別境界での性能向上が期待されます。  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]  # Use selected features / 選択した特徴量を使用\n",
        "y_selected = df_fe4['Survived']         # Target variable / 目的変数：生存か否か\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']  # Numerical features (to be standardized) / 数値特徴量（標準化対象）\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]  # Categorical features / 残りはカテゴリ特徴量とする\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVM用の前処理付きパイプラインを定義\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),   # Standardize numerical features / 数値特徴量に標準化を適用\n",
        "    ('cat', 'passthrough', categorical_features)     # Pass categorical features without transformation / カテゴリ特徴量はそのまま通す\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),  # Preprocessing step / 前処理ステップ\n",
        "    ('svc', CalibratedClassifierCV(      # Calibration wrapper for probability output / 確率出力用のキャリブレーションラッパー\n",
        "        LinearSVC(random_state=42, max_iter=5000),  # Linear SVM model / 線形SVMモデル\n",
        "        method='sigmoid',                           # Sigmoid calibration / シグモイドキャリブレーション\n",
        "        cv=5                                        # Inner CV for calibration / キャリブレーション用の内部CV\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ベース学習器を定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),             # Random Forest / ランダムフォレスト\n",
        "    ('xgb', best_model_xgb),               # XGBoost / XGBoost\n",
        "    ('lgb', best_model_lgb),               # LightGBM / LightGBM\n",
        "    ('svm', svm_pipeline_calibrated)       # Calibrated SVM pipeline / キャリブレーション済みSVM\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (Random Forest) / メタモデルを定義（ランダムフォレスト）\n",
        "# =====================================================\n",
        "final_model_rf_tuned = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / スタッキング分類器の構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified 5-Fold CV / 層化5分割交差検証\n",
        "\n",
        "stacking_RF_clf_tuned = StackingClassifier(\n",
        "    estimators=estimators,          # Base models / ベースモデル群\n",
        "    final_estimator=final_model_rf_tuned,  # Meta-model: Random Forest / メタモデル：ランダムフォレスト\n",
        "    passthrough=False,              # Do not pass raw features / 生の特徴量は渡さない\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Hyperparameter tuning using GridSearchCV / ハイパーパラメータチューニング\n",
        "# =====================================================\n",
        "param_grid_rf = {\n",
        "    'final_estimator__n_estimators': [50],   # Number of trees / 木の数\n",
        "    'final_estimator__max_depth': [5, None],     # Max depth of trees / 木の深さ制限\n",
        "    'final_estimator__min_samples_split': [5, 10],  # Minimum samples to split node / 分割の最小サンプル数\n",
        "    'final_estimator__min_samples_leaf': [1, 2]     # Minimum samples at leaf node / 葉ノードの最小サンプル数\n",
        "}\n",
        "\n",
        "grid_stacking_RF = GridSearchCV(\n",
        "    stacking_RF_clf_tuned,\n",
        "    param_grid_rf,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_stacking_RF.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 8. Use best estimator from GridSearchCV / 最良モデルを取得\n",
        "# =====================================================\n",
        "best_model_stacking_RF_tuned = grid_stacking_RF.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 9. Define evaluation function / 評価用関数を定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    \"\"\"\n",
        "    Evaluate model performance using cross-validation.\n",
        "    モデルを交差検証で評価する関数\n",
        "    \"\"\"\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()   # 平均正解率\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()     # 平均ROC AUC\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 10. Evaluate the best tuned stacking model / チューニング済みモデルを評価\n",
        "# =====================================================\n",
        "scores_stacking_RF_tuned = evaluate_model_cv(best_model_stacking_RF_tuned, X_selected, y_selected, cv)\n",
        "\n",
        "y_pred_stacking_RF_tuned = cross_val_predict(best_model_stacking_RF_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_RF_tuned = classification_report(y_selected, y_pred_stacking_RF_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 11. Output results / 結果を出力\n",
        "# =====================================================\n",
        "print(\"🌲 Tuned StackingClassifier with Calibrated LinearSVC (Random Forest meta-model)\")\n",
        "print(\"✅ Best Parameters:\", grid_stacking_RF.best_params_)\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_RF_tuned['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_RF_tuned['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_RF_tuned)"
      ],
      "metadata": {
        "id": "qqcImYXgQqr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgN_V0VMVZX3"
      },
      "source": [
        "### 🔍 Model Comparison / モデル比較結果\n",
        "\n",
        "| 🧪 Evaluation Metric / 評価指標                   | 🔹 Before Tuning / 調整前 | 🔸 After Tuning / 調整後 |\n",
        "|--------------------------------------------------|---------------------------|--------------------------|\n",
        "| ✅ Mean CV Accuracy / 平均精度                   | 0.8384                    | 0.8327                   |\n",
        "| 📈 Mean ROC AUC / 平均ROC AUC                    | 0.8763                    | 0.8698                   |\n",
        "| 🎯 Precision (Class 1) / 適合率（クラス1）       | 0.80                      | 0.79                     |\n",
        "| 🔁 Recall (Class 1) / 再現率（クラス1）          | 0.77                      | 0.77                     |\n",
        "| 🧮 F1-score (Class 1) / F1スコア（クラス1）      | 0.78                      | 0.78                     |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Discussion\n",
        "\n",
        "The results show that tuning the Random Forest meta-model slightly reduced the overall accuracy and ROC AUC, though class-level metrics remained stable. In particular:\n",
        "\n",
        "- **Performance Trade-off**:  \n",
        "  After tuning, accuracy dropped marginally by ~0.0056 and ROC AUC by ~0.0065. This small decrease may result from the constraints introduced (e.g., limiting `max_depth=5`) to improve generalization.\n",
        "\n",
        "- **Class-wise Stability**:  \n",
        "  Precision, recall, and F1-score remained almost identical for both classes. This indicates that tuning didn’t significantly impact how well the model distinguished between survivors and non-survivors.\n",
        "\n",
        "- **Interpretability vs. Performance**:  \n",
        "  The tuned model might generalize better to unseen data, despite slightly lower CV metrics. Simplifying the meta-model (via controlled depth and split criteria) enhances interpretability and reduces overfitting risk.\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Summary\n",
        "\n",
        "While tuning led to a minor decrease in performance metrics, the model’s stability across classes and potential for improved generalization make the trade-off acceptable—especially when interpretability and robustness are priorities.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 考察\n",
        "\n",
        "結果から見ると、ランダムフォレストのメタモデルをチューニングしたことで、全体の正解率およびROC AUCがやや低下しましたが、クラスごとの指標には大きな変化はありませんでした。\n",
        "\n",
        "- **性能のトレードオフ**：  \n",
        "  精度は約0.0056、ROC AUCは約0.0065低下しました。これは、`max_depth=5`などの制限によって過学習を防ぎ、汎化性能を向上させたことによると考えられます。\n",
        "\n",
        "- **クラス単位での安定性**：  \n",
        "  クラス0・1それぞれのF1スコアは変わらず（0.87 / 0.78）、分類性能自体に大きな変動は見られませんでした。\n",
        "\n",
        "- **解釈性と性能のバランス**：  \n",
        "  木の深さなどを制限したことで、モデルの解釈性が向上し、未知データに対する頑健性も期待できます。わずかな精度の低下を受け入れてでも、過学習を抑えるメリットは大きいです。\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 総括\n",
        "\n",
        "パフォーマンスは微減したものの、モデルの安定性と汎化性の向上が期待され、実務的には有効なチューニングといえます。特に、解釈性や過学習対策を重視する場合には、有力なアプローチとなるでしょう。"
      ],
      "metadata": {
        "id": "oYoMnqv_Xnw1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96WuQSw5wZq"
      },
      "source": [
        "\n",
        "### 🌟 5.4.13 Stacking with XGBoost Meta-Model (Default Settings) / XGBoost をメタモデルに用いたスタッキング（デフォルト設定）  \n",
        "\n",
        "To explore the effect of a powerful non-linear meta-model, we replaced the logistic regression with XGBoost in the stacking ensemble. This setup aims to capture complex interactions among the base model outputs without any hyperparameter tuning.\n",
        "\n",
        "強力な非線形メタモデルの効果を確認するため、スタッキングのメタモデルとしてロジスティック回帰の代わりに XGBoost を使用しました。この構成では、ハイパーパラメータの調整を行わず、ベースモデルの出力間に存在する複雑な相互作用の学習を狙います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak6AHMdA5c_Z"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and Calibrated SVM pipeline / SVM用の前処理＋パイプラインを定義\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models / ベースモデルを定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (XGBoost) / メタモデルを定義（XGBoost）\n",
        "# =====================================================\n",
        "final_model_xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier / スタッキング分類器を構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_XGB_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_xgb,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define evaluation function / 評価関数を定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / 評価の実行\n",
        "# =====================================================\n",
        "scores_stacking_XGB = evaluate_model_cv(stacking_XGB_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / 分類レポートの作成\n",
        "# =====================================================\n",
        "y_pred_stacking_XGB = cross_val_predict(stacking_XGB_clf, X_selected, y_selected, cv=cv)\n",
        "report_stacking_XGB = classification_report(y_selected, y_pred_stacking_XGB)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"⚡ StackingClassifier with Calibrated LinearSVC (XGBoost meta-model)\")\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_XGB['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_XGB['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_XGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insight (Pre-Tuning Stage) / 考察（チューニング前）\n",
        "\n",
        "The StackingClassifier using XGBoost as a meta-model showed comparable results to those using logistic regression or random forest in terms of precision (0.79) and F1-score (0.77).\n",
        "However, its recall (0.76) was slightly lower.\n",
        "\n",
        "This means the model is more conservative when predicting positive cases (i.e., predicting someone survived). It avoids making incorrect positive predictions (i.e., calling someone \"survived\" when they didn't), which helps reduce false positives.\n",
        "But as a result, it ends up missing some actual survivors, increasing the number of false negatives.\n",
        "\n",
        "In practical terms, the model is careful when saying \"this person survived\", but might miss some who actually did.\n",
        "\n",
        "---\n",
        "\n",
        "XGBoost をメタモデルとしたスタッキング分類器は、**適合率（0.79）やF1スコア（0.77）**ではロジスティック回帰やランダムフォレストと同等の性能を示しました。\n",
        "しかし、**再現率（0.76）**がわずかに低下しました。\n",
        "\n",
        "これは、モデルが「この人は生存した」と判断する際に、より慎重な姿勢を取っていることを意味します。つまり、誤って「生存」と予測する（偽陽性）ことを避ける一方で、実際には生存していた人を「死亡」と予測してしまう（偽陰性）ケースが増える可能性があります。\n",
        "\n",
        "実際には生き延びた人を一部見逃してしまうリスクがあるものの、「生存」と予測した人の精度は高い、という保守的な予測スタイルを持っています。"
      ],
      "metadata": {
        "id": "urUFOtp0sQdm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX0OI4WY6bnH"
      },
      "source": [
        "### 🧪 5.4.14 Stacking with Tuned XGBoost Meta-Model / スタッキング（XGBoostメタモデル、チューニング付き）  \n",
        "This section demonstrates how to build a stacking ensemble classifier using multiple base learners and an XGBoost model as the meta-model. The XGBoost meta-model undergoes hyperparameter tuning to optimize its performance. The tuning process leverages cross-validation to find the best combination of parameters, improving the overall predictive accuracy and robustness of the stacking ensemble.  \n",
        "  \n",
        "本節では、複数のベース学習器を組み合わせ、メタモデルにXGBoostを用いたスタッキング分類器の構築方法を示します。XGBoostのメタモデルはハイパーパラメータのチューニングを実施し、クロスバリデーションを用いて最適なパラメータの組み合わせを探索します。これにより、スタッキング全体の予測精度とモデルの安定性を向上させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w32KjlUj6hZf"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVM用の前処理付きパイプライン\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ベースモデル群を定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (XGBoost) / メタモデルにXGBoostを設定\n",
        "# =====================================================\n",
        "final_model_xgb_tuned = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',  # 警告回避のために設定\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / スタッキング分類器を構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_XGB_clf_tuned = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_xgb_tuned,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define hyperparameter grid for XGBoost meta-model / ハイパーパラメータグリッド\n",
        "# =====================================================\n",
        "param_grid_xgb = {\n",
        "    'final_estimator__n_estimators': [50, 100],\n",
        "    'final_estimator__max_depth': [3, 5],\n",
        "    'final_estimator__learning_rate': [0.01, 0.1],\n",
        "    'final_estimator__subsample': [0.7, 1.0],\n",
        "    'final_estimator__colsample_bytree': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run GridSearchCV for tuning / GridSearchCVでチューニング実行\n",
        "# =====================================================\n",
        "grid_stacking_XGB = GridSearchCV(\n",
        "    stacking_XGB_clf_tuned,\n",
        "    param_grid_xgb,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_stacking_XGB.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Get best estimator from tuning / 最良モデルを取得\n",
        "# =====================================================\n",
        "best_model_stacking_XGB_tuned = grid_stacking_XGB.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 10. Define evaluation function / 評価関数を定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 11. Evaluate tuned model / チューニング済みモデルの評価\n",
        "# =====================================================\n",
        "scores_stacking_XGB_tuned = evaluate_model_cv(best_model_stacking_XGB_tuned, X_selected, y_selected, cv)\n",
        "\n",
        "y_pred_stacking_XGB_tuned = cross_val_predict(best_model_stacking_XGB_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_XGB_tuned = classification_report(y_selected, y_pred_stacking_XGB_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 12. Output results / 結果を出力\n",
        "# =====================================================\n",
        "print(\"🌲 Tuned StackingClassifier with Calibrated LinearSVC (XGBoost meta-model)\")\n",
        "print(\"✅ Best Parameters:\", grid_stacking_XGB.best_params_)\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_XGB_tuned['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_XGB_tuned['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_XGB_tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔍 Model Comparison / モデル比較結果\n",
        "\n",
        "| 🧪 Evaluation Metric / 評価指標         | 🔹 Before Tuning / 調整前 | 🔸 After Tuning / 調整後 |\n",
        "| ----------------------------------- | ---------------------- | --------------------- |\n",
        "| ✅ Mean CV Accuracy / 平均精度           | 0.829                  | 0.836                 |\n",
        "| 📈 Mean ROC AUC / 平均ROC AUC         | 0.872                  | 0.874                 |\n",
        "| 🎯 Precision (Class 1) / 適合率（クラス1）  | 0.79                   | 0.82                  |\n",
        "| 🔁 Recall (Class 1) / 再現率（クラス1）     | 0.76                   | 0.73                  |\n",
        "| 🧮 F1-score (Class 1) / F1スコア（クラス1） | 0.77                   | 0.77                  |\n",
        "            |\n"
      ],
      "metadata": {
        "id": "7oKAKFx8xTpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance After Tuning / チューニング後のモデル性能\n",
        "\n",
        "After tuning, the XGBoost meta-model achieved slightly higher accuracy and ROC AUC, along with improved precision for the positive class (0.82).\n",
        "However, recall dropped from 0.76 to 0.73, meaning the model became more conservative in predicting survivors—it makes fewer false positives, but misses more actual survivors.\n",
        "\n",
        "This reflects a trade-off:\n",
        "\n",
        "🔼 Better precision (more correct \"survived\" predictions)\n",
        "\n",
        "🔽 Lower recall (more missed survivors)\n",
        "\n",
        "Thus, the model is more cautious in predicting survival, which may be preferable if the cost of a false positive (incorrectly predicting survival) is higher.\n",
        "\n",
        "---\n",
        "チューニング後のXGBoostメタモデルは、精度とROC AUCがわずかに向上し、クラス1（生存者）の適合率も 0.79 → 0.82 に改善されました。\n",
        "一方で、再現率は 0.76 → 0.73 に低下し、生存者を「死亡」と誤分類する割合がやや増加しました。\n",
        "\n",
        "これは、モデルが「生存」と判断する際により慎重になり、偽陽性（誤って生存と判断）を減らす一方で、偽陰性（実際には生存していた人を見逃す）が増えることを意味します。\n",
        "\n",
        "つまり、次のようなトレードオフが見られます：\n",
        "\n",
        "🔼 適合率の向上（「生存」と予測した人の正確性が高い）\n",
        "\n",
        "🔽 再現率の低下（実際に生存した人の見逃しが増える）\n",
        "\n",
        "これは、誤って「生存」と予測するリスクを減らしたいケースでは有効ですが、すべての生存者を確実に見つけたいケースでは再現率の低下に注意が必要です。\n"
      ],
      "metadata": {
        "id": "2VrgkUs6yY7a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAohKwVf9MQV"
      },
      "source": [
        "### 🔆5.4.15 Stacking with LightGBM as Meta-Model / メタモデルに LightGBM を用いたスタッキング  \n",
        "To assess the performance of a more expressive meta-model, we constructed a stacking classifier using LightGBM as the final estimator. LightGBM is a gradient boosting framework that can capture complex non-linear relationships in the base model predictions. This approach is aimed at leveraging the strengths of both ensemble methods and boosting for improved generalization.\n",
        "\n",
        "より表現力の高いメタモデルの性能を評価するために、スタッキング分類器のメタ推定器として LightGBM を採用しました。LightGBM は勾配ブースティングに基づくフレームワークであり、ベースモデルの予測に含まれる複雑な非線形関係を捉えることが可能です。このアプローチにより、アンサンブルとブースティングの両方の強みを活かし、より高い汎化性能を目指します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eygtYnNp87Qk"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Separate numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define preprocessing and Calibrated SVM pipeline / SVM用の前処理＋パイプラインを定義\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base models / ベースモデルを定義\n",
        "# ※ best_model_rf_sel, best_model_xgb, best_model_lgb は既にチューニング済みとします\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (LightGBM) / メタモデルを定義（LightGBM）\n",
        "# =====================================================\n",
        "final_model_lgb = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Build StackingClassifier / スタッキング分類器を構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_LGB_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_lgb,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define evaluation function / 評価関数を定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run evaluation / 評価の実行\n",
        "# =====================================================\n",
        "scores_stacking_LGB = evaluate_model_cv(stacking_LGB_clf, X_selected, y_selected, cv)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Generate classification report / 分類レポートの作成\n",
        "# =====================================================\n",
        "y_pred_stacking_LGB = cross_val_predict(stacking_LGB_clf, X_selected, y_selected, cv=cv)\n",
        "report_stacking_LGB = classification_report(y_selected, y_pred_stacking_LGB)\n",
        "\n",
        "# =====================================================\n",
        "# 10. Output results / 結果の出力\n",
        "# =====================================================\n",
        "print(\"💡 StackingClassifier with Calibrated LinearSVC (LightGBM meta-model)\")\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_LGB['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_LGB['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_LGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo5iKl1q-Yn_"
      },
      "source": [
        "### Default LightGBM Stacking Model Results / デフォルトLightGBMスタッキングモデルの結果\n",
        "\n",
        "The LightGBM meta-model achieved decent performance (CV Accuracy: 0.8249, ROC AUC: 0.8668). However, compared to other meta-models like Random Forest and XGBoost, it showed slightly lower recall for the positive class (0.75). This means that the model was slightly more conservative in predicting survivors, possibly leading to more false negatives.\n",
        "\n",
        "Despite having the same precision (0.79) and F1-score (0.77) as XGBoost before tuning, its lower recall indicates it may miss some true survivors. Therefore, while LightGBM is computationally efficient, it might not be the best choice if the goal is to minimize missed survivors.\n",
        "\n",
        "---\n",
        "\n",
        "LightGBM をメタモデルに使用したスタッキング分類器は、まずまずの性能（平均精度 0.8249、ROC AUC 0.8668）を示しました。ただし、他のメタモデル（特にランダムフォレストやXGBoost）と比較すると、陽性（生存）クラスの再現率（0.75）がやや低くなっています。\n",
        "\n",
        "これは、LightGBMが生存者を予測する際にやや慎重な傾向を持ち、偽陽性を抑えつつも、真の生存者を見逃す可能性（偽陰性の増加）があることを意味します。精度やF1スコアはXGBoost（調整前）と同等ですが、再現率が低いため、「生存者の見逃しを減らす」ことが重要なタスクにおいては、最適な選択ではないかもしれません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCgsLnwB_D8-"
      },
      "source": [
        "### 5.4.16 Tuned LightGBM Meta-Model for Stacking Ensemble / チューニング後のLightGBMメタモデル（スタッキングアンサンブル）\n",
        "\n",
        "This section focuses on the implementation and evaluation of the LightGBM meta-model used as the final estimator in the stacking ensemble after hyperparameter tuning. By optimizing the LightGBM parameters, we aim to enhance the overall predictive performance of the ensemble, particularly improving metrics like accuracy and ROC AUC.\n",
        "\n",
        "このセクションでは、ハイパーパラメータの調整を経たLightGBMメタモデルをスタッキングアンサンブルの最終推定器として実装・評価します。LightGBMのパラメータを最適化することで、精度やROC AUCなどの指標向上を目指し、アンサンブル全体の予測性能を高めます。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT0zNRWG-Hpd"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Define features and target variable / 特徴量と目的変数を定義\n",
        "# =====================================================\n",
        "X_selected = df_fe4[selected_features]\n",
        "y_selected = df_fe4['Survived']\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define numerical and categorical features / 数値特徴量とカテゴリ特徴量を分類\n",
        "# =====================================================\n",
        "numerical_features = ['Fare','Fare_log', 'Age', 'Family', 'TicketGroupSize']\n",
        "categorical_features = [col for col in X_selected.columns if col not in numerical_features]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Define pipeline for SVM (with preprocessing) / SVM用の前処理付きパイプライン\n",
        "# =====================================================\n",
        "preprocessor_svm = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', 'passthrough', categorical_features)\n",
        "])\n",
        "\n",
        "svm_pipeline_calibrated = Pipeline([\n",
        "    ('preprocessor', preprocessor_svm),\n",
        "    ('svc', CalibratedClassifierCV(\n",
        "        LinearSVC(random_state=42, max_iter=5000),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    ))\n",
        "])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Define base learners / ベースモデル群を定義\n",
        "# =====================================================\n",
        "estimators = [\n",
        "    ('rf', best_model_rf_sel),\n",
        "    ('xgb', best_model_xgb),\n",
        "    ('lgb', best_model_lgb),\n",
        "    ('svm', svm_pipeline_calibrated)\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 5. Define meta-model (LightGBM) / メタモデルにLightGBMを設定\n",
        "# =====================================================\n",
        "final_model_lgb = LGBMClassifier(\n",
        "    objective='binary',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Create StackingClassifier / スタッキング分類器を構築\n",
        "# =====================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "stacking_LGB_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_model_lgb,\n",
        "    passthrough=False,\n",
        "    cv=cv,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 7. Define hyperparameter grid for LightGBM meta-model / LightGBMのハイパーパラメータグリッド\n",
        "# =====================================================\n",
        "param_grid_lgb = {\n",
        "    'final_estimator__n_estimators': [100],\n",
        "    'final_estimator__learning_rate': [0.1],\n",
        "    'final_estimator__max_depth': [3, 5],\n",
        "    'final_estimator__num_leaves': [15, 31],\n",
        "    'final_estimator__subsample': [0.7],\n",
        "    'final_estimator__colsample_bytree': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "# =====================================================\n",
        "# 8. Run GridSearchCV for tuning / GridSearchCVでチューニング実行\n",
        "# =====================================================\n",
        "grid_stacking_LGB = GridSearchCV(\n",
        "    stacking_LGB_clf,\n",
        "    param_grid_lgb,\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_stacking_LGB.fit(X_selected, y_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 9. Get best estimator from tuning / 最良モデルを取得\n",
        "# =====================================================\n",
        "best_model_stacking_LGB_tuned = grid_stacking_LGB.best_estimator_\n",
        "\n",
        "# =====================================================\n",
        "# 10. Define evaluation function / 評価関数を定義\n",
        "# =====================================================\n",
        "def evaluate_model_cv(model, X, y, cv):\n",
        "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
        "    roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc').mean()\n",
        "    return {'accuracy': accuracy, 'roc_auc': roc_auc}\n",
        "\n",
        "# =====================================================\n",
        "# 11. Evaluate tuned model / チューニング済みモデルの評価\n",
        "# =====================================================\n",
        "scores_stacking_LGB_tuned = evaluate_model_cv(best_model_stacking_LGB_tuned, X_selected, y_selected, cv)\n",
        "\n",
        "y_pred_stacking_LGB_tuned = cross_val_predict(best_model_stacking_LGB_tuned, X_selected, y_selected, cv=cv)\n",
        "report_stacking_LGB_tuned = classification_report(y_selected, y_pred_stacking_LGB_tuned)\n",
        "\n",
        "# =====================================================\n",
        "# 12. Output results / 結果を出力\n",
        "# =====================================================\n",
        "print(\"🌿 Tuned StackingClassifier with Calibrated LinearSVC (LightGBM meta-model)\")\n",
        "print(\"✅ Best Parameters:\", grid_stacking_LGB.best_params_)\n",
        "print(\"📈 CV Accuracy (mean):\", scores_stacking_LGB_tuned['accuracy'])\n",
        "print(\"📈 CV ROC AUC (mean):\", scores_stacking_LGB_tuned['roc_auc'])\n",
        "print(\"📝 Classification Report:\\n\", report_stacking_LGB_tuned)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔍 Model Comparison / モデル比較結果\n",
        "\n",
        "| 🧪 Evaluation Metric / 評価指標         | 💡 Before Tuning / 調整前 | 🌿 After Tuning / 調整後 |\n",
        "| ----------------------------------- | ---------------------- | --------------------- |\n",
        "| ✅ Mean CV Accuracy / 平均精度           | 0.8249                 | 0.8249                |\n",
        "| 📈 Mean ROC AUC / 平均ROC AUC         | 0.8668                 | 0.8668                |\n",
        "| 🎯 Precision (Class 1) / 適合率（クラス1）  | 0.79                   | 0.79                  |\n",
        "| 🔁 Recall (Class 1) / 再現率（クラス1）     | 0.75                   | 0.75                  |\n",
        "| 🧮 F1-score (Class 1) / F1スコア（クラス1） | 0.77                   | 0.77                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "7O84F_QX8f-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Interpretation / 考察\n",
        "\n",
        "There was no change in performance after tuning the LightGBM meta-model. All key metrics — accuracy, ROC AUC, precision, recall, and F1-score — remained the same. This suggests that either:\n",
        "\n",
        "The initial hyperparameters were already near-optimal, or\n",
        "\n",
        "The tuning grid did not explore a significantly better combination.\n",
        "\n",
        "---\n",
        "\n",
        "LightGBM メタモデルのチューニング後も、すべての評価指標に変化はありませんでした。精度・ROC AUC・適合率・再現率・F1スコアはいずれもチューニング前と同じです。\n",
        "このことは以下のいずれかを示唆しています：\n",
        "\n",
        "初期のハイパーパラメータがすでにほぼ最適だった\n",
        "\n",
        "チューニングの際に使用したパラメータ範囲が不十分だった\n"
      ],
      "metadata": {
        "id": "IFZ4aQf49qRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 📊 Evaluation & Comparison / モデル評価と比較\n",
        "In this section, I evaluate multiple models using cross-validation.\n",
        "Key metrics such as accuracy, F1 score, ROC AUC, and training/prediction time are measured.\n",
        "Additionally, I compare the cross-validation results with Kaggle leaderboard scores to assess generalization.\n",
        "\n",
        "このセクションでは、交差検証を用いて複数のモデルを評価します。\n",
        "評価指標として 正解率（Accuracy）、F1スコア、ROC AUC に加え、学習・推論時間も測定します。\n",
        "また、交差検証によるスコアとKaggle提出時のスコアを比較し、モデルの汎化性能についても検討します。"
      ],
      "metadata": {
        "id": "7vp4jp2Xlc2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1  ✅ Model Evaluation Summary / モデル評価サマリー\n",
        "\n",
        "以下の表は、複数の機械学習モデルについて、精度（Accuracy）、F1スコア、ROC AUCスコア、学習時間および推論時間を比較したものです。  \n",
        "各モデルは5分割の層化交差検証(Stratified K-Fold CV)を用いて評価されており、  \n",
        "分類性能と計算コストの両面から総合的な比較を行っています。\n",
        "\n",
        "This table summarizes the evaluation of multiple machine learning models in terms of accuracy, F1 score, ROC AUC, training time, and prediction time.  \n",
        "All models were assessed using 5-fold Stratified Cross-Validation, allowing a comprehensive comparison of classification performance and computational cost.\n"
      ],
      "metadata": {
        "id": "x2IGAj3mDZtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Function to evaluate model and measure time\n",
        "# モデルの評価と時間測定のための関数\n",
        "# ===============================================\n",
        "def evaluate_model_with_timing(model, X, y, cv, model_name=\"model\"):\n",
        "    result = {}  # Dictionary to store results / 結果を格納する辞書\n",
        "    result['Model'] = model_name  # Store model name / モデル名を格納\n",
        "\n",
        "    # Measure training time (explicitly fit the model) / 学習時間（明示的にfit）\n",
        "    start_train = time.time()\n",
        "    model.fit(X, y)\n",
        "    end_train = time.time()\n",
        "    result['Train Time (s)'] = end_train - start_train  # Calculate training time / 学習時間を計算\n",
        "\n",
        "    # Measure prediction time (using cross-validation) / 推論時間（cross_val_predict）\n",
        "    start_pred = time.time()\n",
        "    y_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)  # Prediction (with cross-validation) / 推論（予測）\n",
        "    end_pred = time.time()\n",
        "    result['Predict Time (s)'] = end_pred - start_pred  # Calculate prediction time / 推論時間を計算\n",
        "\n",
        "    # Evaluation metrics / 評価指標\n",
        "    result['Accuracy'] = accuracy_score(y, y_pred)  # Accuracy / 正解率\n",
        "    result['F1 Score'] = f1_score(y, y_pred)  # F1 Score / F1スコア\n",
        "    result['ROC AUC'] = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1).mean()  # ROC AUC\n",
        "    result['Classification Report'] = classification_report(y, y_pred, output_dict=False)  # Classification report / 分類レポート\n",
        "\n",
        "    return result  # Return the result / 結果を返す\n",
        "\n",
        "# ===============================================\n",
        "# Define models to compare\n",
        "# 比較するモデルを定義\n",
        "# ===============================================\n",
        "models = {\n",
        "    'Random Forest': best_model_rf_sel,  # Random Forest / ランダムフォレスト\n",
        "    'XGBoost': best_model_xgb,  # XGBoost / XGBoost\n",
        "    'LightGBM': best_model_lgb,  # LightGBM / LightGBM\n",
        "    'Calibrated LinearSVC': svm_pipeline_calibrated,  # Calibrated LinearSVC / キャリブレーションしたLinearSVC\n",
        "    'Voting': voting_clf,  # Voting Classifier (ensemble) / アンサンブル（投票）\n",
        "    'Stacking (LR meta)': best_model_stacking_LR_tuned,  # Stacking (LR meta) / スタッキング（LRメタ）\n",
        "    'Stacking (RF meta)': best_model_stacking_RF_tuned,  # Stacking (RF meta) / スタッキング（RFメタ）\n",
        "    'Stacking (XGB meta)': best_model_stacking_XGB_tuned,  # Stacking (XGB meta) / スタッキング（XGBメタ）\n",
        "    'Stacking (LGB meta)': best_model_stacking_LGB_tuned  # Stacking (LGB meta) / スタッキング（LGBメタ）\n",
        "}\n",
        "\n",
        "# ===============================================\n",
        "# Stratified KFold cross-validation setup and result storage\n",
        "# Stratified KFold交差検証の設定と結果の格納\n",
        "# ===============================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # StratifiedKFold (層化交差検証) / StratifiedKFold cross-validation\n",
        "results = []  # List to store results / 結果を格納するリスト\n",
        "\n",
        "# ===============================================\n",
        "# Evaluate all models\n",
        "# 全てのモデルを評価\n",
        "# ===============================================\n",
        "for name, model in models.items():\n",
        "    print(f\"🔍 Evaluating: {name}\")  # Print the current model being evaluated / 評価中のモデル名を表示\n",
        "    result = evaluate_model_with_timing(model, X_selected, y_selected, cv, model_name=name)  # Call the evaluation function / 評価関数を呼び出し\n",
        "    results.append(result)  # Append result to the list / 結果をリストに追加\n",
        "\n",
        "# ===============================================\n",
        "# Convert results to DataFrame and sort by accuracy\n",
        "# 結果をDataFrameに変換し、正解率でソート\n",
        "# ===============================================\n",
        "df_results = pd.DataFrame(results)  # Convert to DataFrame / DataFrameに変換\n",
        "df_results_sorted = df_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)  # Sort by accuracy in descending order / 正解率順に並び替え\n",
        "\n",
        "# ===============================================\n",
        "# Display results\n",
        "# 結果を表示\n",
        "# ===============================================\n",
        "print(\"\\n✅ モデル比較表（精度順）:\")  # Print the results / 結果を表示\n",
        "print(df_results_sorted[['Model', 'Accuracy', 'F1 Score', 'ROC AUC', 'Train Time (s)', 'Predict Time (s)']])  # Display evaluation metrics and time / 評価指標と時間を表示"
      ],
      "metadata": {
        "id": "NyaGay494kC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison Results / モデル比較結果\n",
        "\n",
        "The table below compares various models based on their **Accuracy**, **F1 Score**, **ROC AUC**, **Training Time**, and **Prediction Time**. When selecting models, it is important to consider these metrics holistically.\n",
        "\n",
        "以下の表は、各モデルの**精度 (Accuracy)**、**F1スコア (F1 Score)**、**ROC AUC**、**学習時間 (Training Time)**、**予測時間 (Prediction Time)**を比較したものです。モデル選定の際には、これらの指標を総合的に考慮することが重要です。\n",
        "\n",
        "| Model Name / モデル名             | Accuracy / 精度   | F1 Score / F1スコア   | ROC AUC / ROC AUC   | Train Time (s) / 学習時間 (秒) | Predict Time (s) / 予測時間 (秒) |\n",
        "|-----------------------------------|-------------------|-----------------------|---------------------|------------------------------|----------------------------------|\n",
        "| **Random Forest**                 | 0.847363          | 0.790769              | 0.876013            | 0.570739                     | 5.786143                        |\n",
        "| **Voting**                        | 0.841751          | 0.786039              | 0.881637            | 0.513798                     | 2.070606                        |\n",
        "| **Stacking (LR meta)**            | 0.840629          | 0.781538              | 0.881800            | 1.824270                     | 9.396360                        |\n",
        "| **XGBoost**                       | 0.839506          | 0.784962              | 0.886833            | 0.472127                     | 0.426812                        |\n",
        "| **LightGBM**                      | 0.838384          | 0.782477              | 0.880299            | 0.048291                     | 4.601200                        |\n",
        "| **Stacking (XGB meta)**           | 0.836139          | 0.773994              | 0.874314            | 1.841040                     | 9.379701                        |\n",
        "| **Stacking (RF meta)**            | 0.832772          | 0.779911              | 0.869783            | 1.870320                     | 10.905935                       |\n",
        "| **Stacking (LGB meta)**           | 0.824916          | 0.766467              | 0.866834            | 1.868337                     | 11.698572                       |\n",
        "| **Calibrated LinearSVC**          | 0.819304          | 0.756430              | 0.866038            | 0.667395                     | 1.026984                        |\n",
        "\n",
        "### Explanation / 解説\n",
        "\n",
        "- **Accuracy**: The proportion of correct predictions made by the model.  \n",
        "`Random Forest` showed the highest accuracy among all models.\n",
        "\n",
        "- **F1 Score**: The harmonic mean of Precision and Recall.  \n",
        "`XGBoost` had a relatively high F1 score, indicating it balances precision and recall well.\n",
        "\n",
        "- **ROC AUC**: The area under the ROC curve, which measures the performance of a classification model.  \n",
        "`XGBoost` achieved the highest ROC AUC, indicating its strong classification ability.\n",
        "\n",
        "- **Training Time**: The time it took to train the model.  \n",
        "`LightGBM` was exceptionally fast, taking only 0.048 seconds for training.\n",
        "\n",
        "- **Prediction Time**: The time it took for the model to make predictions.  \n",
        "`XGBoost` and `Voting` were very fast in terms of prediction time.\n",
        "\n",
        "### 解説\n",
        "\n",
        "- **Accuracy (精度)**: モデルが正しく予測した割合です。  \n",
        "`Random Forest` は全モデルの中で最も高い精度を示しました。\n",
        "\n",
        "- **F1 Score (F1スコア)**: 精度と再現率の調和平均です。  \n",
        "`XGBoost` は比較的高いF1スコアを持ち、精度と再現率のバランスが良いことを示しています。\n",
        "\n",
        "- **ROC AUC (ROC AUC)**: 受信者動作特性曲線（ROC曲線）の下の面積で、分類モデルの性能を測る指標です。  \n",
        "`XGBoost` は最も高いROC AUCを達成し、強力な分類能力を示しています。\n",
        "\n",
        "- **Training Time (学習時間)**: モデルの学習にかかった時間です。  \n",
        "`LightGBM` は非常に高速で、学習にわずか0.048秒しかかかりませんでした。\n",
        "\n",
        "- **Prediction Time (予測時間)**: モデルが予測を行うのにかかった時間です。  \n",
        "`XGBoost` と `Voting` は予測時間が非常に速いです。\n",
        "\n",
        "### Model Selection Considerations / モデル選定時の考慮事項\n",
        "\n",
        "- If accuracy is the priority, Random Forest or Voting are the top contenders.\n",
        "- If training and prediction time are more important (for real-time systems, for example), then XGBoost and LightGBM are better choices due to their fast processing times.\n",
        "- Stacking Models generally perform well in terms of accuracy but take longer to train and predict.\n",
        "- Model Selection should consider the trade-off between performance and computational efficiency.\n",
        "- Additionally, Stacking Models can offer better overall performance but are more computationally expensive.  \n",
        "\n",
        "- 精度が最優先であれば、Random Forest や Voting が最適な候補です。\n",
        "- 学習時間や予測時間がより重要な場合（例えば、リアルタイムシステムの場合）は、XGBoost や LightGBM が高速で処理できるため、これらのモデルが最適です。\n",
        "- Stacking 系のモデルは一般的に精度が高いですが、学習と予測に時間がかかります。\n",
        "- モデル選定は、性能と計算効率のトレードオフを考慮するべきです。\n",
        "- また、Stacking 系のモデルは総じて高い性能を発揮するものの、計算コストが高くなりがちです。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TWQUcLFP7gkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2🚢 Kaggle Scores Comparison Table / カグルスコア比較表\n",
        "\n",
        "| ✅ Model Type / モデル種別       | Model / モデル名                  | 🏅 Kaggle Score | 📝 Notes / 備考                                                                                      |\n",
        "| -------------------------- | ----------------------------- | --------------- | -------------------------------------------------------------------------------------------------- |\n",
        "| ✅ Single Model / 単独モデル     | Random Forest (tuned)         | **0.77990**     | 🌲 **Best single model. Strong balance of accuracy and robustness.** / 単独モデルで最高性能。精度と安定性のバランスが優秀。  |\n",
        "|                            | LightGBM (tuned)              | 0.74880         | ⚡ Very fast training but weaker generalization. / 学習は非常に高速だが、汎化性能はやや劣る。                            |\n",
        "|                            | XGBoost (tuned)               | 0.77033         | 🔥 Reliable and stable with solid generalization. / 安定した性能と汎化能力。                                   |\n",
        "|                            | SVM (Calibrated, tuned)       | 0.75598         | 🧭 Reasonable performance, but slightly lower than tree-based models. / 精度はまずまずだが、決定木系モデルよりやや劣る。   |\n",
        "| ✅ Voting Ensemble / アンサンブル | Voting (RF + XGB + LGB + SVM) | 0.77033         | 🗳️ Robust and stable by combining strengths of multiple models. / 複数モデルの強みを組み合わせて安定性を確保。          |\n",
        "| ✅ Stacking / スタッキング        | Meta: Logistic Regression     | 0.77272         | 📊 Interpretable and effective. Performs better than some single models. / 解釈しやすく、単独モデルより優れる場面もあり。 |\n",
        "|                            | Meta: XGBoost                 | **0.77990**     | 🚀 Matches best single model. Strong meta-learner. / 単体XGBoostと同等のスコア。強力なメタ学習器。                    |\n",
        "|                            | Meta: LightGBM                | 0.76076         | ⚙️ Improved from base LGB, but still below top models. / 単体LGBより改善したが、上位モデルには及ばない。                 |\n",
        "|                            | Meta: Random Forest           | 0.77272         | 🌳 Stable with good overall metrics, slightly lower than the top. / 安定しているが、スコアはやや劣る。              |\n"
      ],
      "metadata": {
        "id": "QOKe4KTEu06P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary / まとめ\n",
        "- The comparison of various machine learning models for the Titanic survival prediction task reveals several important insights:\n",
        "\n",
        "- Random Forest (tuned) achieved the highest Kaggle score (0.77990) among all models. It balances accuracy and robustness well, making it a strong standalone choice.\n",
        "\n",
        "- XGBoost, both as a single model and as a meta-model in stacking, also performed consistently well, matching the best score. This confirms its strong generalization ability.\n",
        "\n",
        "- Voting ensemble and Logistic Regression stacking achieved slightly lower scores, but showed stable performance, indicating they effectively aggregate diverse model strengths.\n",
        "\n",
        "- LightGBM, while extremely fast, had relatively lower scores. Even when used as a meta-model, the performance did not surpass Random Forest or XGBoost, suggesting it may be more sensitive to tuning or data structure.\n",
        "\n",
        "- Calibrated LinearSVC performed reasonably but did not outperform tree-based models, possibly due to linear limitations in a complex, non-linear problem like this one.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- タイタニック生存予測タスクにおける様々な機械学習モデルの比較から、いくつかの重要な知見が得られました。\n",
        "\n",
        "- チューニング済みのランダムフォレストが最も高いKaggleスコア（0.77990）を記録し、精度と安定性のバランスが良いため、単独モデルとして優秀です。\n",
        "\n",
        "- XGBoostは単独モデルとしてもスタッキングのメタモデルとしても安定した高い性能を示し、強い汎化能力が確認されました。\n",
        "\n",
        "- Votingアンサンブルやロジスティック回帰のスタッキングはややスコアが劣りますが、安定的な性能を示し、多様なモデルの強みをうまく統合していることが分かります。\n",
        "\n",
        "- LightGBMは非常に高速ですがスコアは比較的低く、メタモデルとして使用してもランダムフォレストやXGBoostを上回ることはなく、チューニングやデータ構造に影響されやすい可能性があります。\n",
        "\n",
        "- Calibrated LinearSVCはまずまずの性能ですが、非線形問題においては木構造モデルに劣る結果となり、線形モデルの限界が影響していると考えられます。\n",
        "\n"
      ],
      "metadata": {
        "id": "_8zWUBn7h1z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Comparison of Kaggle Scores and Cross-Validation Accuracy with Difference Labels / Kaggleスコアと交差検証精度の比較（差分ラベル付き）\n",
        "\n",
        "This bar chart compares the Kaggle leaderboard scores and cross-validation (CV) accuracies for each model.\n",
        "The numbers displayed next to the CV accuracy bars indicate the difference between CV accuracy and Kaggle score, which can help identify potential overfitting or model stability issues.\n",
        "\n",
        "この棒グラフは、各モデルのKaggleリーダーボードスコアと交差検証（CV）精度を比較しています。\n",
        "CV精度のバーの横に表示されている数値は、CV精度とKaggleスコアの差を表しており、過学習やモデルの安定性の問題を見極める手がかりになります。"
      ],
      "metadata": {
        "id": "bOmKvXhyZ82k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル性能データの準備\n",
        "data = {\n",
        "    \"Model\": [\n",
        "        \"Random Forest\",\n",
        "        \"LightGBM\",\n",
        "        \"XGBoost\",\n",
        "        \"SVM\",\n",
        "        \"Voting\",\n",
        "        \"Stacking (LR)\",\n",
        "        \"Stacking (XGB)\",\n",
        "        \"Stacking (LGB)\",\n",
        "        \"Stacking (RF)\"\n",
        "    ],\n",
        "    \"Kaggle Score\": [\n",
        "        0.77990,\n",
        "        0.74880,\n",
        "        0.77033,\n",
        "        0.75598,\n",
        "        0.77033,\n",
        "        0.77272,\n",
        "        0.77990,\n",
        "        0.76076,\n",
        "        0.77272\n",
        "    ],\n",
        "    \"CV Accuracy\": [\n",
        "        0.847363,\n",
        "        0.838384,\n",
        "        0.839506,\n",
        "        0.819304,\n",
        "        0.841751,\n",
        "        0.840629,\n",
        "        0.836139,\n",
        "        0.824916,\n",
        "        0.832772\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 差分計算\n",
        "df[\"Difference\"] = df[\"CV Accuracy\"] - df[\"Kaggle Score\"]\n",
        "\n",
        "# melt処理でSeaborn用データ整形\n",
        "df_plot = df.melt(id_vars=\"Model\", value_vars=[\"Kaggle Score\", \"CV Accuracy\"],\n",
        "                  var_name=\"Metric\", value_name=\"Score\")\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.set(style=\"whitegrid\")\n",
        "barplot = sns.barplot(data=df_plot, x=\"Score\", y=\"Model\", hue=\"Metric\")\n",
        "\n",
        "# 差分のラベルを追加\n",
        "for index, row in df.iterrows():\n",
        "    plt.text(\n",
        "        row[\"CV Accuracy\"] + 0.002,\n",
        "        index,\n",
        "        f'{row[\"Difference\"]:.3f}',\n",
        "        color='black',\n",
        "        va='center'\n",
        "    )\n",
        "\n",
        "plt.title(\"Comparison of Kaggle Score and CV Accuracy by Model with Difference Labels\", fontsize=14)\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.legend(title=\"Metric\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vs7RemOyXYhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📊 Model Performance Comparison / モデル性能比較（CV vs Kaggle）\n",
        "\n",
        "\n",
        "| ✅ **Model / モデル名** | 📈 **CV Accuracy** | 🏅 **Kaggle Score** | 📉 **Gap / 乖離** | 📝 **Notes / コメント**                                            |\n",
        "| ------------------ | ------------------ | ------------------- | --------------- | -------------------------------------------------------------- |\n",
        "| Random Forest      | 0.847              | 0.7799              | 0.0675          | ⚠️ High accuracy but gap indicates overfitting / 高精度だが過学習の懸念あり |\n",
        "| LightGBM           | 0.838              | 0.7488              | 0.0896          | ⚠️ Fast but shows significant drop / 高速だがCVとの乖離が大きい            |\n",
        "| XGBoost            | 0.840              | 0.7703              | 0.0692          | ✅ Balanced performance / バランスの良い性能                             |\n",
        "| SVM                | 0.819              | 0.7559              | 0.0633          | 🧭 Moderate generalization / まずまずの汎化性能                         |\n",
        "| Voting             | 0.842              | 0.7703              | 0.0715          | ❗ Stable but gap exists / 安定性はあるが乖離あり                          |\n",
        "| Stacking (LR)      | 0.841              | 0.7727              | 0.0680          | 📊 Interpretability with decent generalization / 解釈性とそこそこの汎化性能 |\n",
        "| Stacking (XGB)     | 0.836              | 0.7799              | 0.0562          | ✅ Best consistency across metrics / 最も安定したモデル                  |\n",
        "| Stacking (LGB)     | 0.825              | 0.7608              | 0.0642          | ⚙️ Improvement from base LGB / 単体LGBより改善                       |\n",
        "| Stacking (RF)      | 0.833              | 0.7727              | 0.0603          | 🌳 Stable stacking performance / 安定したスタッキング性能                  |\n",
        "\n"
      ],
      "metadata": {
        "id": "gphBMzSVVOrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion / 結論：\n",
        "Although the tuned Random Forest achieved the highest cross-validation accuracy (0.8474), its relatively large gap with the Kaggle score (0.7799) suggests potential overfitting. This makes it a risky choice as the final model, especially for deployment or unseen data.\n",
        "On the other hand, the Stacking model with XGBoost as meta-learner showed a smaller performance gap and consistent scores across validation and Kaggle.\n",
        "Therefore, it is safer and more reliable to select the Stacking (XGBoost meta) model as the final choice.  \n",
        "\n",
        "チューニング済みのランダムフォレストは交差検証で最高の精度（0.8474）を示しましたが、Kaggleスコア（0.7799）との乖離が大きく、過学習の可能性が示唆されます。そのため、未知データや本番運用を想定した場合、最終モデルとして選ぶにはややリスクがあります。\n",
        "一方で、XGBoostをメタモデルに使用したスタッキングモデルは、CVスコアとKaggleスコアの差が小さく、より一貫した安定した性能を示しました。\n",
        "したがって、最終モデルとしては Stacking (XGBoost meta) を選択するのが安全で信頼性が高いと言えます。"
      ],
      "metadata": {
        "id": "cXJJIzkcbVDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 📤 Final Submission Code / 最終提出用コード\n",
        "\n",
        "This section generates the final prediction file (`submission_final.csv`) using the best-performing model from our comparisons.  \n",
        "以下は、モデル比較の結果最も性能が良かったモデルを使って、Kaggle に提出する予測ファイル（`submission_final.csv`）を作成するコードです。\n"
      ],
      "metadata": {
        "id": "fIz2IudVEfKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTabtBuW8Mho"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# 1. Prepare test data / テストデータの特徴量を準備\n",
        "# ※ df_test に必要な前処理（欠損処理・特徴量変換）が済んでいる前提です\n",
        "# =====================================================\n",
        "X_test_selected = df_fe4_test[selected_features]  # test.csvと同様の特徴量セットを使用\n",
        "passenger_ids = df_test['PassengerId']  # Kaggle 提出用の ID カラム\n",
        "\n",
        "# =====================================================\n",
        "# 2. Make predictions / 予測を実行\n",
        "# =====================================================\n",
        "y_test_pred = best_model_stacking_XGB_tuned.predict(X_test_selected)\n",
        "\n",
        "# =====================================================\n",
        "# 3. Create submission DataFrame / 提出用のデータフレームを作成\n",
        "# =====================================================\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Survived': y_test_pred\n",
        "})\n",
        "\n",
        "# =====================================================\n",
        "# 4. Save to CSV / CSVファイルとして保存（Kaggleに提出可能）\n",
        "# =====================================================\n",
        "submission.to_csv('submission_final.csv', index=False)\n",
        "print(\"✅ submission.csv has been saved!\")\n",
        "\n",
        "\n",
        "# Colabの場合、自動でダウンロード（必要に応じて）\n",
        "from google.colab import files\n",
        "files.download('submission_final.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 ✅ Final Model Recommendation Report / 最終モデル推薦レポート\n",
        "\n",
        "### 🏆 Selected Model / 選定モデル  \n",
        "**Stacking (XGB meta)**\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Reason for Selection / 選定理由\n",
        "\n",
        "- Consistent performance across both CV and Kaggle leaderboard  \n",
        "- High ROC AUC and F1 score, indicating strong classification balance  \n",
        "- Although it requires longer training (1.84 sec) and prediction time (9.37 sec) compared to others, its superior generalization and stability outweigh the computational cost  \n",
        "\n",
        "- CVとKaggleスコアの両方で安定した性能  \n",
        "- ROC AUCやF1スコアも高く、バランスの取れた分類性能  \n",
        "- 学習時間は1.84秒、推論時間は9.37秒と他モデルより長いですが、安定した性能がそれを補っています\n",
        "\n",
        "---\n",
        "\n",
        "### 📉 Alternative Models Considered / 他に検討したモデル\n",
        "\n",
        "- **Random Forest**: Highest CV accuracy but larger gap with Kaggle score, suggesting overfitting. Training time 0.57 sec, prediction time 5.78 sec.  \n",
        "- **XGBoost**: Strong and fast, with training time 0.23 sec and prediction time 0.73 sec, but slightly outperformed by Stacking (XGB) in generalization  \n",
        "- **Voting**: Good accuracy but larger variation and weaker test set performance. Training and prediction times are relatively low (0.51 sec and 2.07 sec), making it suitable for faster inference scenarios despite slightly lower stability.  \n",
        "\n",
        "- **Random Forest**：CV精度が最も高いが、Kaggleとの乖離が大きく過学習の懸念あり（学習0.57秒、推論5.78秒）  \n",
        "- **XGBoost**：安定性は高く予測も高速（学習0.23秒、推論0.73秒）だが、Stackingの方が総合的に優れていた  \n",
        "- **Voting**：高精度だが、やや汎化性能が劣る。学習0.51秒、推論2.07秒と高速で、推論速度を重視する場合に適している  \n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Summary Table / サマリー表（抜粋）\n",
        "\n",
        "| Model               | CV Accuracy | Kaggle Score | Gap    | Train Time | Predict Time |\n",
        "|---------------------|-------------|---------------|--------|-------------|---------------|\n",
        "| Stacking (XGB meta) | 0.836       | 0.7799        | 0.0562 | 1.84 sec    | 9.37 sec      |\n",
        "| Random Forest       | 0.847       | 0.7799        | 0.0675 | 0.57 sec    | 5.78 sec      |\n",
        "| Voting              | 0.842       | 0.7703        | 0.0715 | 0.51 sec    | 2.07 sec      |\n",
        "\n",
        "---\n",
        "\n",
        "### 🟢 Final Recommendation / 最終推薦\n",
        "\n",
        "> **Deploy Stacking (XGB meta)** as the final model for its excellent generalization and consistent performance across all metrics.  \n",
        "> **採用モデル：Stacking (XGB meta)**  \n",
        "> 理由：精度、汎化性能、安定性、Kaggleとの一致性において最もバランスが良かったため。\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8YfamNmwCn3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8🎯 SHAP-Based Model Contribution (Meta Model: XGBoost) / SHAPによるベースモデルの貢献度分析（メタモデル：XGBoost）  \n",
        "\n",
        "We analyzed the importance of each base model’s predictions using SHAP values from the final meta-model (XGBoost) in the stacking ensemble.\n",
        "\n",
        "スタッキングのメタモデル（XGBoost）に対し、各ベースモデルの予測値がどれほど重要だったかをSHAP値で評価しました。"
      ],
      "metadata": {
        "id": "neQJC7e-JNCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Step 1: Prepare base model predictions\n",
        "# ステップ1：各ベースモデルの予測値（生存確率）を計算\n",
        "# These will be used as input features for the meta-model\n",
        "# → メタモデル（XGBoost）の入力特徴量となります\n",
        "# ===========================================\n",
        "base_preds = []\n",
        "\n",
        "# Get base model names\n",
        "model_names = list(best_model_stacking_XGB_tuned.named_estimators_.keys())\n",
        "\n",
        "# For each base model, get the predicted probability for class 1 (Survived)\n",
        "# 各ベースモデルに対して、生存クラス（クラス1）の予測確率を取得\n",
        "for name, model in best_model_stacking_XGB_tuned.named_estimators_.items():\n",
        "    # Use predict_proba (SVM pipeline is already calibrated)\n",
        "    preds = model.predict_proba(X_selected)[:, 1]\n",
        "    base_preds.append(preds)\n",
        "\n",
        "# ===========================================\n",
        "# Step 2: Stack predictions as meta-features\n",
        "# ステップ2：予測値を列方向に結合して、メタモデルの入力特徴量を作成\n",
        "# Shape will be (n_samples, n_base_models)\n",
        "# → (サンプル数, ベースモデル数)の2次元配列になります\n",
        "# ===========================================\n",
        "meta_features = np.column_stack(base_preds)\n",
        "\n",
        "# ===========================================\n",
        "# Step 3: Create SHAP explainer for the meta-model\n",
        "# ステップ3：メタモデル（XGBoost）用のSHAP Explainerを作成\n",
        "# ===========================================\n",
        "explainer_meta = shap.Explainer(best_model_stacking_XGB_tuned.final_estimator_)\n",
        "\n",
        "# ===========================================\n",
        "# Step 4: Calculate SHAP values for meta-model inputs\n",
        "# ステップ4：メタモデルに対するSHAP値を計算\n",
        "# → 各ベースモデルの予測が、スタック予測にどれだけ貢献しているかを分析\n",
        "# ===========================================\n",
        "shap_values_meta = explainer_meta(meta_features)\n",
        "\n",
        "# ===========================================\n",
        "# Step 5: Plot SHAP summary plot\n",
        "# ステップ5：SHAPサマリープロットで視覚化\n",
        "# → 各ベースモデルの「平均的な重要度」を可視化\n",
        "# ===========================================\n",
        "shap.summary_plot(shap_values_meta, features=meta_features, feature_names=model_names)\n",
        "\n",
        "# ===========================================\n",
        "# Step 6: Print mean absolute SHAP values\n",
        "# ステップ6：各特徴量（ベースモデル）の平均SHAP値を数値で表示\n",
        "# → グラフだけでなく、数値でも比較可能にします\n",
        "# ===========================================\n",
        "print(\"🧭 Mean SHAP Value (Importance) per Base Model / 各ベースモデルの平均SHAP値（重要度）:\")\n",
        "mean_abs_shap = np.abs(shap_values_meta.values).mean(axis=0)\n",
        "\n",
        "for name, val in zip(model_names, mean_abs_shap):\n",
        "    print(f\"{name}: {val:.4f}\")"
      ],
      "metadata": {
        "id": "r6oiB3MEUMUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### 🧭 Mean SHAP Values (Model Importance) / 平均SHAP値（モデルの重要度）\n",
        "\n",
        "| Base Model | SHAP Importance | Interpretation / 解釈 |\n",
        "|------------|------------------|------------------------|\n",
        "| XGBoost (`xgb`) | 0.3570 | ⭐ Most important contributor to final predictions / 最も予測に貢献 |\n",
        "| Random Forest (`rf`) | 0.3067 | 🔸 Strong contribution / 強い影響あり |\n",
        "| LightGBM (`lgb`) | 0.2203 | ⚠️ Moderate impact / 中程度の貢献 |\n",
        "| SVM (`svm`) | 0.0418 | 🔻 Minimal influence / ほとんど影響なし |\n",
        "\n",
        "This analysis confirms that the meta-model heavily relied on XGBoost and Random Forest to make its final decisions.  \n",
        "\n",
        "この結果から、メタモデルはXGBoostとRandom Forestの出力を主に信頼していたことがわかります。\n"
      ],
      "metadata": {
        "id": "3fB5MLTxUwQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Contribution Analysis Based on SHAP Values / SHAP値に基づくモデル貢献度の分析\n",
        "\n",
        "The SHAP summary plot shows the contribution of each base model to the final stacked prediction. Among the base models, XGBoost has the highest mean SHAP value (0.357), followed by Random Forest (0.307), LightGBM (0.220), and SVM (0.042).\n",
        "\n",
        "This suggests that the meta-model relies most heavily on the tree-based models (XGBoost, Random Forest, LightGBM) for its predictions. The relatively low contribution of SVM may be due to its linear nature, which limits its ability to capture complex nonlinear relationships in the data. Moreover, since the SVM is calibrated, its predicted probabilities are expected to be reliable but may lack the expressive power of tree-based models.\n",
        "\n",
        "Therefore, combining diverse model types in stacking leverages their complementary strengths: tree models capture nonlinearities well, while SVM provides a linear perspective. The meta-model effectively weights these contributions to improve overall prediction accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "SHAPサマリープロットは、最終的なスタッキング予測に対する各ベースモデルの貢献度を示しています。ベースモデルの中では、XGBoostが最も高い平均SHAP値（0.357）を持ち、次いでランダムフォレスト（0.307）、LightGBM（0.220）、SVM（0.042）となっています。\n",
        "\n",
        "これはメタモデルが主に木構造ベースのモデル（XGBoost、ランダムフォレスト、LightGBM）に依存していることを示唆しています。一方で、SVMの貢献度が低い理由は、線形モデルであるため複雑な非線形関係を捉えにくいことに起因していると考えられます。SVMは確率キャリブレーションを施しているため、予測確率の信頼性は高いものの、表現力では木構造モデルに劣る可能性があります。\n",
        "\n",
        "したがって、スタッキングにおいて多様なモデルを組み合わせることで、それぞれの強みを活かしています。木構造モデルは非線形性をよく捉え、SVMは線形性を補完する役割を果たしています。メタモデルはこれらの寄与度をうまく加重し、全体の予測精度を向上させています。\n",
        "\n"
      ],
      "metadata": {
        "id": "AJqbAZ2MYw0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9 🔍 Final Model Analysis / 最終モデルの評価と考察\n",
        "\n",
        "In this section, I summarize the performance of the final stacking model using both quantitative metrics and model interpretation techniques.\n",
        "Cross-validation scores are analyzed alongside SHAP values to evaluate which base models contributed most to the final prediction.\n",
        "This analysis helps assess the model’s strengths and limitations, especially in handling class imbalance.  \n",
        "\n",
        "このセクションでは、最終的に採用したスタッキングモデルの性能を、定量的な評価指標とモデル解釈の観点から総括します。\n",
        "交差検証のスコアに加えてSHAP値を用いることで、どのベースモデルが最終予測に最も貢献しているかを可視化し、モデルの強みと課題（特にクラス不均衡への対応）について考察を行います。\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 1. Model Performance Summary / モデルの性能評価\n",
        "\n",
        "| Metric / 指標            | Value / 値  |\n",
        "|--------------------------|-------------|\n",
        "| 🎯 Mean CV Accuracy      | **0.836**   |\n",
        "| 📈 Mean CV ROC AUC       | **0.874**   |\n",
        "\n",
        "📋 **Classification Report (CV predictions) / 分類レポート（交差検証）:**\n",
        "\n",
        "| Class / クラス | Precision | Recall | F1-score |\n",
        "|----------------|-----------|--------|----------|\n",
        "| 0 (Not Survived / 非生存) | 0.84 | 0.90 | 0.87 |\n",
        "| 1 (Survived / 生存)       | 0.82 | 0.73 | 0.77 |\n",
        "\n",
        "🗣️ **Interpretation / 解釈:**\n",
        "- The model predicts non-survivors well, with high recall (0.90).  \n",
        "- Survivor recall (0.73) is lower, indicating some survivors are missed.\n",
        "- モデルは非生存者の予測精度が高く、再現率（Recall）が0.90と優れている。\n",
        "- 一方、生存者のRecallが0.73と低めで、一部の生存者を見逃している可能性がある。\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 2. SHAP-Based Contribution of Base Models / SHAPによるベースモデルの重要度分析\n",
        "\n",
        "| Base Model | Mean SHAP Value / 平均SHAP値 |\n",
        "|------------|-------------------------------|\n",
        "| XGBoost    | **0.3570**                    |\n",
        "| RandomForest | **0.3067**                  |\n",
        "| LightGBM   | **0.2203**                    |\n",
        "| SVM        | **0.0418**                    |\n",
        "\n",
        "🧠 **Discussion / 考察:**\n",
        "- XGBoost and RandomForest had the highest SHAP importance → strong contribution to final prediction.  \n",
        "- These models handle non-linear relationships well, which fits the Titanic dataset.  \n",
        "- SVM had the lowest contribution, likely because it is linear and cannot capture complex patterns.  \n",
        "- Therefore, tree-based models are better suited for this task.\n",
        "\n",
        "-  XGBoostとRandomForestのSHAP値が最も高く、最終予測への貢献が大きいとわかる。\n",
        "- これらのモデルは非線形構造に強く、Titanicのような複雑なデータに適している。\n",
        "-  SVMは線形モデルであるため、複雑な構造を捉えにくく、貢献度が低かったと考えられる。\n",
        "-  よって、ツリー系モデルがこの問題にはより適していると結論づけられる。\n",
        "\n",
        "---\n",
        "\n",
        "### 📝 3. Conclusion / 結論\n",
        "\n",
        "📌 The stacking model with XGBoost meta-learner performs well overall, but recall for survivors can be improved.  \n",
        "📌 XGBoostをメタ学習器に使ったスタッキングモデルは全体として高性能だが、生存者のRecallは改善の余地がある。\n",
        "\n",
        "📈 **Possible improvements / 改善の方向性:**\n",
        "- Class balancing (e.g. SMOTE) or class weight tuning may improve recall for minority class (survivors).  \n",
        "- Base model selection can be further optimized.  \n",
        "-   クラス不均衡への対応（例：SMOTE）やクラス重み調整でRecallを改善できる可能性がある。\n",
        "- ベースモデルの構成や選定を見直すことで、さらなる性能向上が期待できる。\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Summary / まとめ\n",
        "\n",
        "- Final stacking model (meta: XGBoost) achieves 83.6% accuracy and 87.4% ROC AUC.\n",
        "- SHAP analysis shows strongest contributions from XGBoost and RandomForest.\n",
        "- SVM has low impact, likely due to its linear nature.\n",
        "- Overall strong performance, with room to improve recall for class 1.\n",
        "\n",
        "- 最終的なスタッキングモデル（メタモデル：XGBoost）は、**正解率 83.6%**、**ROC AUC 87.4%** を達成しました。  \n",
        "- SHAP解析によれば、XGBoostとRandomForestが最も大きな貢献をしていることが分かりました。  \n",
        "- SVMの貢献度は低く、これは線形モデルであるため複雑なパターンを捉えにくいことが原因と考えられます。  \n",
        "- 全体として性能は高いですが、生存者（クラス1）のRecallには改善の余地があります。\n"
      ],
      "metadata": {
        "id": "mrHJHGem7dWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 📌 Conclusion / 結論  \n",
        "\n",
        "In this final section, I summarize the overall approach, results, key insights, and areas for future improvement based on the Titanic survival prediction project.\n",
        "This reflection highlights what was learned through model development, evaluation, and interpretation.\n",
        "\n",
        "この最終セクションでは、Titanic生存予測プロジェクトにおけるアプローチ、結果、得られた知見、今後の改善点について総括します。\n",
        "モデル構築から評価・解釈に至るまでの一連の過程を振り返り、得られた学びを明確にします。\n",
        "\n",
        "🎯 **Objective**  \n",
        "The goal of this project was to predict passenger survival on the Titanic using machine learning.  \n",
        "本プロジェクトの目的は、Titanic乗客の生存を機械学習で予測することでした。\n",
        "\n",
        "🔧 **Approach**  \n",
        "- Data preprocessing (missing value imputation, feature engineering, encoding)  \n",
        "- Built and tuned base models: RandomForest, XGBoost, LightGBM, and a calibrated SVM  \n",
        "- Combined all models using stacking, with XGBoost as the meta-learner  \n",
        "データの前処理（欠損値補完、特徴量エンジニアリング、カテゴリ変換）を行い、  \n",
        "RandomForest、XGBoost、LightGBM、SVMなどのモデルを構築・チューニングし、  \n",
        "それらをXGBoostメタモデルでスタッキングしました。\n",
        "\n",
        "📈 **Results**  \n",
        "- Final stacking model achieved **83.6% accuracy** and **87.4% ROC AUC**  \n",
        "- SHAP analysis showed that XGBoost and RandomForest had the highest impact  \n",
        "- SVM contributed the least, likely due to its linear nature  \n",
        "最終的なスタッキングモデルは**正解率83.6%**、**ROC AUCは87.4%**を記録しました。  \n",
        "SHAP分析の結果、XGBoostとRandomForestが最も大きな影響を持ち、  \n",
        "線形モデルであるSVMは、貢献度が比較的低いことが分かりました。\n",
        "\n",
        "🧠 **Insights**  \n",
        "- Tree-based models handled complex, non-linear patterns better in the Titanic dataset  \n",
        "- Linear models like SVM may be less effective in such settings  \n",
        "ツリー系モデルはTitanicのような複雑で非線形なパターンに強く、  \n",
        "SVMのような線形モデルはそのようなデータには不向きであることが示されました。\n",
        "\n",
        "🚀 **Future Work**  \n",
        "- Improve recall for the minority class (survivors) using techniques like SMOTE or class weighting  \n",
        "- Consider adding more features or optimizing base model selection  \n",
        "少数クラス（生存者）のRecallを改善するには、SMOTEなどの手法やクラス重みの調整が有効です。  \n",
        "また、特徴量の追加やベースモデルの選定や構成の最適化も今後の改善ポイントです。\n",
        "\n",
        "🧾 **Summary**  \n",
        "This project demonstrated that stacking ensemble models—combined with SHAP for interpretability—can achieve strong predictive performance on classification tasks like Titanic survival.  \n",
        "本プロジェクトでは、スタッキングとSHAPによる説明可能性を活用することで、Titanic生存予測のような分類タスクで高精度な予測が可能であることを示しました。\n"
      ],
      "metadata": {
        "id": "ALvwY2Sz-Qjm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}